{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "primary_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0 # need to run and restart runtime for TabularDataset, Field and LabelField imports"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i-pKtoIEJpU",
        "outputId": "c4650822-324a-4cc8-926d-e60e57c82a92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.6.15)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "Successfully installed sentencepiece-0.1.96 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### LIBRARY IMPORTS ###\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchtext\n",
        "#import torchtext.data as data\n",
        "from torchtext.data import get_tokenizer, TabularDataset, Field, LabelField\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import spacy   # may be unused not sure currently\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "#import torchtext.data as data\n",
        "#from spacy.en import English"
      ],
      "metadata": {
        "id": "4Zojl-TKxK5c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WeiQXAOm-jHL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "VNaiN771x4gy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9507221d-8ae6-4a47-c585-5bced53ea64e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacey"
      ],
      "metadata": {
        "id": "ZssxsDUFthmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8018e851-cbbe-43e1-f245-da7924b812f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacey\n",
            "  Downloading spacey-0.1.1-py3-none-any.whl (2.1 kB)\n",
            "Installing collected packages: spacey\n",
            "Successfully installed spacey-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "dB7_Eq5Lwrif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01472fe3-0f19-4757-8e27-2e1c60a2f8f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.8 MB 528 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"Data.csv\") # must upload data csv to run\n",
        "data[[\"Tweet\", \"ratio\"]]"
      ],
      "metadata": {
        "id": "BSe4Ut_HrTBh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "03c4fca7-80ef-4461-9ff2-320f5fc4cdfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Tweet   ratio\n",
              "0                Use a emoji to describe Rafa Nadal üëá  0.8459\n",
              "1   @seahawksfan2314 @SportsCenter not a sport say...  0.0160\n",
              "2   Just played tennis. Probably shouldn‚Äôt have wo...  0.0878\n",
              "3   THE BOYZ WORLD TOUR : THE B-ZONE IN JAKARTAüåê\\n...  0.0975\n",
              "4   What are the toughest sports out there? Boxing...  0.3592\n",
              "..                                                ...     ...\n",
              "95  late night tennis! cheers @Janina for the phot...  0.1548\n",
              "96  time for tennis, me vs luke. who do you think ...  0.8887\n",
              "97  had a nice tennis session, hot and sweaty ; ) ...  0.2110\n",
              "98               Playin some tennis with my cousins ‚úä  0.0616\n",
              "99    Good early morning tennis match with the baes üéæ  0.2962\n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-289194f8-3406-4942-ae35-657c6ea0e9e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Use a emoji to describe Rafa Nadal üëá</td>\n",
              "      <td>0.8459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@seahawksfan2314 @SportsCenter not a sport say...</td>\n",
              "      <td>0.0160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Just played tennis. Probably shouldn‚Äôt have wo...</td>\n",
              "      <td>0.0878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THE BOYZ WORLD TOUR : THE B-ZONE IN JAKARTAüåê\\n...</td>\n",
              "      <td>0.0975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the toughest sports out there? Boxing...</td>\n",
              "      <td>0.3592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>late night tennis! cheers @Janina for the phot...</td>\n",
              "      <td>0.1548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>time for tennis, me vs luke. who do you think ...</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>had a nice tennis session, hot and sweaty ; ) ...</td>\n",
              "      <td>0.2110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Playin some tennis with my cousins ‚úä</td>\n",
              "      <td>0.0616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Good early morning tennis match with the baes üéæ</td>\n",
              "      <td>0.2962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows √ó 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-289194f8-3406-4942-ae35-657c6ea0e9e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-289194f8-3406-4942-ae35-657c6ea0e9e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-289194f8-3406-4942-ae35-657c6ea0e9e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame ###\n",
        "# data['tokenized_tweets'] = data.apply(lambda row: nltk.word_tokenize(row['Tweet']), axis=1) # tokenize tweets\n",
        "data_reduced = data[[\"Tweet\", \"ratio\"]]"
      ],
      "metadata": {
        "id": "RcpOk5oQta0Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_reduced.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6eZQD4Sq661I",
        "outputId": "cbf38d3f-7a95-425d-bfa4-0e4e3693ec09"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Tweet   ratio\n",
              "95  late night tennis! cheers @Janina for the phot...  0.1548\n",
              "96  time for tennis, me vs luke. who do you think ...  0.8887\n",
              "97  had a nice tennis session, hot and sweaty ; ) ...  0.2110\n",
              "98               Playin some tennis with my cousins ‚úä  0.0616\n",
              "99    Good early morning tennis match with the baes üéæ  0.2962"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35bbbb36-7d86-469a-9433-157da49323bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>late night tennis! cheers @Janina for the phot...</td>\n",
              "      <td>0.1548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>time for tennis, me vs luke. who do you think ...</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>had a nice tennis session, hot and sweaty ; ) ...</td>\n",
              "      <td>0.2110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Playin some tennis with my cousins ‚úä</td>\n",
              "      <td>0.0616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Good early morning tennis match with the baes üéæ</td>\n",
              "      <td>0.2962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35bbbb36-7d86-469a-9433-157da49323bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-35bbbb36-7d86-469a-9433-157da49323bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-35bbbb36-7d86-469a-9433-157da49323bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to tabular dataset\n",
        "data_reduced.to_csv(\"tokenized_data.csv\", index=None)\n",
        "tokenised_data = Field(tokenize=\"spacy\", tokenizer_language=\"en_core_web_sm\")\n",
        "# https://github.com/pytorch/text/issues/78\n",
        "labels = LabelField(dtype = torch.float, use_vocab=False, preprocessing=float)#, postprocessing=torchtext.data.Pipeline(lambda x: float(x))) # converting string ratios to doubles and not using a vocab for target labels\n",
        "dataset = TabularDataset(path=\"tokenized_data.csv\", format=\"CSV\", fields=[(\"tweet\", tokenised_data),(\"ratio\", labels)],skip_header=True)\n",
        "vars(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVNupGl0z3sc",
        "outputId": "e16a5338-c1dc-48af-a1e1-124ab92ad04e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ratio': 0.8459,\n",
              " 'tweet': ['Use', 'a', 'emoji', 'to', 'describe', 'Rafa', 'Nadal', 'üëá']}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set, test_set = dataset.split(split_ratio=[0.7, 0.15, 0.15],random_state=random.seed(0))  "
      ],
      "metadata": {
        "id": "tL82Qk8ix7lQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenised_data.build_vocab(train_set, max_size=20000) # dictionary stores a maximum of 20000 words as to get the jist of the tweets while saving space.\n",
        "labels.build_vocab(train_set) \n",
        "print(len(tokenised_data.vocab))\n",
        "tokenised_data.vocab.freqs.most_common(20)\n",
        "tokenised_data.vocab.itos[21]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3HhY58sp38QL",
        "outputId": "122b930d-a233-4ee0-8543-fc03f12585b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "998\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_set), len(val_set), len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPRQ8VVs3HWB",
        "outputId": "0a4578b9-b505-4722-c5d4-dab5079b33da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70 15 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sanity_set = dataset # for now, later when we have actual dataset, this will need to be removed to only be a subset of roughly 100 tweets from the dataset"
      ],
      "metadata": {
        "id": "hpQdYR0_3hsz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader  = torchtext.data.BucketIterator( train_set, batch_size=10, device = device, shuffle=True, sort_key = len, sort_within_batch=False)\n",
        "val_loader    = torchtext.data.BucketIterator(   val_set, batch_size=10, device = device, shuffle=True, sort_key = len, sort_within_batch=False)\n",
        "test_loader   = torchtext.data.BucketIterator(  test_set, batch_size=10, device = device, shuffle=True, sort_key = len, sort_within_batch=False)\n",
        "sanity_loader = torchtext.data.BucketIterator(sanity_set, batch_size=10, device = device, shuffle=True, sort_key = len, sort_within_batch=False) # lambda x: len(x.tokenized_tweets)\n",
        "sanity_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esAe1-emp-sL",
        "outputId": "f176d6f8-e0f9-4a28-fc74-e16975fd964d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.dataset.TabularDataset at 0x7f40e0638550>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for data in sanity_loader:\n",
        "#  break\n",
        "for i, data in enumerate(sanity_loader, 0):\n",
        "  text, label = data\n",
        "  print(label) \n",
        "  print(text)      # each column represents 1 tweet, each word coded to one integer. 1s represent sentence padding, each batch has different sentence lengths\n",
        "  \n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB6sBOOBBIqm",
        "outputId": "be18c03c-802a-485d-b98f-681b87c93657"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0063, 0.0625, 0.1995, 0.5540, 0.0610, 1.5381, 0.1336, 0.1899, 0.0330,\n",
            "        0.1757], device='cuda:0')\n",
            "tensor([[400, 378, 353,   7, 102,  19,   7, 110, 397, 406],\n",
            "        [ 57,  58,  21, 148,  85, 249,  62, 307, 114,  24],\n",
            "        [209, 903,  89,  43,  29,   5, 260, 383, 685,  17],\n",
            "        [ 79, 196, 310, 542,  91,  51, 890, 111,  11, 811],\n",
            "        [ 31,  32,  49,   6,  86,  68,   6, 616,   4, 660],\n",
            "        [253, 223, 501, 533,   4,   5, 112, 851,  56, 950],\n",
            "        [263,   6,   4,  32,   5,  93,  31, 761,   7,  14],\n",
            "        [ 21, 139, 249, 260,   7, 860, 168, 696,  81,  17],\n",
            "        [655,   3,  51, 205, 100,   6, 123, 531,   6,  45],\n",
            "        [254,   4,  68, 852, 553,  81, 621, 638, 854,   4],\n",
            "        [  3,   9,  87,   5, 519, 944,   2, 670, 884,  45],\n",
            "        [  4, 747,  26, 134,  11,   5,  12,   5,  83,   6],\n",
            "        [ 64,  58, 447, 834, 288, 186, 105, 697,   1, 539],\n",
            "        [  9,   6, 246, 217, 895,  31, 885,   5,   1, 150],\n",
            "        [ 60, 611, 931,  44, 242,  53, 853,   4,   1,   7],\n",
            "        [625,  20,   9, 751,   5,  23, 548, 645,   1,  34],\n",
            "        [680,  84, 322,  16, 266, 841,  20,  22,   1, 808],\n",
            "        [239, 509,  80, 213, 818,   6, 227, 640,   1,   6],\n",
            "        [126,  28, 908,  21, 290, 795, 819, 970,   1,  25],\n",
            "        [  3, 677, 821,  92, 127,   3, 131, 724,   1,  77],\n",
            "        [967,  24,  11,   8,  29, 369, 899,   1,   1, 261],\n",
            "        [768,  15,   3, 749, 927, 172,   9,   1,   1,   2],\n",
            "        [ 14, 570, 319,  11, 926,   5,  23,   1,   1, 463],\n",
            "        [ 37,   1, 172,   3, 767, 513,  56,   1,   1,  91],\n",
            "        [942,   1,   5, 268,   2,   9, 572,   1,   1,  25],\n",
            "        [ 37,   1, 183, 281,   1,  36,  11,   1,   1,   8],\n",
            "        [858,   1, 557,   5,   1,   3,   3,   1,   1, 218],\n",
            "        [ 37,   1, 614,  59,   1, 560, 673,   1,   1, 258],\n",
            "        [ 40,   1, 198,  24,   1,   4,  20,   1,   1, 151],\n",
            "        [ 43,   1, 126, 634,   1, 930, 769,   1,   1, 624],\n",
            "        [848,   1,   3, 620,   1, 213,  10,   1,   1, 248],\n",
            "        [732,   1, 601,  21,   1,   8,   4,   1,   1,  18],\n",
            "        [  1,   1,  18,  25,   1, 343, 552,   1,   1,   1],\n",
            "        [  1,   1, 144,  20,   1, 945,   2,   1,   1,   1],\n",
            "        [  1,   1,   1,  60,   1,  23,  12,   1,   1,   1],\n",
            "        [  1,   1,   1, 665,   1, 866, 453,   1,   1,   1],\n",
            "        [  1,   1,   1,  29,   1,   6,  56,   1,   1,   1],\n",
            "        [  1,   1,   1,  18,   1, 592, 812,   1,   1,   1],\n",
            "        [  1,   1,   1,  22,   1,   2, 666,   1,   1,   1],\n",
            "        [  1,   1,   1,  66,   1,  22,   2,   1,   1,   1],\n",
            "        [  1,   1,   1, 438,   1,  73, 176,   1,   1,   1],\n",
            "        [  1,   1,   1,   5,   1,  28,  21,   1,   1,   1],\n",
            "        [  1,   1,   1,   7,   1,  60,  11,   1,   1,   1],\n",
            "        [  1,   1,   1, 964,   1, 921,   3,   1,   1,   1],\n",
            "        [  1,   1,   1, 120,   1,  24, 906,   1,   1,   1],\n",
            "        [  1,   1,   1,  57,   1,  31, 189,   1,   1,   1],\n",
            "        [  1,   1,   1,   6,   1,  18,   9,   1,   1,   1],\n",
            "        [  1,   1,   1,   3,   1,   1, 189,   1,   1,   1],\n",
            "        [  1,   1,   1, 631,   1,   1,   2,   1,   1,   1],\n",
            "        [  1,   1,   1,   9,   1,   1,  98,   1,   1,   1],\n",
            "        [  1,   1,   1, 695,   1,   1, 716,   1,   1,   1],\n",
            "        [  1,   1,   1,   8,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1,   4,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1,  64,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1, 184,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1,  17,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1, 211,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1,  14,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1, 699,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1,   2,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1,  61,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1,  12,   1,   1,   1,   1,   1,   1],\n",
            "        [  1,   1,   1, 727,   1,   1,   1,   1,   1,   1]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "P_EACVjJvzAe"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules import dropout\n",
        "class RatioNet(nn.Module):\n",
        "  def __init__(self, input_size, batch_size, device, hidden_size=50, embedding_size=20, lstm_input_size=100, num_layers = 2, dropout=0.5):\n",
        "    super(RatioNet, self).__init__()\n",
        "\n",
        "    self.name = \"RatioNet\"\n",
        "    self.device =device\n",
        "    self.bs = batch_size\n",
        "    self.lstmis = lstm_input_size\n",
        "    self.hs = hidden_size\n",
        "    self.nl = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)  # sequence size (padded tweet length), batch_size, embedding size (ie vector representation of integer words)\n",
        "\n",
        "    self.conv1 = nn.Conv1d(embedding_size, 200, 3)\n",
        "    o = (64 - 3)//1 + 1         # conv1 output size\n",
        "    #o = (o - 2)//2 + 1                      # maxpool output size\n",
        "    #print(o)\n",
        "\n",
        "    self.conv2 = nn.Conv1d(200, 250, 3)\n",
        "    o = (o - 3)//1 + 1                      # conv2 output size\n",
        "    #o = (o - 2)//2 + 1                      # maxpool output size\n",
        "    #print(o)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(250, self.lstmis, 3)\n",
        "    o = (o - 3)//1 + 1                      # conv3 output size\n",
        "    #print(o)\n",
        "    self.maxpool = nn.MaxPool1d(2, 2)\n",
        "    lstm_input_size = (o - 2)//2 + 1        # maxpool output size\n",
        "    #print(lstm_input_size, \"word reduction\")\n",
        "\n",
        "    self.stacked_LSTM = nn.LSTM(input_size = self.lstmis, hidden_size = self.hs, num_layers = self.nl, batch_first=True, dropout=dropout, bidirectional=True) # batch_first==True >>> (batch, seq, features) , batch_first==False (seq, batch, feature)\n",
        "    \n",
        "    self.output_layer = nn.Linear(self.hs * 2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # convert integer coded tweets to embeddings\n",
        "    x = self.embedding(x)\n",
        "    x = x.permute(1, 2, 0)   # convert to (batch_size, embedding dimension, number of words) order to feed into conv layers (embedding dimension == # channels)\n",
        "    \n",
        "    #print(x.size(), \"permute\")\n",
        "    x = (F.relu(self.conv1(x)))\n",
        "    x = (F.relu(self.conv2(x)))\n",
        "    x = self.maxpool(F.relu(self.conv3(x)))\n",
        "    #print(x.size())\n",
        "\n",
        "    x = x.permute(0, 2, 1) # convert to (batch_size, number of words, embedding dimension) order to feed into stacked lstm layer (batch_first set to True) \n",
        "\n",
        "    h_0 = torch.ones(self.nl * 2, self.bs, self.hs).to(self.device)\n",
        "    c_0 = torch.ones(self.nl * 2, self.bs, self.hs).to(self.device)\n",
        "    x, (h_n, c_n) = self.stacked_LSTM(x, (h_0, c_0))  # output, hidden state, cell state\n",
        "    # if you visualize the lstm you can see that x[:,-1,:] (output from the final word) is the same as the output of the second hidden layer output (only for non-bidirectional LSTMs)\n",
        "    # print(h_n[-1]==x[:,-1,:])\n",
        "    #print(x.size())\n",
        "    #print(h_n.size())\n",
        "\n",
        "    # passing lstm final output into output layer for ratio prediction\n",
        "    #out = F.relu(self.output_layer(h_n[-1])) # output should be above 0 but unbounded above pass in final hidden layer\n",
        "    out = F.relu(self.output_layer(x[:,-1,:]))\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import dropout\n",
        "class RatioNet2(nn.Module):\n",
        "  def __init__(self, input_size, batch_size, device, hidden_size=50, embedding_size=20, lstm_input_size=100, num_layers = 2, dropout=0.5):\n",
        "    super(RatioNet2, self).__init__()\n",
        "\n",
        "    self.name = \"RatioNet2\"\n",
        "    self.device =device\n",
        "    self.bs = batch_size\n",
        "    self.lstmis = lstm_input_size\n",
        "    self.hs = hidden_size\n",
        "    self.nl = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)  # sequence size (padded tweet length), batch_size, embedding size (ie vector representation of integer words)\n",
        "\n",
        "    self.conv1 = nn.Conv1d(embedding_size, 200, 3)\n",
        "    o = (64 - 3)//1 + 1         # conv1 output size\n",
        "    #o = (o - 2)//2 + 1                      # maxpool output size\n",
        "    #print(o)\n",
        "\n",
        "    self.conv2 = nn.Conv1d(200, self.lstmis, 3)\n",
        "    o = (o - 3)//1 + 1                      # conv2 output size\n",
        "    self.maxpool = nn.MaxPool1d(2, 2)\n",
        "    lstm_input_size = (o - 2)//2 + 1        # maxpool output size\n",
        "    #print(lstm_input_size, \"word reduction\")\n",
        "\n",
        "    self.stacked_LSTM = nn.LSTM(input_size = self.lstmis, hidden_size = self.hs, num_layers = self.nl, batch_first=True, dropout=dropout, bidirectional=True) # batch_first==True >>> (batch, seq, features) , batch_first==False (seq, batch, feature)\n",
        "    \n",
        "    self.output_layer = nn.Linear(self.hs * 2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # convert integer coded tweets to embeddings\n",
        "    x = self.embedding(x)\n",
        "    x = x.permute(1, 2, 0)   # convert to (batch_size, embedding dimension, number of words) order to feed into conv layers (embedding dimension == # channels)\n",
        "    \n",
        "    #print(x.size(), \"permute\")\n",
        "    x = (F.relu(self.conv1(x)))\n",
        "    x = self.maxpool(F.relu(self.conv2(x)))\n",
        "    #print(x.size())\n",
        "\n",
        "    x = x.permute(0, 2, 1) # convert to (batch_size, number of words, embedding dimension) order to feed into stacked lstm layer (batch_first set to True) \n",
        "\n",
        "    h_0 = torch.ones(self.nl * 2, self.bs, self.hs).to(self.device)\n",
        "    c_0 = torch.ones(self.nl * 2, self.bs, self.hs).to(self.device)\n",
        "    x, (h_n, c_n) = self.stacked_LSTM(x, (h_0, c_0))  # output, hidden state, cell state\n",
        "    # if you visualize the lstm you can see that x[:,-1,:] (output from the final word) is the same as the output of the second hidden layer output (only for non-bidirectional LSTMs)\n",
        "    # print(h_n[-1]==x[:,-1,:])\n",
        "    #print(x.size())\n",
        "    #print(h_n.size())\n",
        "    #print(self.stacked_LSTM.all_weights)\n",
        "    # passing lstm final output into output layer for ratio prediction\n",
        "    #out = F.relu(self.output_layer(h_n[-1])) # output should be above 0 but unbounded above pass in final hidden layer\n",
        "    out = F.relu(self.output_layer(x[:,-1,:]))    \n",
        "    return out"
      ],
      "metadata": {
        "id": "TPDQJVIN_Sxq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class no_conv_RatioNet(nn.Module):\n",
        "  def __init__(self, input_size, batch_size, device, hidden_size=50, embedding_size=20, lstm_input_size=100, num_layers = 2, dropout=0.5):\n",
        "    super(no_conv_RatioNet, self).__init__()\n",
        "\n",
        "    self.name = \"no_conv_RatioNet\"\n",
        "    self.device =device\n",
        "    self.bs = batch_size\n",
        "    self.lstmis = lstm_input_size\n",
        "    self.hs = hidden_size\n",
        "    self.nl = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)  # sequence size (padded tweet length), batch_size, embedding size (ie vector representation of integer words)\n",
        "\n",
        "    self.stacked_LSTM = nn.LSTM(input_size = self.lstmis, hidden_size = self.hs, num_layers = self.nl, batch_first=True, dropout=dropout, bidirectional=True) # batch_first==True >>> (batch, seq, features) , batch_first==False (seq, batch, feature)\n",
        "    \n",
        "    self.output_layer = nn.Linear(self.hs * 2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # convert integer coded tweets to embeddings\n",
        "    x = self.embedding(x)\n",
        "    x = x.permute(1, 0, 2)   # convert to (batch_size, number of words, embedding dimension) order to feed into stacked lstm layer (batch_first set to True)    \n",
        "\n",
        "    h_0 = torch.ones(self.nl * 2, self.bs, self.hs).to(self.device)\n",
        "    c_0 = torch.ones(self.nl * 2, self.bs, self.hs).to(self.device)\n",
        "    x, (h_n, c_n) = self.stacked_LSTM(x, (h_0, c_0))  # output, hidden state, cell state\n",
        "    # if you visualize the lstm you can see that x[:,-1,:] (output from the final word) is the same as the output of the second hidden layer output (only for non-bidirectional LSTMs)\n",
        "    # print(h_n[-1]==x[:,-1,:])\n",
        "    #print(x.size())\n",
        "    #print(h_n.size())\n",
        "    \n",
        "    # passing lstm final output into output layer for ratio prediction\n",
        "    #out = F.relu(self.output_layer(h_n[-1])) # output should be above 0 but unbounded above pass in final hidden layer\n",
        "    out = F.relu(self.output_layer(x[:,-1,:]))\n",
        "    \n",
        "    return out"
      ],
      "metadata": {
        "id": "8qoR1YHVuNaz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import dropout\n",
        "class no_lstm_RatioNet(nn.Module):\n",
        "  def __init__(self, input_size, batch_size, device, hidden_size=50, embedding_size=20, lstm_input_size=100, num_layers = 2, dropout=0.5):\n",
        "    super(no_lstm_RatioNet, self).__init__()\n",
        "\n",
        "    self.name = \"no_lstm_RatioNet\"\n",
        "    self.device =device\n",
        "    self.bs = batch_size\n",
        "    self.lstmis = lstm_input_size\n",
        "    self.hs = hidden_size\n",
        "    self.nl = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)  # sequence size (padded tweet length), batch_size, embedding size (ie vector representation of integer words)\n",
        "\n",
        "    self.conv1 = nn.Conv1d(embedding_size, 200, 3)\n",
        "    o = (64 - 3)//1 + 1         # conv1 output size\n",
        "    #o = (o - 2)//2 + 1                      # maxpool output size\n",
        "    #print(o)\n",
        "\n",
        "    self.conv2 = nn.Conv1d(200, 250, 3)\n",
        "    o = (o - 3)//1 + 1                      # conv2 output size\n",
        "    #o = (o - 2)//2 + 1                      # maxpool output size\n",
        "    #print(o)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(250, 300, 3)\n",
        "    o = (o - 3)//1 + 1                      # conv3 output size\n",
        "    #print(o)\n",
        "    self.maxpool = nn.MaxPool1d(2, 2)\n",
        "    fc_input_size = (o - 2)//2 + 1        # maxpool output size\n",
        "    #print(lstm_input_size, \"word reduction\")\n",
        "    \n",
        "    self.output_layer = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # convert integer coded tweets to embeddings\n",
        "    x = self.embedding(x)\n",
        "    x = x.permute(1, 2, 0)   # convert to (batch_size, embedding dimension, number of words) order to feed into conv layers (embedding dimension == # channels)\n",
        "    \n",
        "    #print(x.size(), \"permute\")\n",
        "    x = (F.relu(self.conv1(x)))\n",
        "    x = (F.relu(self.conv2(x)))\n",
        "    x = self.maxpool(F.relu(self.conv3(x)))\n",
        "    x, _ = x.max(dim=-1)\n",
        "    out = F.relu(self.output_layer(x))    \n",
        "    return out"
      ],
      "metadata": {
        "id": "JqU0HIxXvYub"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[0].size())\n",
        "net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10, device=device, embedding_size=200, hidden_size=200, lstm_input_size=300).to(device)\n",
        "out = net(text)\n",
        "#len(tokenised_data.vocab)\n",
        "print(out, \"out\") # testing forward pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KrgeBdN5JbF",
        "outputId": "49e57a0d-11a6-4bfe-8899-c5803ffff6f0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "tensor([[0.1378],\n",
            "        [0.1534],\n",
            "        [0.1606],\n",
            "        [0.1463],\n",
            "        [0.1162],\n",
            "        [0.1456],\n",
            "        [0.1305],\n",
            "        [0.1630],\n",
            "        [0.1253],\n",
            "        [0.1927]], device='cuda:0', grad_fn=<ReluBackward0>) out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curve(path='', data=[-1,-1,-1,-1]): # from lab 2 with some small additions\n",
        "    \"\"\" Plots the training curve for a model run, given the csv files\n",
        "    containing the train/validation error/loss.\n",
        "\n",
        "    Args:\n",
        "        path: The base path of the csv files produced during training\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    # If path not given for loading data, read from data input\n",
        "    # this option was added as sometimes the training would not save the error \n",
        "    # and loss as csvs, and rather than retraining the set for hours, I thought \n",
        "    # it would be quicker to get the loss and error from the console prints and \n",
        "    # place them into arrays.\n",
        "    if path!='':\n",
        "      #train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
        "      #val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
        "      train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
        "      val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
        "    else:\n",
        "      #train_err, val_err, \n",
        "      train_loss, val_loss = data\n",
        "    #plt.title(\"Train vs Validation Error\")\n",
        "    n = len(train_loss) # number of epochs\n",
        "    #plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
        "    #plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
        "    #plt.xlabel(\"Epoch\")\n",
        "    #plt.ylabel(\"Error\")\n",
        "    #plt.legend(loc='best')\n",
        "    #plt.show()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
        "    #plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gYunIcrxL0I9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_net(net,\n",
        "              data_loaders,\n",
        "              epochs=5,\n",
        "              learning_rate=0.001,\n",
        "              batch_size=10,\n",
        "              hidden_size=100,\n",
        "              embedding_size=20,\n",
        "              lstm_input_size=100,\n",
        "              lstm_layers=2,\n",
        "              dropout=0.5,\n",
        "              momentum=0.9,\n",
        "              sanity_check=False,\n",
        "              device=\"cpu\",\n",
        "              weight_decay = 1e-5,\n",
        "              adam=False\n",
        "              ):\n",
        "  net.train()\n",
        "  torch.manual_seed(0)\n",
        "  net = net.to(device)\n",
        "\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  if adam:\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  else:\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "  # initialize error and loss statistic numpy arrays\n",
        "  train_err = np.zeros(epochs)\n",
        "  train_loss= np.zeros(epochs)\n",
        "  val_err   = np.zeros(epochs)\n",
        "  val_loss  = np.zeros(epochs)\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  if sanity_check:\n",
        "    train_load = data_loaders\n",
        "  else:\n",
        "    train_load, val_load, test_load = data_loaders\n",
        "  \n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_epoch_err = 0\n",
        "    train_epoch_loss = 0\n",
        "    val_epoch_err = 0\n",
        "    val_Epoch_loss = 0\n",
        "    for i, data in enumerate(train_load,0):\n",
        "      tweet,ratio = data\n",
        "      tweet = tweet.to(device)\n",
        "      ratio = ratio.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      out = net(tweet)\n",
        "\n",
        "      loss = criterion(out, ratio)\n",
        "      #loss.register_hook(lambda grad: print(grad))\n",
        "      loss.backward()\n",
        "      #print(net.conv1.weight.grad)\n",
        "      optimizer.step()\n",
        "      train_epoch_loss += loss.item()\n",
        "    train_loss[epoch] = train_epoch_loss/(i+1)\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {train_loss[epoch]}, Validation Loss: [not yet available], prediction: {out[-1][-1]}, true ratio: {ratio[-1]}\")\n",
        "    model_path = \"//content//model_{0}_bs{1}_lr{2}_epoch{3}\".format(net.name,\n",
        "                                                                    batch_size,\n",
        "                                                                    learning_rate,\n",
        "                                                                    epoch)\n",
        "    # save the parameters of the net every n epochs.\n",
        "    n=5\n",
        "    if epoch % n == 0:\n",
        "      torch.save(net.state_dict(), model_path)\n",
        "  \n",
        "  np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
        "  np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
        "  pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "  print(pytorch_total_params)\n"
      ],
      "metadata": {
        "id": "_9zSODTO1-FH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def path(name, batch_size, learning_rate, epoch):\n",
        "  model_path = \"//content//model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                                  batch_size,\n",
        "                                                                  learning_rate,\n",
        "                                                                  epoch)\n",
        "  return model_path"
      ],
      "metadata": {
        "id": "hBTIbnEgLRyP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10)\n",
        "ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10, device=device, embedding_size=200, hidden_size=200, lstm_input_size=300)\n",
        "\n",
        "train_net(ratio_net, sanity_loader, device=device, sanity_check=True, epochs=500 , learning_rate=0.001, adam=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9AW6gaz-xqc",
        "outputId": "d3005cb4-b085-47e5-c850-bc3fe8df7bd8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 0.24107037521898747, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 1, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 2, Train Loss: 0.1869876359589398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 3, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 4, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 5, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 6, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 7, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 8, Train Loss: 0.18698764275759458, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 9, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 10, Train Loss: 0.18698763120919465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 11, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 12, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 13, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 14, Train Loss: 0.18698763232678176, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 15, Train Loss: 0.1869876401498914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 16, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 17, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 18, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 19, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 20, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 21, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 22, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 23, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 24, Train Loss: 0.18698764080181718, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 25, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 26, Train Loss: 0.18698763651773334, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 27, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 28, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 29, Train Loss: 0.18698762618005277, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 30, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 31, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 32, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 33, Train Loss: 0.1869876269251108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 34, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 35, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 36, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 37, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 38, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 39, Train Loss: 0.18698764117434621, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 40, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 41, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 42, Train Loss: 0.1869876400567591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 43, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 44, Train Loss: 0.18698763195425272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2198999971151352\n",
            "Epoch: 45, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11159999668598175\n",
            "Epoch: 46, Train Loss: 0.18698764629662037, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 47, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 48, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 49, Train Loss: 0.1869876361452043, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 50, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 51, Train Loss: 0.18698763339780272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 52, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 53, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 54, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 55, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 2.1512999534606934\n",
            "Epoch: 56, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 57, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 58, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 59, Train Loss: 0.18698762892745435, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 60, Train Loss: 0.18698763865977525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 61, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 62, Train Loss: 0.1869876442477107, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 63, Train Loss: 0.1869876401498914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 64, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 65, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 66, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.052299998700618744\n",
            "Epoch: 67, Train Loss: 0.18698764080181718, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 68, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 69, Train Loss: 0.18698763446882366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042100001126527786\n",
            "Epoch: 70, Train Loss: 0.18698763167485594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 71, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 72, Train Loss: 0.18698762580752373, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 73, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 74, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042100001126527786\n",
            "Epoch: 75, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 76, Train Loss: 0.18698762808926403, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 77, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 78, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 79, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 80, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 81, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 82, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 83, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 84, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 85, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 86, Train Loss: 0.18698763521388173, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 87, Train Loss: 0.18698763577267527, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 88, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 89, Train Loss: 0.18698764350265265, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 90, Train Loss: 0.18698763977736235, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 91, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 92, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 93, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 94, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 95, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 96, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 97, Train Loss: 0.1869876286946237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 98, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 99, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 100, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 101, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 102, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 103, Train Loss: 0.18698764368891715, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 104, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 105, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 106, Train Loss: 0.18698764126747847, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 107, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 108, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 109, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 110, Train Loss: 0.18698763335123658, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 111, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 112, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 113, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 114, Train Loss: 0.18698763074353336, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 115, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 116, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 117, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 118, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 119, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 120, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 121, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 122, Train Loss: 0.1869876326993108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 123, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 124, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 125, Train Loss: 0.18698762785643339, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 126, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 127, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 128, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 129, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 130, Train Loss: 0.1869876416400075, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 131, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 132, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 2.1512999534606934\n",
            "Epoch: 133, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 134, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 135, Train Loss: 0.18698763786815106, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 136, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 137, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 138, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 139, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 140, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 141, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 142, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 143, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 144, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 145, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 146, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 147, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 148, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 149, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 150, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 151, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 152, Train Loss: 0.1869876346550882, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 153, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 154, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 155, Train Loss: 0.1869876289740205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 156, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 157, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 158, Train Loss: 0.18698764322325587, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 159, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 160, Train Loss: 0.18698763931170106, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 161, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 162, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 163, Train Loss: 0.1869876477867365, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 164, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 165, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 166, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 167, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 168, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 169, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 170, Train Loss: 0.18698763158172368, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 171, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 172, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 173, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 174, Train Loss: 0.18698763800784945, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 175, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 176, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 177, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 178, Train Loss: 0.1869876398704946, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 179, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 180, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 181, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 182, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 183, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 184, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 185, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 186, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 187, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 188, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 189, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 190, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 191, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 192, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 193, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 194, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 195, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 196, Train Loss: 0.18698764480650426, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 197, Train Loss: 0.1869876400567591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 198, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16830000281333923\n",
            "Epoch: 199, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 200, Train Loss: 0.18698763391003012, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 201, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 202, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 203, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 204, Train Loss: 0.18698764140717686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 205, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 206, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 207, Train Loss: 0.18698763567954302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 208, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 209, Train Loss: 0.1869876440614462, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 210, Train Loss: 0.1869876359589398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 211, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 212, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 213, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 214, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 215, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11159999668598175\n",
            "Epoch: 216, Train Loss: 0.18698763232678176, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 217, Train Loss: 0.18698764117434621, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 218, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 219, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 220, Train Loss: 0.1869876358192414, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 221, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 222, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 223, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 224, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 225, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 226, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 227, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 228, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 229, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 230, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 231, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 232, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 233, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 234, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 235, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 236, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 237, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 238, Train Loss: 0.18698763558641077, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 239, Train Loss: 0.18698763707652688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 240, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 241, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 242, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 243, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 244, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 245, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 246, Train Loss: 0.18698764247819782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 247, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 248, Train Loss: 0.18698763339780272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 249, Train Loss: 0.18698763800784945, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 250, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 251, Train Loss: 0.1869876422919333, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 252, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 253, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 254, Train Loss: 0.1869876289740205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 255, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 256, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 257, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 258, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 259, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 260, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 261, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 262, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 263, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 264, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 265, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 266, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 267, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 268, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 269, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 270, Train Loss: 0.18698764145374297, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 271, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 272, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 273, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 274, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 275, Train Loss: 0.1869876416400075, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 276, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 277, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 278, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 279, Train Loss: 0.1869876293465495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 280, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 281, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 282, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 283, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 284, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 285, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 286, Train Loss: 0.18698763428255916, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 287, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 288, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 289, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 290, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 291, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 292, Train Loss: 0.18698762729763985, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 293, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 294, Train Loss: 0.18698763567954302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 295, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 296, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 297, Train Loss: 0.18698762860149146, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 298, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 299, Train Loss: 0.1869876326993108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 300, Train Loss: 0.18698763800784945, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 301, Train Loss: 0.18698764257133008, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 302, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 303, Train Loss: 0.18698763782158495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 304, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 305, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 306, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 307, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 308, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 309, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 310, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 311, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 312, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 313, Train Loss: 0.18698763768188656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 314, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 315, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 316, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 317, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 318, Train Loss: 0.18698763800784945, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 319, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 320, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 321, Train Loss: 0.18698764201253654, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 322, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 323, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 324, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 325, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 326, Train Loss: 0.1869876326993108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 327, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 328, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 329, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 330, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 331, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 332, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 333, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 334, Train Loss: 0.186987633863464, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 335, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 336, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 337, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 338, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 339, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 340, Train Loss: 0.1869876267388463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 341, Train Loss: 0.18698764145374297, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 342, Train Loss: 0.18698762943968178, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 343, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 344, Train Loss: 0.18698763297870755, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 345, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 346, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 347, Train Loss: 0.1869876326993108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 348, Train Loss: 0.18698763391003012, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 349, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 350, Train Loss: 0.18698764350265265, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 351, Train Loss: 0.18698764648288488, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 352, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 353, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 354, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 355, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 356, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 357, Train Loss: 0.18698763428255916, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 358, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 359, Train Loss: 0.18698763907887042, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 360, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 361, Train Loss: 0.18698762571439148, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 362, Train Loss: 0.1869876416400075, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2198999971151352\n",
            "Epoch: 363, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 364, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 365, Train Loss: 0.18698762729763985, Validation Loss: [not yet available], prediction: 0.0, true ratio: 2.1512999534606934\n",
            "Epoch: 366, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 367, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 368, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 369, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 370, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 371, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 372, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 373, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 374, Train Loss: 0.1869876396842301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 375, Train Loss: 0.18698763037100435, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 376, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 377, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 378, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 379, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 380, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 381, Train Loss: 0.18698763120919465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 382, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 383, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 384, Train Loss: 0.18698764145374297, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 385, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 386, Train Loss: 0.18698763912543653, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 387, Train Loss: 0.18698763800784945, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 388, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 389, Train Loss: 0.18698763167485594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 390, Train Loss: 0.1869876401498914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 391, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 392, Train Loss: 0.18698763567954302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 393, Train Loss: 0.1869876326061785, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 394, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 395, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 396, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 397, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 398, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 399, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 400, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 401, Train Loss: 0.18698764247819782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 402, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 403, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 404, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 405, Train Loss: 0.18698762953281403, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 406, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 407, Train Loss: 0.18698762860149146, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 408, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 409, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 410, Train Loss: 0.18698764201253654, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 411, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 412, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 413, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 414, Train Loss: 0.18698762776330113, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 415, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 416, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 417, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 418, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 419, Train Loss: 0.18698763651773334, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 420, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 421, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 422, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 423, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 424, Train Loss: 0.18698762804269792, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 425, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 426, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 427, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 428, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 429, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 430, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 431, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 432, Train Loss: 0.18698762953281403, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 433, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 434, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 435, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 436, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 437, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 438, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 439, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.052299998700618744\n",
            "Epoch: 440, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 441, Train Loss: 0.1869876293465495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 442, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 443, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 444, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 445, Train Loss: 0.18698763875290753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 446, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 447, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 448, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 449, Train Loss: 0.1869876326993108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 450, Train Loss: 0.18698763945139946, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 451, Train Loss: 0.18698764396831394, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 452, Train Loss: 0.1869876293465495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 453, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 454, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 455, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 456, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 457, Train Loss: 0.18698764173313975, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 458, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 2.1512999534606934\n",
            "Epoch: 459, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 460, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 461, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 462, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 463, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 464, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 465, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 466, Train Loss: 0.18698763130232693, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 467, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 468, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 469, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 470, Train Loss: 0.18698762580752373, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 471, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 472, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 473, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 474, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 475, Train Loss: 0.18698763875290753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 476, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 477, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 478, Train Loss: 0.18698764257133008, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 479, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 480, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 481, Train Loss: 0.18698763195425272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 482, Train Loss: 0.18698763023130596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 483, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 484, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 485, Train Loss: 0.18698763195425272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 486, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 487, Train Loss: 0.18698763567954302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 488, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 489, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 490, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11159999668598175\n",
            "Epoch: 491, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 492, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 493, Train Loss: 0.18698763120919465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 494, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 495, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 496, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 497, Train Loss: 0.18698764136061072, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 498, Train Loss: 0.18698763409629465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 499, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "2462151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = path(ratio_net.name, 10, 0.001, 499)\n",
        "\n",
        "#model_path = \"model_RatioNet_bs10_lr0.001_epoch495\"\n",
        "print(model_path)\n",
        "plot_training_curve(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "xGeIeUqoMIGH",
        "outputId": "03427c42-dc9f-40d3-e03f-0363c0467643"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//content//model_RatioNet_bs10_lr0.001_epoch499\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10)\n",
        "ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10, device=device, embedding_size=200,num_layers=4, hidden_size=200, lstm_input_size=500)\n",
        "\n",
        "train_net(ratio_net, sanity_loader, device=device, sanity_check=True, epochs=500 , learning_rate=0.001, adam=True)\n",
        "\n",
        "model_path = path(ratio_net.name, 10, 0.001, 499)\n",
        "\n",
        "print(model_path)\n",
        "plot_training_curve(model_path)\n"
      ],
      "metadata": {
        "id": "p2rZUTunqk6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "287a1223-6a06-450b-c6a9-66bb5e6ad059"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 0.23004467319697142, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 1, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 2, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 3, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 4, Train Loss: 0.18698763130232693, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 5, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 6, Train Loss: 0.18698763272259383, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 7, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 8, Train Loss: 0.18698763977736235, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 9, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 10, Train Loss: 0.18698763158172368, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 11, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 12, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 13, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 14, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 15, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 16, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16830000281333923\n",
            "Epoch: 17, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 18, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 19, Train Loss: 0.18698762860149146, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 20, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 21, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 22, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 23, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 24, Train Loss: 0.18698762804269792, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 25, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 26, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.052299998700618744\n",
            "Epoch: 27, Train Loss: 0.18698763428255916, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 28, Train Loss: 0.1869876348413527, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 29, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 30, Train Loss: 0.18698763120919465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 31, Train Loss: 0.18698764238506554, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 32, Train Loss: 0.1869876326993108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 33, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 34, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 35, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042100001126527786\n",
            "Epoch: 36, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 37, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 38, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 39, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 40, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 41, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 42, Train Loss: 0.18698764331638812, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 43, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 44, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 45, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 46, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 47, Train Loss: 0.18698764629662037, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 48, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 49, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 50, Train Loss: 0.1869876359589398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 51, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 52, Train Loss: 0.18698763409629465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 53, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 54, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 55, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16830000281333923\n",
            "Epoch: 56, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 57, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 58, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 59, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 60, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 61, Train Loss: 0.18698764331638812, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 62, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 63, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 64, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 65, Train Loss: 0.18698763335123658, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 66, Train Loss: 0.18698764201253654, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 67, Train Loss: 0.18698763409629465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 68, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 69, Train Loss: 0.18698763567954302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 70, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 71, Train Loss: 0.18698764331638812, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 72, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 73, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 74, Train Loss: 0.18698762580752373, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 75, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 76, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 77, Train Loss: 0.1869876269251108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 78, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 79, Train Loss: 0.18698763898573817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 80, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 81, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 82, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 83, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 84, Train Loss: 0.18698763204738497, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 85, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 86, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 87, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 88, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 89, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 90, Train Loss: 0.186987647973001, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 91, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 92, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.052299998700618744\n",
            "Epoch: 93, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 94, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 95, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 96, Train Loss: 0.1869876466691494, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 97, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 98, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 99, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 100, Train Loss: 0.18698763651773334, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 101, Train Loss: 0.18698764499276876, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 102, Train Loss: 0.18698763577267527, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 103, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 104, Train Loss: 0.18698763619177045, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 105, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 106, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 107, Train Loss: 0.1869876304641366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 108, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 109, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 110, Train Loss: 0.18698763926513493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 111, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 112, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 113, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 114, Train Loss: 0.18698764257133008, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 115, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 116, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 117, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 118, Train Loss: 0.1869876304641366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 119, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 120, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 121, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16830000281333923\n",
            "Epoch: 122, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 123, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 124, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 125, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 126, Train Loss: 0.18698763875290753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 127, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 128, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 129, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 130, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 131, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 132, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 133, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 134, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 135, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 136, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 137, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042100001126527786\n",
            "Epoch: 138, Train Loss: 0.18698764443397523, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 139, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 140, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 141, Train Loss: 0.1869876298122108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 142, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 143, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 144, Train Loss: 0.18698764275759458, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 145, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 146, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 147, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 148, Train Loss: 0.18698764350265265, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 149, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 150, Train Loss: 0.18698763819411396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 151, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 152, Train Loss: 0.18698763540014623, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 153, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 154, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 155, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 156, Train Loss: 0.18698763931170106, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 157, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 158, Train Loss: 0.18698764201253654, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 159, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 160, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 161, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 162, Train Loss: 0.18698764313012362, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 163, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 164, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 165, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 166, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 167, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 168, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 169, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 170, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 171, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 172, Train Loss: 0.18698763353750109, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 173, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 174, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 175, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 176, Train Loss: 0.1869876304641366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 177, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 178, Train Loss: 0.18698762767016888, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 179, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 180, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 181, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 182, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 183, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 184, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 185, Train Loss: 0.18698764350265265, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 186, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 187, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 188, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 189, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 190, Train Loss: 0.18698764480650426, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 191, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 192, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 193, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 194, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 195, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 196, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 197, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 198, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 199, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 200, Train Loss: 0.18698763865977525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 201, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 202, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 203, Train Loss: 0.18698764266446233, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 204, Train Loss: 0.18698764517903327, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 205, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 206, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 207, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 208, Train Loss: 0.18698764480650426, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 209, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 210, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 211, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 212, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 213, Train Loss: 0.18698763912543653, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 214, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 215, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 216, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 217, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 218, Train Loss: 0.1869876401498914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 219, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 220, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 221, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 222, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 223, Train Loss: 0.1869876398704946, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 224, Train Loss: 0.18698762860149146, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 225, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 226, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 227, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 228, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 229, Train Loss: 0.1869876401498914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 230, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 231, Train Loss: 0.18698764126747847, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 232, Train Loss: 0.18698762245476247, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 233, Train Loss: 0.18698763297870755, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 234, Train Loss: 0.18698763763532042, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 235, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 236, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 237, Train Loss: 0.1869876440614462, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 238, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 239, Train Loss: 0.18698762748390435, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 240, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 241, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 242, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 243, Train Loss: 0.1869876440614462, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 244, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 245, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 246, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 247, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 248, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 249, Train Loss: 0.18698763158172368, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 250, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11159999668598175\n",
            "Epoch: 251, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 252, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 253, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 254, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 255, Train Loss: 0.18698764191940426, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 256, Train Loss: 0.1869876416400075, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 257, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 258, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 259, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 260, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 261, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 262, Train Loss: 0.18698764443397523, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 263, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 264, Train Loss: 0.18698762999847532, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 265, Train Loss: 0.1869876304641366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 266, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 267, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 268, Train Loss: 0.18698762794956564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 269, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 270, Train Loss: 0.1869876398704946, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 271, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 272, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 273, Train Loss: 0.18698763074353336, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 274, Train Loss: 0.18698763158172368, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 275, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 276, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2198999971151352\n",
            "Epoch: 277, Train Loss: 0.1869876455515623, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 278, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16830000281333923\n",
            "Epoch: 279, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 280, Train Loss: 0.18698764042928814, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 281, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 282, Train Loss: 0.1869876401498914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 283, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 284, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 285, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 286, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 287, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 288, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 289, Train Loss: 0.18698764145374297, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 290, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 291, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 292, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 293, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 294, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 295, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 296, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 297, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 298, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 299, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 300, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 301, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 302, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 303, Train Loss: 0.1869876346550882, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 304, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2198999971151352\n",
            "Epoch: 305, Train Loss: 0.18698764350265265, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 306, Train Loss: 0.18698763279244304, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 307, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 308, Train Loss: 0.18698763195425272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 309, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 310, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 311, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 312, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 313, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 314, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 315, Train Loss: 0.18698763120919465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 316, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 317, Train Loss: 0.18698762953281403, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 318, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 319, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 320, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 321, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 322, Train Loss: 0.18698764257133008, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 323, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 324, Train Loss: 0.18698764098808168, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 325, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 326, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 327, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 328, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 329, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 330, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 331, Train Loss: 0.1869876383803785, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 332, Train Loss: 0.18698764238506554, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.052299998700618744\n",
            "Epoch: 333, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 334, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 335, Train Loss: 0.18698762292042376, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 336, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 337, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 338, Train Loss: 0.18698764257133008, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 339, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11159999668598175\n",
            "Epoch: 340, Train Loss: 0.18698764480650426, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 341, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 342, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 343, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 344, Train Loss: 0.18698763428255916, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 345, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 346, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 347, Train Loss: 0.18698762841522693, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 348, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 349, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 350, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 351, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 352, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 353, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 354, Train Loss: 0.18698762813583017, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 355, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 356, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 357, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 358, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 359, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 360, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 361, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 362, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 363, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 364, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 365, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.052299998700618744\n",
            "Epoch: 366, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 367, Train Loss: 0.18698763865977525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 368, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 369, Train Loss: 0.18698764573782684, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 370, Train Loss: 0.1869876383803785, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 371, Train Loss: 0.18698763744905592, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 372, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 373, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 374, Train Loss: 0.18698762953281403, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 375, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 376, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 377, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 378, Train Loss: 0.186987632419914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 379, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 380, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 381, Train Loss: 0.1869876363314688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 382, Train Loss: 0.1869876293465495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 383, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 384, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 385, Train Loss: 0.18698763158172368, Validation Loss: [not yet available], prediction: 0.0, true ratio: 2.1512999534606934\n",
            "Epoch: 386, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 387, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 388, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 389, Train Loss: 0.1869876434095204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 390, Train Loss: 0.18698762953281403, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 391, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 392, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 393, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 394, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 395, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 396, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 397, Train Loss: 0.18698762804269792, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 398, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 399, Train Loss: 0.1869876326061785, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 400, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 401, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 402, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 403, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 404, Train Loss: 0.18698764434084297, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 405, Train Loss: 0.1869876293465495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 406, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 407, Train Loss: 0.18698763158172368, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 408, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 409, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 410, Train Loss: 0.1869876348413527, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 411, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 412, Train Loss: 0.18698764098808168, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 413, Train Loss: 0.18698764480650426, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 414, Train Loss: 0.18698764443397523, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 415, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 416, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 417, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 418, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 419, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 420, Train Loss: 0.18698764313012362, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 421, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 422, Train Loss: 0.18698764480650426, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 423, Train Loss: 0.18698763977736235, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 424, Train Loss: 0.18698764517903327, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 425, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 426, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 427, Train Loss: 0.18698762971907854, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 428, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 429, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 430, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 431, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 432, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 433, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 434, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 435, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 436, Train Loss: 0.18698764201253654, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 437, Train Loss: 0.18698762655258178, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 438, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 439, Train Loss: 0.1869876272045076, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 440, Train Loss: 0.18698762729763985, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16830000281333923\n",
            "Epoch: 441, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 442, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 443, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 444, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 445, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 446, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 447, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 448, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 449, Train Loss: 0.18698764471337198, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 450, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 451, Train Loss: 0.18698763577267527, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 452, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 453, Train Loss: 0.1869876345153898, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 454, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 455, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 456, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 457, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 458, Train Loss: 0.18698762971907854, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 459, Train Loss: 0.18698763195425272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 460, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 461, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 462, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 463, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 464, Train Loss: 0.1869876401498914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 465, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 466, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 467, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 468, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 469, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 470, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 471, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 472, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 473, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 474, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 475, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 476, Train Loss: 0.1869876416400075, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 477, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 478, Train Loss: 0.18698763577267527, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 479, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 480, Train Loss: 0.1869876260869205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 481, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 2.1512999534606934\n",
            "Epoch: 482, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16830000281333923\n",
            "Epoch: 483, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 484, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 485, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 486, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 487, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 488, Train Loss: 0.18698764257133008, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 489, Train Loss: 0.18698762767016888, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 490, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 491, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 492, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 493, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 494, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 495, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 496, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 497, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 498, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 499, Train Loss: 0.18698763297870755, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "4858751\n",
            "//content//model_RatioNet_bs10_lr0.001_epoch499\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10)\n",
        "ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10, device=device, embedding_size=50,num_layers=1, hidden_size=50, lstm_input_size=500)\n",
        "\n",
        "train_net(ratio_net, sanity_loader, device=device, sanity_check=True, epochs=200 , learning_rate=10, adam=False)\n",
        "\n",
        "model_path = path(ratio_net.name, 10, 10, 199)\n",
        "\n",
        "print(model_path)\n",
        "plot_training_curve(model_path)\n"
      ],
      "metadata": {
        "id": "2ki32v8-q5wJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75fd04e6-d610-45c3-b51a-848d0dcb6d00"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 19.93190265931189, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 1, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 2, Train Loss: 0.18698764517903327, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 3, Train Loss: 0.18698763232678176, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 4, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 5, Train Loss: 0.18698763865977525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 6, Train Loss: 0.18698764275759458, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 7, Train Loss: 0.18698763232678176, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 8, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 9, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 10, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 11, Train Loss: 0.18698764145374297, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 12, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 13, Train Loss: 0.1869876421056688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 14, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 15, Train Loss: 0.18698762729763985, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 16, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 17, Train Loss: 0.18698762999847532, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 18, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 19, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 20, Train Loss: 0.18698763232678176, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 21, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 22, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 23, Train Loss: 0.1869876361452043, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 24, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 25, Train Loss: 0.18698762841522693, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 26, Train Loss: 0.18698763279244304, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 27, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 28, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 29, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 30, Train Loss: 0.1869876440614462, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 31, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 32, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 33, Train Loss: 0.18698763009160757, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 34, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 35, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 36, Train Loss: 0.18698763158172368, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 37, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 38, Train Loss: 0.1869876434095204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 39, Train Loss: 0.1869876326993108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 40, Train Loss: 0.1869876362849027, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 41, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 42, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2198999971151352\n",
            "Epoch: 43, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 44, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 45, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 46, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 47, Train Loss: 0.18698763521388173, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 48, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 49, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 50, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 51, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 52, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 53, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 54, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 55, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 56, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 57, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 58, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 59, Train Loss: 0.18698764890432357, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 60, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 61, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 62, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 63, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 64, Train Loss: 0.18698764247819782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.2827999591827393\n",
            "Epoch: 65, Train Loss: 0.18698763865977525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 66, Train Loss: 0.18698762953281403, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 67, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 68, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 69, Train Loss: 0.18698762841522693, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 70, Train Loss: 0.1869876322336495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 71, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 72, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 73, Train Loss: 0.18698762785643339, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 74, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 75, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 76, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 77, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 78, Train Loss: 0.1869876269251108, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 79, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 80, Train Loss: 0.18698764350265265, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 81, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 82, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 83, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 84, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 85, Train Loss: 0.18698762338608504, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 86, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 87, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 88, Train Loss: 0.186987638566643, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 89, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 90, Train Loss: 0.18698762971907854, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16830000281333923\n",
            "Epoch: 91, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 92, Train Loss: 0.18698763344436883, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 93, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 94, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 95, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 96, Train Loss: 0.1869876405224204, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 97, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 98, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 99, Train Loss: 0.186987630976364, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 100, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 101, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 102, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 103, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 104, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 105, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 106, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 107, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 108, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 109, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 110, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 111, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 112, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 113, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 114, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 115, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 116, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.017100000753998756\n",
            "Epoch: 117, Train Loss: 0.18698764080181718, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 118, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 119, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 120, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 121, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 122, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.061000000685453415\n",
            "Epoch: 123, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 124, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 125, Train Loss: 0.1869876254349947, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5956000089645386\n",
            "Epoch: 126, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 127, Train Loss: 0.1869876448996365, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 128, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 129, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 130, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 131, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 132, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 133, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 134, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 135, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 136, Train Loss: 0.18698763195425272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 137, Train Loss: 0.186987641826272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 138, Train Loss: 0.1869876364246011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 139, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 140, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 141, Train Loss: 0.18698763782158495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 142, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 143, Train Loss: 0.18698764638975263, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 144, Train Loss: 0.18698763009160757, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 145, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 146, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 147, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 148, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 149, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 150, Train Loss: 0.18698763120919465, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 151, Train Loss: 0.18698763009160757, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 152, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 153, Train Loss: 0.18698763912543653, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 154, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 155, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 156, Train Loss: 0.1869876256212592, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 157, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 158, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2198999971151352\n",
            "Epoch: 159, Train Loss: 0.18698763074353336, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 160, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.052299998700618744\n",
            "Epoch: 161, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 162, Train Loss: 0.18698763670399784, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 163, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 164, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 165, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 166, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 167, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 168, Train Loss: 0.18698764112778007, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 169, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 170, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07270000129938126\n",
            "Epoch: 171, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 172, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 173, Train Loss: 0.1869876380544156, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 174, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 175, Train Loss: 0.18698764089494943, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 176, Train Loss: 0.1869876346550882, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 177, Train Loss: 0.1869876346550882, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 178, Train Loss: 0.18698763428255916, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 179, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 180, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 181, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 182, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 183, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 184, Train Loss: 0.18698764881119131, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 185, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 186, Train Loss: 0.1869876289740205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 187, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 188, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 189, Train Loss: 0.18698762685526163, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 190, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 191, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029100000858306885\n",
            "Epoch: 192, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 193, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 194, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 195, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 196, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 197, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 198, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 199, Train Loss: 0.18698763707652688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "826751\n",
            "//content//model_RatioNet_bs10_lr10_epoch199\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfvElEQVR4nO3de5RcZZnv8e+vOyHBJNySJkBCCCiTI3gk4fQBUXSCKAIieIVkWBoUVyYsGWWpg1xUGEdneTl6FKMycYiABxAVozlHUAIDAqMITSZAuJmAydghJE3AJMo15Dl/7LeSXVW7Ot2d3lWd5PdZq1bv/e7b07ur66n9vnu/ryICMzOzWm2tDsDMzIYmJwgzMyvkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QNmRJuknSrFbHMRCSrpT0xTT9ZkmP9WXdAR7rL5IOGej2Zo04QdigSh9WlddmSc/n5s/sz74i4qSIuKqsWHsjaYakFZJUUz5M0lpJp/R1XxFxZ0RMGaS4bpf00Zr9j46IJwZj/zXHWiHpbYO9X9txOEHYoEofVqMjYjTwX8C7cmXXVNaTNKx1UfbJz4G9gL+tKT8RCOBXTY/IrMmcIKwpJE2X1C3pM5KeAn4gaW9J/09Sj6Rn0/TE3DZbvi1LOkvSXZL+V1r3j5JOanCsz0j6aU3ZtyRdltvXE5I2pv3UXdlExAvAj4EP1Sz6EHBtRGyS9BNJT0laL+kOSYf39rvn5qdJWpyOfz0wMres4TmR9CXgzcDcdEU2N5WHpNek6T0lXZ22Xynps5La+nsOeyNphKRvSnoyvb4paURaNi7F/GdJz0i6M3f8z0halX7vxyQd399jW3M5QVgz7QfsAxwEzCZ7//0gzU8Cngfm9rL90cBjwDjgq8AVtVVAyY+AkyWNAZDUDpwOXCtpFHAZcFJEjAHeCCxpcLyrgPdL2j3tZ0/gXakc4CbgUGBfYDFwTdFO8iTtRnZ18kOyc/ET4H25VRqek4i4GLgTODddkZ1bcIhvA3sCh5Bd/XwI+HBueV/PYW8uBt4ATAWOAI4CPpuWfQroBjqA8cBFQEiaApwL/M903t8BrOjnca3JnCCsmTYDl0TEixHxfESsi4gbIuK5iNgIfIn6Kp28lRHx/Yh4hexDen+yD6EqEbGS7AP7PanorcBzEXF3Lo7XSdo9IlZHxENFB4uI/wDW5PZzOvCHiFiSls+PiI0R8SJwKXBESiK9eQMwHPhmRLwcET8F7s0ds7/nZIuUCGcAF6a4VgBfBz6YW61P53AbzgS+EBFrI6IH+KfcMV5O+zwo/X53Rtbh2yvACOAwScMjYkVEPN7P41qTOUFYM/WkqhsAJL1K0r+mqpANwB3AXumDrshTlYmIeC5Njm6w7rXAzDT9d2meiPgrcAYwB1gt6ZeS/lsvMV/N1mqmD6Z5JLVL+rKkx1PsK9I643rZF8ABwKqo7iVzZWViAOckbxxZ8lmZK1sJTMjN9+cc9vY71B7jgDT9NWA5cHOqxrsgHWs5cB5ZIl0r6UeSDsCGNCcIa6baroM/BUwBjo6IPYC3pPL+VnkU+QkwPdXfv4eUIAAi4tcR8Xayb7qPAt/vZT8/BI6XdAzZt/9KNdLfAacBbyOr0pncx9hXAxNqqnUm5aa3dU566375abJv8AfV7HvVNmLqrycLjvEkQLpy+VREHAKcCnyy0tYQEddGxLFp2wC+Mshx2SBzgrBWGkNWx/5nSfsAlwzWjlPVx+1k9fl/jIhHACSNl3Raaot4EfgLWZVTo/2sAO4CrgMWRUTlG/iYtP064FXAv/QxtN8Bm4CPSxou6b1kdfgV2zona8jaF4pifYWsYf1LksZIOgj4JPB/+hhbkeGSRuZew8jOxWcldUgaB3y+cgxJp0h6TUqA68mqljZLmiLprakx+4X0OzY87zY0OEFYK30T2J3sm+/dDP6to9eSfcO/NlfWRvah+STwDFn9/jnb2M9VZN96r86VXU1WtbIKeJgs/m2KiJeA9wJnpeOfAfwst8q2zsm3yBrOn63clVXjH4C/Ak+QJbZrgfl9ia2BG8k+zCuvS4EvAl3AA8CDZO09lQf9DgVuIUu8vwO+GxG3kbU/fDn9Xk+RNexfuB1xWRPIAwaZmVkRX0GYmVkhJwgzMyvkBGFmZoWcIMzMrNBQ7zCtX8aNGxeTJ09udRhmZjuM++677+mI6ChatlMliMmTJ9PV1dXqMMzMdhiSVjZa5iomMzMr5ARhZmaFnCDMzKzQTtUGYWbWHy+//DLd3d288MIL2155Bzdy5EgmTpzI8OHD+7xNaQlC0oFk/dWMJ+u5cV5EfCt1QHY9We+XK4DTI+LZgu1nsXUQki+2amxiM9t5dXd3M2bMGCZPnkz/x03acUQE69ato7u7m4MPPrjP25VZxbQJ+FREHEbWTfLHJB0GXADcGhGHArem+Sq5XiyPJuvp8hJJe5cYq5ntgl544QXGjh27UycHAEmMHTu231dKpSWINFLX4jS9EXiEbOCS09g6ZONVwLsLNn8HWdfKz6Sri0Vkg8WbmQ2qnT05VAzk92xKI7WkycA04PfA+IhYnRY9RfFwhxOAP+Xmu6keFWtQffvWZfzmDz1l7d7MbIdUeoKQNBq4ATgvIjbkl6VhF7erv3FJsyV1Serq6RnYh/x3b3+cu5Y5QZhZc61bt46pU6cydepU9ttvPyZMmLBl/qWXXup1266uLj7+8Y+XGl+pdzFJGk6WHK6JiMqgKGsk7R8RqyXtD6wt2HQVMD03P5FsdLA6ETEPmAfQ2dk5oGTTJvCwGGbWbGPHjmXJkiUAXHrppYwePZpPf/rTW5Zv2rSJYcOKP6Y7Ozvp7OwsNb7SriDSkINXAI9ExDdyixYCs9L0LOAXBZv/GjhB0t6pcfqEVFaKNonNThBmNgScddZZzJkzh6OPPprzzz+fe+65h2OOOYZp06bxxje+kcceewyA22+/nVNOOQXIkstHPvIRpk+fziGHHMJllxUNNth/ZV5BvAn4IPCgpCWp7CKyYQd/LOlssiEbTweQ1AnMiYiPRsQzkv4ZuDdt94WIeKasQCXY7EsIs13aP/3fh3j4yQ3bXrEfDjtgDy551+H93q67u5vf/va3tLe3s2HDBu68806GDRvGLbfcwkUXXcQNN9xQt82jjz7KbbfdxsaNG5kyZQrnnHNOv555KFJagoiIu4BGzebHF6zfBXw0Nz+f7RtLt8/a2oSHXjWzoeIDH/gA7e3tAKxfv55Zs2axbNkyJPHyyy8XbvPOd76TESNGMGLECPbdd1/WrFnDxIkTtysOP0mNq5jMjAF90y/LqFGjtkx/7nOf47jjjmPBggWsWLGC6dOnF24zYsSILdPt7e1s2rRpu+NwX0xkjdSuYjKzoWj9+vVMmJDd5X/llVc29dhOEGQPkPgKwsyGovPPP58LL7yQadOmDcpVQX9oZ6p77+zsjIEMGHT0v9zCcVP25cvve30JUZnZUPXII4/w2te+ttVhNE3R7yvpvogovF/WVxBU2iB2nkRpZjYYnCBwI7WZWREnCPwchNmubGeqZu/NQH5PJwiyK4hd5D1iZjkjR45k3bp1O32SqIwHMXLkyH5t5+cg8G2uZruqiRMn0t3dzUA7+tyRVEaU6w8nCNwGYbarGj58eL9GWNvVuIoJt0GYmRVxgqDSBuEEYWaW5wRBqmLa3OoozMyGFicIXMVkZlbECQI3UpuZFXGCANradp2HZczM+qq021wlzQdOAdZGxOtS2fXAlLTKXsCfI2JqwbYrgI3AK8CmRh1JDRb3xWRmVq/M5yCuBOYCV1cKIuKMyrSkrwPre9n+uIh4urToctzdt5lZvTKHHL1D0uSiZZJENhb1W8s6fn/4SWozs3qtaoN4M7AmIpY1WB7AzZLukzS77GDcF5OZWb1WdbUxE7iul+XHRsQqSfsCiyQ9GhF3FK2YEshsgEmTJg0oGF9BmJnVa/oVhKRhwHuB6xutExGr0s+1wALgqF7WnRcRnRHR2dHRMdCYnCDMzGq0oorpbcCjEdFdtFDSKEljKtPACcDSMgPKriDKPIKZ2Y6ntAQh6Trgd8AUSd2Szk6LZlBTvSTpAEk3ptnxwF2S7gfuAX4ZEb8qK05wX0xmZkXKvItpZoPyswrKngROTtNPAEeUFVcRP0ltZlbPT1LjvpjMzIo4QeArCDOzIk4QZI3UboMwM6vmBIH7YjIzK+IEQXoOwgMGmZlVcYLAT1KbmRVxgsB9MZmZFXGCIBswyFcQZmbVnCBwX0xmZkWcIHAVk5lZEScI3EhtZlbECQI/SW1mVsQJAvfFZGZWxAkCt0GYmRVxgsBtEGZmRZwgcF9MZmZFyhxRbr6ktZKW5soulbRK0pL0OrnBtidKekzSckkXlBVj7niuYjIzq1HmFcSVwIkF5f87Iqam1421CyW1A98BTgIOA2ZKOqzEOD0mtZlZgdISRETcATwzgE2PApZHxBMR8RLwI+C0QQ2uhsekNjOr14o2iHMlPZCqoPYuWD4B+FNuvjuVlcaN1GZm9ZqdIL4HvBqYCqwGvr69O5Q0W1KXpK6enp6B7sNVTGZmNZqaICJiTUS8EhGbge+TVSfVWgUcmJufmMoa7XNeRHRGRGdHR8eA4vKDcmZm9ZqaICTtn5t9D7C0YLV7gUMlHSxpN2AGsLDMuPygnJlZvWFl7VjSdcB0YJykbuASYLqkqUAAK4C/T+seAPxbRJwcEZsknQv8GmgH5kfEQ2XFCW6DMDMrUlqCiIiZBcVXNFj3SeDk3PyNQN0tsGXxg3JmZvX8JDVupDYzK+IEQVbF5OcgzMyqOUHg8SDMzIo4QeBGajOzIk4QbO2sz9VMZmZbOUGQVTEBfhbCzCzHCYKsiglczWRmlucEAbSlDOGGajOzrZwgyPpiAl9BmJnlOUHgNggzsyJOELgNwsysiBMEW68gnCDMzLZygiB7DgLcSG1mlucEwdYqJj8oZ2a2lRME+SqmFgdiZjaEOEHgRmozsyKlJQhJ8yWtlbQ0V/Y1SY9KekDSAkl7Ndh2haQHJS2R1FVWjLnjAU4QZmZ5ZV5BXAmcWFO2CHhdRLwe+ANwYS/bHxcRUyOis6T4tvBzEGZm9UpLEBFxB/BMTdnNEbEpzd4NTCzr+P3hKiYzs3qtbIP4CHBTg2UB3CzpPkmzyw7EjdRmZvWGteKgki4GNgHXNFjl2IhYJWlfYJGkR9MVSdG+ZgOzASZNmjTAeLKfm50hzMy2aPoVhKSzgFOAM6PBgwcRsSr9XAssAI5qtL+ImBcRnRHR2dHRMaCY3AZhZlavqQlC0onA+cCpEfFcg3VGSRpTmQZOAJYWrTtY2tJZcBuEmdlWZd7meh3wO2CKpG5JZwNzgTFk1UZLJF2e1j1A0o1p0/HAXZLuB+4BfhkRvyorTnBfTGZmRUprg4iImQXFVzRY90ng5DT9BHBEWXEVcV9MZmb1/CQ17ovJzKyIEwS+zdXMrIgTBH5QzsysiBME7ovJzKyIEwR+DsLMrIgTBK5iMjMr4gSBG6nNzIo4QZDri8lXEGZmWzhBkG+DcIIwM6twgsBVTGZmRZwgyDVSO0OYmW3RpwSRelhtS9N/I+lUScPLDa153BeTmVm9vl5B3AGMlDQBuBn4INmY0zuFLX0x4QxhZlbR1wShNH7De4HvRsQHgMPLC6u52tr8oJyZWa0+JwhJxwBnAr9MZe3lhNR8flDOzKxeXxPEecCFwIKIeEjSIcBt5YXVXG6DMDOr16cBgyLiN8BvAFJj9dMR8fEyA2smjyhnZlavr3cxXStpjzRG9FLgYUn/2Ift5ktaK2lprmwfSYskLUs/926w7ay0zjJJs/r6Cw2EBwwyM6vX1yqmwyJiA/Bu4CbgYLI7mbblSuDEmrILgFsj4lDg1jRfRdI+wCXA0cBRwCWNEslg2HIFsbmsI5iZ7Xj6miCGp+ce3g0sjIiXYdv3hEbEHcAzNcWnAVel6avSPmu9A1gUEc9ExLPAIuoTzaBxX0xmZvX6miD+FVgBjALukHQQsGGAxxwfEavT9FPA+IJ1JgB/ys13p7I6kmZL6pLU1dPTM6CA3NWGmVm9PiWIiLgsIiZExMmRWQkct70Hj6zSf7s+liNiXkR0RkRnR0fHgPbhzvrMzOr1tZF6T0nfqHxTl/R1squJgVgjaf+03/2BtQXrrAIOzM1PTGWl2FrFVNYRzMx2PH2tYpoPbAROT68NwA8GeMyFQOWupFnALwrW+TVwgqS9U+P0CamsFH5QzsysXp+egwBeHRHvy83/k6Ql29pI0nXAdGCcpG6yO5O+DPxY0tnASrKEg6ROYE5EfDQinpH0z8C9aVdfiIjaxu5BIz8HYWZWp68J4nlJx0bEXQCS3gQ8v62NImJmg0XHF6zbBXw0Nz+f7MqldFvbIJpxNDOzHUNfE8Qc4GpJe6b5Z9laTbTDcxWTmVm9vna1cT9whKQ90vwGSecBD5QZXLP4Nlczs3r9GlEuIjakJ6oBPllCPC3hB+XMzOptz5CjGrQoWszPQZiZ1dueBLHTfJq6isnMrF6vbRCSNlKcCATsXkpELeBGajOzer0miIgY06xAWskDBpmZ1dueKqadhseDMDOr5wRBfjwIJwgzswonCNxIbWZWxAkCUDoLbqQ2M9vKCQL3xWRmVsQJAt/mamZWxAkCt0GYmRVxgsB9MZmZFXGCwH0xmZkVaXqCkDRF0pLcq9J1eH6d6ZLW59b5fJkxuYrJzKxeXwcMGjQR8RgwFUBSO7AKWFCw6p0RcUozYnIjtZlZvVZXMR0PPB4RK1sZhPtiMjOr1+oEMQO4rsGyYyTdL+kmSYc32oGk2ZK6JHX19PQMOJA2uQ3CzCyvZQlC0m7AqcBPChYvBg6KiCOAbwM/b7SfiJgXEZ0R0dnR0THgeNokVzGZmeW08griJGBxRKypXZCGNv1Lmr4RGC5pXJnBZAmizCOYme1YWpkgZtKgeknSfkoNA5KOIotzXZnBSG6kNjPLa/pdTACSRgFvB/4+VzYHICIuB94PnCNpE/A8MCNKbiBok9wXk5lZTksSRET8FRhbU3Z5bnouMLeZMbXJ40GYmeW1+i6mIcNtEGZm1ZwgErdBmJlVc4JI2trk5yDMzHKcIBJXMZmZVXOCSNpcxWRmVsUJIpGvIMzMqjhBJFmPrs4QZmYVThBJm8Tmza2Owsxs6HCCSNxZn5lZNSeIJHsOotVRmJkNHU4QSdYXkzOEmVmFE0Ti21zNzKo5QSR+UM7MrJoTROK+mMzMqjlBJB4PwsysmhNE4ttczcyqtSxBSFoh6UFJSyR1FSyXpMskLZf0gKQjy43HVUxmZnktGVEu57iIeLrBspOAQ9PraOB76Wcp3EhtZlZtKFcxnQZcHZm7gb0k7V/Wwdra8HMQZmY5rUwQAdws6T5JswuWTwD+lJvvTmVVJM2W1CWpq6enZ8DB+ArCzKxaKxPEsRFxJFlV0sckvWUgO4mIeRHRGRGdHR0dAw5GbqQ2M6vSsgQREavSz7XAAuComlVWAQfm5iemslK0uS8mM7MqLUkQkkZJGlOZBk4AltasthD4ULqb6Q3A+ohYXVZM7ovJzKxaq+5iGg8skFSJ4dqI+JWkOQARcTlwI3AysBx4DvhwmQEJ3+ZqZpbXkgQREU8ARxSUX56bDuBjzYrJAwaZmVUbyre5NpUflDMzq+YEkbgvJjOzak4QSVubryDMzPKcIBJ31mdmVs0JIpGfpDYzq+IEkbTJfTGZmeU5QSTui8nMrJoTRNLm21zNzKo4QSRugzAzq+YEkbgNwsysmhNE4ttczcyqOUEkbqQ2M6vmBJG4LyYzs2pOEIn7YjIzq+YEkfg2VzOzak4QiRupzcyqNT1BSDpQ0m2SHpb0kKRPFKwzXdJ6SUvS6/NNiMsDBpmZ5bRiRLlNwKciYnEal/o+SYsi4uGa9e6MiFOaFZSfgzAzq9b0K4iIWB0Ri9P0RuARYEKz46jl21zNzKq1tA1C0mRgGvD7gsXHSLpf0k2SDu9lH7MldUnq6unpGXAsHjDIzKxayxKEpNHADcB5EbGhZvFi4KCIOAL4NvDzRvuJiHkR0RkRnR0dHdsTj68gzMxyWpIgJA0nSw7XRMTPapdHxIaI+EuavhEYLmlcmTG5DcLMrFor7mIScAXwSER8o8E6+6X1kHQUWZzryozLt7mamVVrxV1MbwI+CDwoaUkquwiYBBARlwPvB86RtAl4HpgRJX+9dyO1mVm1pieIiLgL0DbWmQvMbU5EGffFZGZWzU9SJ+6LycysmhNE4kZqM7NqThCJ2yDMzKo5QSTyXUxmZlWcIJKsiqnVUZiZDR1OEImfgzAzq+YEkXjAIDOzak4QiftiMjOr5gSRtGU9e/hWVzOzxAkiaUvPdvsqwsws4wSRtKUM4XYIM7OME0SiLVcQThBmZuAEscXWNogWB2JmNkQ4QSRtvoIwM6viBJFUriDcSG1mlnGCSCQ3UpuZ5bViRDkknQh8C2gH/i0ivlyzfARwNfA/yIYaPSMiVpQZU6WK6d3f+Q/a1et4RmZmQ8rer9qNH885ZtD32/QEIakd+A7wdqAbuFfSwoh4OLfa2cCzEfEaSTOArwBnlBnX9Cn78p//9Wc2bd5c5mHMzAbdHiOHl7LfVlxBHAUsj4gnACT9CDgNyCeI04BL0/RPgbmSVOa41AePG8VlM6eVtXszsx1OK9ogJgB/ys13p7LCdSJiE7AeGFu0M0mzJXVJ6urp6SkhXDOzXdMO30gdEfMiojMiOjs6OlodjpnZTqMVCWIVcGBufmIqK1xH0jBgT7LGajMza5JWJIh7gUMlHSxpN2AGsLBmnYXArDT9fuDfy2x/MDOzek1vpI6ITZLOBX5Ndpvr/Ih4SNIXgK6IWAhcAfxQ0nLgGbIkYmZmTdSS5yAi4kbgxpqyz+emXwA+0Oy4zMxsqx2+kdrMzMrhBGFmZoW0M7X9SuoBVvZzs3HA0yWEMxiGamyOq38cV/8N1dh2xrgOiojCZwR2qgQxEJK6IqKz1XEUGaqxOa7+cVz9N1Rj29XichWTmZkVcoIwM7NCThAwr9UB9GKoxua4+sdx9d9QjW2XimuXb4MwM7NivoIwM7NCThBmZlZol04Qkk6U9Jik5ZIuaGEcB0q6TdLDkh6S9IlUfqmkVZKWpNfJLYhthaQH0/G7Utk+khZJWpZ+7t3kmKbkzskSSRskndeq8yVpvqS1kpbmygrPkTKXpffcA5KObHJcX5P0aDr2Akl7pfLJkp7PnbvLmxxXw7+dpAvT+XpM0juaHNf1uZhWSFqSypt5vhp9PpT/HouIXfJF1lHg48AhwG7A/cBhLYplf+DIND0G+ANwGNmoep9u8XlaAYyrKfsqcEGavgD4Sov/jk8BB7XqfAFvAY4Elm7rHAEnAzcBAt4A/L7JcZ0ADEvTX8nFNTm/XgvOV+HfLv0f3A+MAA5O/7PtzYqrZvnXgc+34Hw1+nwo/T22K19BbBn6NCJeAipDnzZdRKyOiMVpeiPwCPWj7A0lpwFXpemrgHe3MJbjgccjor9P0A+aiLiDrNfhvEbn6DTg6sjcDewlaf9mxRURN0c2SiPA3WTjsTRVg/PVyGnAjyLixYj4I7Cc7H+3qXFJEnA6cF0Zx+5NL58Ppb/HduUE0ZehT5tO0mRgGvD7VHRuukyc3+yqnCSAmyXdJ2l2KhsfEavT9FPA+BbEVTGD6n/aVp+vikbnaCi97z5C9k2z4mBJ/ynpN5Le3IJ4iv52Q+V8vRlYExHLcmVNP181nw+lv8d25QQx5EgaDdwAnBcRG4DvAa8GpgKryS5xm+3YiDgSOAn4mKS35BdGdk3bknullQ04dSrwk1Q0FM5XnVaeo0YkXQxsAq5JRauBSRExDfgkcK2kPZoY0pD82+XMpPqLSNPPV8HnwxZlvcd25QTRl6FPm0bScLI//jUR8TOAiFgTEa9ExGbg+5R0ad2biFiVfq4FFqQY1lQuWdPPtc2OKzkJWBwRa1KMLT9fOY3OUcvfd5LOAk4BzkwfLKQqnHVp+j6yuv6/aVZMvfzthsL5Gga8F7i+Utbs81X0+UAT3mO7coLoy9CnTZHqN68AHomIb+TK8/WG7wGW1m5bclyjJI2pTJM1cC6lekjYWcAvmhlXTtW3ulafrxqNztFC4EPpTpM3AOtz1QSlk3QicD5wakQ8lyvvkNSepg8BDgWeaGJcjf52C4EZkkZIOjjFdU+z4kreBjwaEd2Vgmaer0afDzTjPdaMVvih+iJr7f8DWfa/uIVxHEt2efgAsCS9TgZ+CDyYyhcC+zc5rkPI7iC5H3ioco6AscCtwDLgFmCfFpyzUcA6YM9cWUvOF1mSWg28TFbfe3ajc0R2Z8l30nvuQaCzyXEtJ6ufrrzPLk/rvi/9jZcAi4F3NTmuhn874OJ0vh4DTmpmXKn8SmBOzbrNPF+NPh9Kf4+5qw0zMyu0K1cxmZlZL5wgzMyskBOEmZkVcoIwM7NCThBmZlbICcKsHyS9ouqeZAetF+DUQ2grn90wqzKs1QGY7WCej4iprQ7CrBl8BWE2CNJYAV9VNnbGPZJek8onS/r31AndrZImpfLxysZjuD+93ph21S7p+6nf/5sl7d6yX8p2eU4QZv2ze00V0xm5Zesj4r8Dc4FvprJvA1dFxOvJOsa7LJVfBvwmIo4gG4PgoVR+KPCdiDgc+DPZE7tmLeEnqc36QdJfImJ0QfkK4K0R8UTqWO2piBgr6WmybiNeTuWrI2KcpB5gYkS8mNvHZGBRRBya5j8DDI+IL5b/m5nV8xWE2eCJBtP98WJu+hXcTmgt5ARhNnjOyP38XZr+LVlPwQBnAnem6VuBcwAktUvas1lBmvWVv52Y9c/uSgPXJ7+KiMqtrntLeoDsKmBmKvsH4AeS/hHoAT6cyj8BzJN0NtmVwjlkPYmaDRlugzAbBKkNojMinm51LGaDxVVMZmZWyFcQZmZWyFcQZmZWyAnCzMwKOUGYmVkhJwgzMyvkBGFmZoX+P/Kft91pzu1aAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10)\n",
        "ratio_net = no_conv_RatioNet(input_size=len(tokenised_data.vocab), batch_size=10, device=device, embedding_size=50,num_layers=1, hidden_size=50, lstm_input_size=50)\n",
        "\n",
        "train_net(ratio_net, sanity_loader, device=device, sanity_check=True, epochs=200 , learning_rate=0.01, adam=False)\n",
        "\n",
        "model_path = path(ratio_net.name, 10, 0.01, 199)\n",
        "\n",
        "print(model_path)\n",
        "plot_training_curve(model_path)\n"
      ],
      "metadata": {
        "id": "2YewFiEBumqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "788f73e2-ca54-466c-c1d9-019c662a2c82"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 0.15203523626551033, Validation Loss: [not yet available], prediction: 0.33548691868782043, true ratio: 0.006300000008195639\n",
            "Epoch: 1, Train Loss: 0.12839226890355349, Validation Loss: [not yet available], prediction: 0.3302033245563507, true ratio: 0.04399999976158142\n",
            "Epoch: 2, Train Loss: 0.14084033537656068, Validation Loss: [not yet available], prediction: 0.22666576504707336, true ratio: 0.28529998660087585\n",
            "Epoch: 3, Train Loss: 0.14029381135478616, Validation Loss: [not yet available], prediction: 0.22291621565818787, true ratio: 0.07270000129938126\n",
            "Epoch: 4, Train Loss: 0.1854032076895237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 5, Train Loss: 0.18698764145374297, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 6, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 7, Train Loss: 0.18698764247819782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 8, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 9, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042100001126527786\n",
            "Epoch: 10, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 11, Train Loss: 0.18698764964938164, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 12, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 13, Train Loss: 0.18698763754218817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 14, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 15, Train Loss: 0.1869876322336495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 16, Train Loss: 0.1869784075766802, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 17, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 18, Train Loss: 0.18687619231641292, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 19, Train Loss: 0.1869876398704946, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2198999971151352\n",
            "Epoch: 20, Train Loss: 0.18698005601763726, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 21, Train Loss: 0.18659528912976384, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 22, Train Loss: 0.18567218855023385, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 23, Train Loss: 0.18467151559889317, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 24, Train Loss: 0.18025026619434356, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 25, Train Loss: 0.18087813258171082, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 26, Train Loss: 0.18165966495871544, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 27, Train Loss: 0.18105610273778439, Validation Loss: [not yet available], prediction: 0.2931879758834839, true ratio: 1.2827999591827393\n",
            "Epoch: 28, Train Loss: 0.17777454294264317, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 29, Train Loss: 0.18042357331141828, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 30, Train Loss: 0.18092936128377915, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 31, Train Loss: 0.1808122733607888, Validation Loss: [not yet available], prediction: 0.28810781240463257, true ratio: 0.21119999885559082\n",
            "Epoch: 32, Train Loss: 0.18090459555387498, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 33, Train Loss: 0.17969763958826662, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 34, Train Loss: 0.17959357239305973, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 35, Train Loss: 0.1805690847337246, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 36, Train Loss: 0.18083392940461634, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 37, Train Loss: 0.18000339418649675, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 38, Train Loss: 0.18039359897375107, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 39, Train Loss: 0.18012096416205167, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 40, Train Loss: 0.1778858033940196, Validation Loss: [not yet available], prediction: 0.20608088374137878, true ratio: 0.5540000200271606\n",
            "Epoch: 41, Train Loss: 0.18065045047551392, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 42, Train Loss: 0.18054490387439728, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 43, Train Loss: 0.17913518007844687, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 44, Train Loss: 0.18047574236989022, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 45, Train Loss: 0.1799254696816206, Validation Loss: [not yet available], prediction: 0.2526944577693939, true ratio: 0.026200000196695328\n",
            "Epoch: 46, Train Loss: 0.1804208504036069, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 47, Train Loss: 0.1802564799785614, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 48, Train Loss: 0.17989728432148694, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 49, Train Loss: 0.18035307042300702, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 50, Train Loss: 0.18056778870522977, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 51, Train Loss: 0.18052409654483198, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 52, Train Loss: 0.17945109643042087, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 53, Train Loss: 0.18054854050278663, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.052299998700618744\n",
            "Epoch: 54, Train Loss: 0.18028616067022085, Validation Loss: [not yet available], prediction: 0.24228793382644653, true ratio: 0.026200000196695328\n",
            "Epoch: 55, Train Loss: 0.17896924465894698, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 56, Train Loss: 0.1804873187094927, Validation Loss: [not yet available], prediction: 0.293763667345047, true ratio: 0.953499972820282\n",
            "Epoch: 57, Train Loss: 0.17960574589669703, Validation Loss: [not yet available], prediction: 0.22848056256771088, true ratio: 0.029200000688433647\n",
            "Epoch: 58, Train Loss: 0.17975655384361744, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 59, Train Loss: 0.1805839380249381, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 60, Train Loss: 0.1811269880272448, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 61, Train Loss: 0.17880479600280524, Validation Loss: [not yet available], prediction: 0.22805985808372498, true ratio: 0.026200000196695328\n",
            "Epoch: 62, Train Loss: 0.18053010683506726, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 63, Train Loss: 0.18040471579879522, Validation Loss: [not yet available], prediction: 0.28961440920829773, true ratio: 0.6384999752044678\n",
            "Epoch: 64, Train Loss: 0.1801550466567278, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 65, Train Loss: 0.18075495176017284, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 66, Train Loss: 0.18030885942280292, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 67, Train Loss: 0.18045434895902873, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 68, Train Loss: 0.17961304895579816, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 69, Train Loss: 0.18031938672065734, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 70, Train Loss: 0.18020935878157615, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 71, Train Loss: 0.18078302666544915, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 72, Train Loss: 0.18027590587735176, Validation Loss: [not yet available], prediction: 0.0, true ratio: 2.1512999534606934\n",
            "Epoch: 73, Train Loss: 0.18008348401635885, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 74, Train Loss: 0.18042297065258026, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 75, Train Loss: 0.17927579823881387, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 76, Train Loss: 0.18040761016309262, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 77, Train Loss: 0.17790974751114846, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 78, Train Loss: 0.18044709488749505, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 79, Train Loss: 0.1801026575267315, Validation Loss: [not yet available], prediction: 0.26332738995552063, true ratio: 0.07270000129938126\n",
            "Epoch: 80, Train Loss: 0.17963345674797893, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 81, Train Loss: 0.17865981496870517, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 82, Train Loss: 0.17992931939661502, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 83, Train Loss: 0.18047594726085664, Validation Loss: [not yet available], prediction: 0.29121658205986023, true ratio: 0.374099999666214\n",
            "Epoch: 84, Train Loss: 0.18013209402561187, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 85, Train Loss: 0.1802292766980827, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 86, Train Loss: 0.18035116661339998, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 87, Train Loss: 0.18003444708883762, Validation Loss: [not yet available], prediction: 0.252703994512558, true ratio: 0.5540000200271606\n",
            "Epoch: 88, Train Loss: 0.1802968059666455, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 89, Train Loss: 0.1797936487942934, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042100001126527786\n",
            "Epoch: 90, Train Loss: 0.17897758912295103, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 91, Train Loss: 0.180108243227005, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 92, Train Loss: 0.18055846840143203, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 93, Train Loss: 0.1796636486425996, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 94, Train Loss: 0.18030142039060593, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 95, Train Loss: 0.1805786183103919, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 96, Train Loss: 0.18032248523086308, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 97, Train Loss: 0.18008576314896346, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 98, Train Loss: 0.1804321963340044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 99, Train Loss: 0.18043312579393386, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 100, Train Loss: 0.17939601205289363, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 101, Train Loss: 0.18021066673099995, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 102, Train Loss: 0.18034324869513513, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 103, Train Loss: 0.17903227042406797, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 104, Train Loss: 0.17934388294816017, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 105, Train Loss: 0.1799342654645443, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 106, Train Loss: 0.1795043759047985, Validation Loss: [not yet available], prediction: 0.23022404313087463, true ratio: 0.121799997985363\n",
            "Epoch: 107, Train Loss: 0.18039134256541728, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 108, Train Loss: 0.18043841253966092, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 109, Train Loss: 0.18025726582854987, Validation Loss: [not yet available], prediction: 0.23694159090518951, true ratio: 0.5494999885559082\n",
            "Epoch: 110, Train Loss: 0.1804096038453281, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 111, Train Loss: 0.18021944537758827, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 112, Train Loss: 0.18036591093987225, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 113, Train Loss: 0.1802658136934042, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 114, Train Loss: 0.17988245869055391, Validation Loss: [not yet available], prediction: 0.2449527531862259, true ratio: 0.07270000129938126\n",
            "Epoch: 115, Train Loss: 0.18042364278808237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 116, Train Loss: 0.17986881118267775, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042100001126527786\n",
            "Epoch: 117, Train Loss: 0.18068163450807334, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 118, Train Loss: 0.17850787434726953, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 119, Train Loss: 0.18016678038984538, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 120, Train Loss: 0.18054822273552418, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 121, Train Loss: 0.18040942810475827, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 122, Train Loss: 0.17830681148916483, Validation Loss: [not yet available], prediction: 0.25349119305610657, true ratio: 0.07270000129938126\n",
            "Epoch: 123, Train Loss: 0.17947989832609892, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 124, Train Loss: 0.18021426908671856, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.121799997985363\n",
            "Epoch: 125, Train Loss: 0.18059927895665168, Validation Loss: [not yet available], prediction: 0.26020547747612, true ratio: 0.026200000196695328\n",
            "Epoch: 126, Train Loss: 0.18039584308862686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 127, Train Loss: 0.1803823534399271, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 128, Train Loss: 0.17869753316044806, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 129, Train Loss: 0.1811084609478712, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 130, Train Loss: 0.18042420567944645, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 131, Train Loss: 0.17971019838005303, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 132, Train Loss: 0.17879848256707193, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11159999668598175\n",
            "Epoch: 133, Train Loss: 0.1802507508546114, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 134, Train Loss: 0.18018516786396505, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5540000200271606\n",
            "Epoch: 135, Train Loss: 0.18050031997263433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 136, Train Loss: 0.18018885441124438, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 137, Train Loss: 0.1805615246295929, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 138, Train Loss: 0.18031695410609244, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 139, Train Loss: 0.18036873815581203, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 140, Train Loss: 0.1802852341905236, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 141, Train Loss: 0.17972095049917697, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 142, Train Loss: 0.17863519676029682, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 143, Train Loss: 0.18002293705940248, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 144, Train Loss: 0.1799888091161847, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 145, Train Loss: 0.1802943479269743, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 146, Train Loss: 0.18022110555320978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 147, Train Loss: 0.17990902606397868, Validation Loss: [not yet available], prediction: 0.31023073196411133, true ratio: 0.374099999666214\n",
            "Epoch: 148, Train Loss: 0.18030843138694763, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 149, Train Loss: 0.17871557623147966, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 150, Train Loss: 0.18009410221129657, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 151, Train Loss: 0.1794627284631133, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 152, Train Loss: 0.180605136975646, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11159999668598175\n",
            "Epoch: 153, Train Loss: 0.1804843692108989, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 154, Train Loss: 0.1779938604682684, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 155, Train Loss: 0.180494587123394, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 156, Train Loss: 0.17803906574845313, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 157, Train Loss: 0.18033456858247518, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 158, Train Loss: 0.18037775587290525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 159, Train Loss: 0.18044837098568678, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0640999972820282\n",
            "Epoch: 160, Train Loss: 0.1803786765784025, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 161, Train Loss: 0.18021609289571644, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 162, Train Loss: 0.17969376798719167, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 163, Train Loss: 0.17978247944265605, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 164, Train Loss: 0.17861293070018291, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4632999897003174\n",
            "Epoch: 165, Train Loss: 0.1803519293665886, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 166, Train Loss: 0.1786659874022007, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 167, Train Loss: 0.18130982164293527, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 168, Train Loss: 0.18020745469257235, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 169, Train Loss: 0.17874268740415572, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 170, Train Loss: 0.1800142029300332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 171, Train Loss: 0.1793705656193197, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 172, Train Loss: 0.1792771739885211, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 173, Train Loss: 0.18078406453132628, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 174, Train Loss: 0.1803869228810072, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 175, Train Loss: 0.180305770970881, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 176, Train Loss: 0.17993236295878887, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 177, Train Loss: 0.17876526787877084, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13099999725818634\n",
            "Epoch: 178, Train Loss: 0.18041912708431482, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 179, Train Loss: 0.17960021290928124, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 180, Train Loss: 0.17990100868046283, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 181, Train Loss: 0.1800304738804698, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 182, Train Loss: 0.17808182891458274, Validation Loss: [not yet available], prediction: 0.2500661611557007, true ratio: 0.28049999475479126\n",
            "Epoch: 183, Train Loss: 0.1777454663068056, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.23800000548362732\n",
            "Epoch: 184, Train Loss: 0.18017805702984332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 185, Train Loss: 0.1804653225466609, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 186, Train Loss: 0.18027113545686008, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 187, Train Loss: 0.17906658612191678, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 188, Train Loss: 0.1805869357660413, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 189, Train Loss: 0.18043431751430034, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 190, Train Loss: 0.18070268649607896, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 191, Train Loss: 0.1805236584506929, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 192, Train Loss: 0.1798700775951147, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 193, Train Loss: 0.18034640941768884, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 194, Train Loss: 0.17787943668663503, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 195, Train Loss: 0.17936938125640153, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 196, Train Loss: 0.1800381623208523, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.35920000076293945\n",
            "Epoch: 197, Train Loss: 0.18027279078960418, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 198, Train Loss: 0.17993881087750196, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 199, Train Loss: 0.18052488341927528, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "//content//model_no_conv_RatioNet_bs10_lr0.01_epoch199\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10)\n",
        "ratio_net = no_lstm_RatioNet(input_size=len(tokenised_data.vocab), batch_size=10, device=device, embedding_size=50,num_layers=1, hidden_size=50, lstm_input_size=50)\n",
        "\n",
        "train_net(ratio_net, sanity_loader, device=device, sanity_check=True, epochs=200 , learning_rate=0.01, adam=False)\n",
        "model_path = path(ratio_net.name, 10, 0.01, 199)\n",
        "\n",
        "print(model_path)\n",
        "plot_training_curve(model_path)\n"
      ],
      "metadata": {
        "id": "3GvTVCbMweza",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c402f5ea-53fe-4944-ce2f-3cee17740b5f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 0.13702635653316975, Validation Loss: [not yet available], prediction: 0.2347220778465271, true ratio: 0.017100000753998756\n",
            "Epoch: 1, Train Loss: 0.1819350441917777, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 2, Train Loss: 0.18698764517903327, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 3, Train Loss: 0.1869876401498914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 4, Train Loss: 0.18698764443397523, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 5, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 6, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 7, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 8, Train Loss: 0.18698764126747847, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 9, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 10, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 11, Train Loss: 0.18698763865977525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 12, Train Loss: 0.18698762953281403, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 13, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 14, Train Loss: 0.18698763912543653, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 15, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 16, Train Loss: 0.18698764625005423, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08489999920129776\n",
            "Epoch: 17, Train Loss: 0.18698762990534307, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 18, Train Loss: 0.18698764704167842, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 19, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.49230000376701355\n",
            "Epoch: 20, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06159999966621399\n",
            "Epoch: 21, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 22, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 23, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 24, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 25, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 26, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 27, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 28, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 29, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 30, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 31, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 32, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08799999952316284\n",
            "Epoch: 33, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.032999999821186066\n",
            "Epoch: 34, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2198999971151352\n",
            "Epoch: 35, Train Loss: 0.18698763530701398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 36, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 37, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 38, Train Loss: 0.18698763567954302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 39, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 40, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 41, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 42, Train Loss: 0.18698763442225755, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 43, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 44, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 45, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 46, Train Loss: 0.18698763865977525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 47, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 48, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 49, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 50, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 51, Train Loss: 0.1869876326061785, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 52, Train Loss: 0.1869876309297979, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 53, Train Loss: 0.18698763195425272, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 54, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 55, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0820000022649765\n",
            "Epoch: 56, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 57, Train Loss: 0.18698762804269792, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 58, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 59, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 60, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 61, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 62, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 63, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 64, Train Loss: 0.18698763153515757, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08070000261068344\n",
            "Epoch: 65, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 66, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 67, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 68, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 69, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.210999995470047\n",
            "Epoch: 70, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.19949999451637268\n",
            "Epoch: 71, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 72, Train Loss: 0.18698763567954302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06430000066757202\n",
            "Epoch: 73, Train Loss: 0.1869876403361559, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01600000075995922\n",
            "Epoch: 74, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 75, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 76, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 77, Train Loss: 0.18698764368891715, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 78, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 79, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0625\n",
            "Epoch: 80, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.013299999758601189\n",
            "Epoch: 81, Train Loss: 0.18698764145374297, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 82, Train Loss: 0.18698763726279138, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16580000519752502\n",
            "Epoch: 83, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 84, Train Loss: 0.18698763512074948, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 85, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 86, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 87, Train Loss: 0.1869876328855753, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 88, Train Loss: 0.18698763931170106, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.374099999666214\n",
            "Epoch: 89, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 90, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 91, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11159999668598175\n",
            "Epoch: 92, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 93, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.053199999034404755\n",
            "Epoch: 94, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.12479999661445618\n",
            "Epoch: 95, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 96, Train Loss: 0.18698762785643339, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.16210000216960907\n",
            "Epoch: 97, Train Loss: 0.18698762804269792, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.051500000059604645\n",
            "Epoch: 98, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07349999994039536\n",
            "Epoch: 99, Train Loss: 0.1869876330718398, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.4821000099182129\n",
            "Epoch: 100, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 101, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1485999971628189\n",
            "Epoch: 102, Train Loss: 0.18698763558641077, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 103, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0737999975681305\n",
            "Epoch: 104, Train Loss: 0.18698764925356953, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 105, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 106, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1875\n",
            "Epoch: 107, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 108, Train Loss: 0.18698764303699136, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 109, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08139999955892563\n",
            "Epoch: 110, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.25200000405311584\n",
            "Epoch: 111, Train Loss: 0.18698763670399784, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.3230000138282776\n",
            "Epoch: 112, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 113, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2084999978542328\n",
            "Epoch: 114, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.888700008392334\n",
            "Epoch: 115, Train Loss: 0.18698764070868493, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0812000036239624\n",
            "Epoch: 116, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.44269999861717224\n",
            "Epoch: 117, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 118, Train Loss: 0.18698764108121396, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 119, Train Loss: 0.18698762785643339, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 120, Train Loss: 0.18698764629662037, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 121, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.056699998676776886\n",
            "Epoch: 122, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28049999475479126\n",
            "Epoch: 123, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 124, Train Loss: 0.18698763959109782, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 125, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 126, Train Loss: 0.18698763977736235, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.05739999935030937\n",
            "Epoch: 127, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 128, Train Loss: 0.18698763214051722, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 129, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.598800003528595\n",
            "Epoch: 130, Train Loss: 0.18698764438740909, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 131, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.8458999991416931\n",
            "Epoch: 132, Train Loss: 0.18698763605207205, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.006300000008195639\n",
            "Epoch: 133, Train Loss: 0.18698763400316237, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.012600000016391277\n",
            "Epoch: 134, Train Loss: 0.18698764061555267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 135, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.11389999836683273\n",
            "Epoch: 136, Train Loss: 0.18698763567954302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 137, Train Loss: 0.1869876392185688, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 138, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042100001126527786\n",
            "Epoch: 139, Train Loss: 0.18698763828724624, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.042500000447034836\n",
            "Epoch: 140, Train Loss: 0.18698763931170106, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 141, Train Loss: 0.18698764387518169, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.07169999927282333\n",
            "Epoch: 142, Train Loss: 0.186987629160285, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 143, Train Loss: 0.18698763102293015, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.31619998812675476\n",
            "Epoch: 144, Train Loss: 0.18698762878775596, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.2962000072002411\n",
            "Epoch: 145, Train Loss: 0.18698763698339463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 146, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 147, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6384999752044678\n",
            "Epoch: 148, Train Loss: 0.1869876366108656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03680000081658363\n",
            "Epoch: 149, Train Loss: 0.18698763493448495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 150, Train Loss: 0.18698763381689787, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7692999839782715\n",
            "Epoch: 151, Train Loss: 0.18698763176798822, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 152, Train Loss: 0.1869876367971301, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.026200000196695328\n",
            "Epoch: 153, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17569999396800995\n",
            "Epoch: 154, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 155, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.13359999656677246\n",
            "Epoch: 156, Train Loss: 0.1869876293465495, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 157, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 158, Train Loss: 0.18698763735592366, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.01730000041425228\n",
            "Epoch: 159, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 160, Train Loss: 0.18698763884603978, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.6937999725341797\n",
            "Epoch: 161, Train Loss: 0.18698763772845267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.24279999732971191\n",
            "Epoch: 162, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 163, Train Loss: 0.1869876379147172, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 164, Train Loss: 0.18698763027787207, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 165, Train Loss: 0.1869876381009817, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 166, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.018699999898672104\n",
            "Epoch: 167, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09130000323057175\n",
            "Epoch: 168, Train Loss: 0.18698763716965913, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14900000393390656\n",
            "Epoch: 169, Train Loss: 0.18698763456195594, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 170, Train Loss: 0.18698764368891715, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 171, Train Loss: 0.1869876429438591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 172, Train Loss: 0.18698763232678176, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.045099999755620956\n",
            "Epoch: 173, Train Loss: 0.1869876267388463, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 174, Train Loss: 0.1869876306504011, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.15479999780654907\n",
            "Epoch: 175, Train Loss: 0.18698764219880104, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08780000358819962\n",
            "Epoch: 176, Train Loss: 0.18698763549327851, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.45320001244544983\n",
            "Epoch: 177, Train Loss: 0.18698763474822044, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03420000150799751\n",
            "Epoch: 178, Train Loss: 0.18698763623833656, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28529998660087585\n",
            "Epoch: 179, Train Loss: 0.18698763251304626, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.1898999959230423\n",
            "Epoch: 180, Train Loss: 0.18698763865977525, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.953499972820282\n",
            "Epoch: 181, Train Loss: 0.1869876343756914, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.7247999906539917\n",
            "Epoch: 182, Train Loss: 0.18698763712309302, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.09749999642372131\n",
            "Epoch: 183, Train Loss: 0.18698762729763985, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.029200000688433647\n",
            "Epoch: 184, Train Loss: 0.18698763940483332, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 185, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.03720000013709068\n",
            "Epoch: 186, Train Loss: 0.18698763363063337, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.5494999885559082\n",
            "Epoch: 187, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.538100004196167\n",
            "Epoch: 188, Train Loss: 0.18698763083666564, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.057500001043081284\n",
            "Epoch: 189, Train Loss: 0.18698764061555267, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.17839999496936798\n",
            "Epoch: 190, Train Loss: 0.18698764257133008, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.04399999976158142\n",
            "Epoch: 191, Train Loss: 0.18698763325810433, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21539999544620514\n",
            "Epoch: 192, Train Loss: 0.18698763847351074, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.06210000067949295\n",
            "Epoch: 193, Train Loss: 0.18698763903230428, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.28519999980926514\n",
            "Epoch: 194, Train Loss: 0.18698763996362686, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "Epoch: 195, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.14409999549388885\n",
            "Epoch: 196, Train Loss: 0.1869876341894269, Validation Loss: [not yet available], prediction: 0.0, true ratio: 1.194200038909912\n",
            "Epoch: 197, Train Loss: 0.1869876458309591, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.016899999231100082\n",
            "Epoch: 198, Train Loss: 0.18698763586580752, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.08940000087022781\n",
            "Epoch: 199, Train Loss: 0.18698763139545918, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.0210999995470047\n",
            "//content//model_no_lstm_RatioNet_bs10_lr0.01_epoch199\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ratio_net = RatioNet(input_size=len(tokenised_data.vocab), batch_size=10)\n",
        "ratio_net = RatioNet2(input_size=len(tokenised_data.vocab), batch_size=10, device=device, embedding_size=50,num_layers=1, hidden_size=50, lstm_input_size=50)\n",
        "\n",
        "train_net(ratio_net, sanity_loader, device=device, sanity_check=True, epochs=200 , learning_rate=0.000000001, adam=True, weight_decay=0)\n",
        "model_path = path(ratio_net.name, 10, 0.000000001, 199)\n",
        "\n",
        "print(model_path)\n",
        "plot_training_curve(model_path)\n"
      ],
      "metadata": {
        "id": "UoJHVohb_KyT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72cd51a4-1b63-4aca-fb1e-813fec123c55"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 0.1654263898730278, Validation Loss: [not yet available], prediction: 0.048810433596372604, true ratio: 0.042500000447034836\n",
            "Epoch: 1, Train Loss: 0.16588093526661396, Validation Loss: [not yet available], prediction: 0.04880716651678085, true ratio: 0.1898999959230423\n",
            "Epoch: 2, Train Loss: 0.16524195219390095, Validation Loss: [not yet available], prediction: 0.048811543732881546, true ratio: 0.2962000072002411\n",
            "Epoch: 3, Train Loss: 0.16549607971683145, Validation Loss: [not yet available], prediction: 0.0487922765314579, true ratio: 0.2962000072002411\n",
            "Epoch: 4, Train Loss: 0.16590033723041414, Validation Loss: [not yet available], prediction: 0.04880519583821297, true ratio: 0.08799999952316284\n",
            "Epoch: 5, Train Loss: 0.16522890720516442, Validation Loss: [not yet available], prediction: 0.04881567135453224, true ratio: 0.0820000022649765\n",
            "Epoch: 6, Train Loss: 0.1659528873860836, Validation Loss: [not yet available], prediction: 0.048813652247190475, true ratio: 0.06159999966621399\n",
            "Epoch: 7, Train Loss: 0.16510863043367863, Validation Loss: [not yet available], prediction: 0.04877925664186478, true ratio: 0.16210000216960907\n",
            "Epoch: 8, Train Loss: 0.1657492347061634, Validation Loss: [not yet available], prediction: 0.04881676658987999, true ratio: 0.061000000685453415\n",
            "Epoch: 9, Train Loss: 0.1655480980873108, Validation Loss: [not yet available], prediction: 0.04881160706281662, true ratio: 0.4632999897003174\n",
            "Epoch: 10, Train Loss: 0.1659144232980907, Validation Loss: [not yet available], prediction: 0.04881540685892105, true ratio: 0.8458999991416931\n",
            "Epoch: 11, Train Loss: 0.165366093814373, Validation Loss: [not yet available], prediction: 0.04882200062274933, true ratio: 0.888700008392334\n",
            "Epoch: 12, Train Loss: 0.16569174136966466, Validation Loss: [not yet available], prediction: 0.04879973083734512, true ratio: 0.03680000081658363\n",
            "Epoch: 13, Train Loss: 0.16512671172386034, Validation Loss: [not yet available], prediction: 0.04881010577082634, true ratio: 0.49230000376701355\n",
            "Epoch: 14, Train Loss: 0.16511404868215324, Validation Loss: [not yet available], prediction: 0.048811234533786774, true ratio: 0.061000000685453415\n",
            "Epoch: 15, Train Loss: 0.16583329308778047, Validation Loss: [not yet available], prediction: 0.04699478670954704, true ratio: 0.0210999995470047\n",
            "Epoch: 16, Train Loss: 0.16544987326487898, Validation Loss: [not yet available], prediction: 0.04882368445396423, true ratio: 0.08940000087022781\n",
            "Epoch: 17, Train Loss: 0.16521582938730717, Validation Loss: [not yet available], prediction: 0.04882175475358963, true ratio: 0.4632999897003174\n",
            "Epoch: 18, Train Loss: 0.16545192152261734, Validation Loss: [not yet available], prediction: 0.04881608486175537, true ratio: 0.15479999780654907\n",
            "Epoch: 19, Train Loss: 0.1660556889139116, Validation Loss: [not yet available], prediction: 0.048768218606710434, true ratio: 0.2198999971151352\n",
            "Epoch: 20, Train Loss: 0.1648612827062607, Validation Loss: [not yet available], prediction: 0.047451552003622055, true ratio: 0.21119999885559082\n",
            "Epoch: 21, Train Loss: 0.16488295062445105, Validation Loss: [not yet available], prediction: 0.04870496690273285, true ratio: 0.01730000041425228\n",
            "Epoch: 22, Train Loss: 0.1659486001357436, Validation Loss: [not yet available], prediction: 0.04882363602519035, true ratio: 0.2198999971151352\n",
            "Epoch: 23, Train Loss: 0.16527919732034208, Validation Loss: [not yet available], prediction: 0.06423857808113098, true ratio: 1.538100004196167\n",
            "Epoch: 24, Train Loss: 0.1650951225310564, Validation Loss: [not yet available], prediction: 0.048814356327056885, true ratio: 0.16580000519752502\n",
            "Epoch: 25, Train Loss: 0.165295500587672, Validation Loss: [not yet available], prediction: 0.04853144288063049, true ratio: 0.06210000067949295\n",
            "Epoch: 26, Train Loss: 0.1658538134768605, Validation Loss: [not yet available], prediction: 0.04883706569671631, true ratio: 0.17569999396800995\n",
            "Epoch: 27, Train Loss: 0.16595120257697998, Validation Loss: [not yet available], prediction: 0.04882662370800972, true ratio: 0.06159999966621399\n",
            "Epoch: 28, Train Loss: 0.1653326668776572, Validation Loss: [not yet available], prediction: 0.028324006125330925, true ratio: 0.121799997985363\n",
            "Epoch: 29, Train Loss: 0.16556464713066815, Validation Loss: [not yet available], prediction: 0.04884035512804985, true ratio: 0.0640999972820282\n",
            "Epoch: 30, Train Loss: 0.1650885172188282, Validation Loss: [not yet available], prediction: 0.04853133112192154, true ratio: 0.2198999971151352\n",
            "Epoch: 31, Train Loss: 0.16565176714211702, Validation Loss: [not yet available], prediction: 0.048299916088581085, true ratio: 0.28529998660087585\n",
            "Epoch: 32, Train Loss: 0.16591619430109858, Validation Loss: [not yet available], prediction: 0.0484677329659462, true ratio: 0.06430000066757202\n",
            "Epoch: 33, Train Loss: 0.16540709510445595, Validation Loss: [not yet available], prediction: 0.04428410530090332, true ratio: 0.953499972820282\n",
            "Epoch: 34, Train Loss: 0.16543575692921877, Validation Loss: [not yet available], prediction: 0.048834554851055145, true ratio: 0.28519999980926514\n",
            "Epoch: 35, Train Loss: 0.16569683523848652, Validation Loss: [not yet available], prediction: 0.055535636842250824, true ratio: 0.16830000281333923\n",
            "Epoch: 36, Train Loss: 0.16516629792749882, Validation Loss: [not yet available], prediction: 0.04881640523672104, true ratio: 0.006300000008195639\n",
            "Epoch: 37, Train Loss: 0.16604800038039685, Validation Loss: [not yet available], prediction: 0.03797774389386177, true ratio: 0.5494999885559082\n",
            "Epoch: 38, Train Loss: 0.16610945109277964, Validation Loss: [not yet available], prediction: 0.048833027482032776, true ratio: 0.0625\n",
            "Epoch: 39, Train Loss: 0.16506434297189115, Validation Loss: [not yet available], prediction: 0.048835866153240204, true ratio: 0.19949999451637268\n",
            "Epoch: 40, Train Loss: 0.1651124486234039, Validation Loss: [not yet available], prediction: 0.04883652180433273, true ratio: 0.1898999959230423\n",
            "Epoch: 41, Train Loss: 0.16525711677968502, Validation Loss: [not yet available], prediction: 0.04104884713888168, true ratio: 0.017100000753998756\n",
            "Epoch: 42, Train Loss: 0.16554302386939526, Validation Loss: [not yet available], prediction: 0.048832230269908905, true ratio: 0.05739999935030937\n",
            "Epoch: 43, Train Loss: 0.16594869662076234, Validation Loss: [not yet available], prediction: 0.04883146658539772, true ratio: 0.2962000072002411\n",
            "Epoch: 44, Train Loss: 0.16523343194276094, Validation Loss: [not yet available], prediction: 0.048837918788194656, true ratio: 0.2962000072002411\n",
            "Epoch: 45, Train Loss: 0.16492619442287831, Validation Loss: [not yet available], prediction: 0.04882480204105377, true ratio: 0.31619998812675476\n",
            "Epoch: 46, Train Loss: 0.16539265466853975, Validation Loss: [not yet available], prediction: 0.048804156482219696, true ratio: 0.45320001244544983\n",
            "Epoch: 47, Train Loss: 0.16608430575579405, Validation Loss: [not yet available], prediction: 0.04884020611643791, true ratio: 0.053199999034404755\n",
            "Epoch: 48, Train Loss: 0.1653269689530134, Validation Loss: [not yet available], prediction: 0.04879246652126312, true ratio: 0.44269999861717224\n",
            "Epoch: 49, Train Loss: 0.16593251805752515, Validation Loss: [not yet available], prediction: 0.04882334545254707, true ratio: 0.045099999755620956\n",
            "Epoch: 50, Train Loss: 0.1656337022315711, Validation Loss: [not yet available], prediction: 0.04884197562932968, true ratio: 0.2198999971151352\n",
            "Epoch: 51, Train Loss: 0.16541882250458, Validation Loss: [not yet available], prediction: 0.04884129390120506, true ratio: 0.11159999668598175\n",
            "Epoch: 52, Train Loss: 0.16599184861406685, Validation Loss: [not yet available], prediction: 0.04871097952127457, true ratio: 0.08489999920129776\n",
            "Epoch: 53, Train Loss: 0.16517309653572737, Validation Loss: [not yet available], prediction: 0.047925129532814026, true ratio: 0.4821000099182129\n",
            "Epoch: 54, Train Loss: 0.16556451693177224, Validation Loss: [not yet available], prediction: 0.04883277416229248, true ratio: 0.210999995470047\n",
            "Epoch: 55, Train Loss: 0.1654725583270192, Validation Loss: [not yet available], prediction: 0.0, true ratio: 0.21119999885559082\n",
            "Epoch: 56, Train Loss: 0.16543754925951362, Validation Loss: [not yet available], prediction: 0.048845719546079636, true ratio: 0.042500000447034836\n",
            "Epoch: 57, Train Loss: 0.16557952277362348, Validation Loss: [not yet available], prediction: 0.048845045268535614, true ratio: 0.7692999839782715\n",
            "Epoch: 58, Train Loss: 0.16592824086546898, Validation Loss: [not yet available], prediction: 0.048657115548849106, true ratio: 0.21539999544620514\n",
            "Epoch: 59, Train Loss: 0.1655051302164793, Validation Loss: [not yet available], prediction: 0.04884697496891022, true ratio: 0.23800000548362732\n",
            "Epoch: 60, Train Loss: 0.16480001993477345, Validation Loss: [not yet available], prediction: 0.04822922870516777, true ratio: 0.16830000281333923\n",
            "Epoch: 61, Train Loss: 0.16520319860428573, Validation Loss: [not yet available], prediction: 0.04884929209947586, true ratio: 0.31619998812675476\n",
            "Epoch: 62, Train Loss: 0.16539564025588332, Validation Loss: [not yet available], prediction: 0.04885010048747063, true ratio: 0.042500000447034836\n",
            "Epoch: 63, Train Loss: 0.16561956969089806, Validation Loss: [not yet available], prediction: 0.04883941262960434, true ratio: 0.12479999661445618\n",
            "Epoch: 64, Train Loss: 0.1652898548170924, Validation Loss: [not yet available], prediction: 0.04884495213627815, true ratio: 0.19949999451637268\n",
            "Epoch: 65, Train Loss: 0.16534035205841063, Validation Loss: [not yet available], prediction: 0.04885207489132881, true ratio: 0.7692999839782715\n",
            "Epoch: 66, Train Loss: 0.1652848438359797, Validation Loss: [not yet available], prediction: 0.04427284002304077, true ratio: 0.051500000059604645\n",
            "Epoch: 67, Train Loss: 0.16546089872717856, Validation Loss: [not yet available], prediction: 0.048851024359464645, true ratio: 0.5956000089645386\n",
            "Epoch: 68, Train Loss: 0.1653817556798458, Validation Loss: [not yet available], prediction: 0.0028301249258220196, true ratio: 0.5494999885559082\n",
            "Epoch: 69, Train Loss: 0.1655323300510645, Validation Loss: [not yet available], prediction: 0.048860229551792145, true ratio: 0.012600000016391277\n",
            "Epoch: 70, Train Loss: 0.16540880054235457, Validation Loss: [not yet available], prediction: 0.06259100139141083, true ratio: 0.029200000688433647\n",
            "Epoch: 71, Train Loss: 0.16573356902226805, Validation Loss: [not yet available], prediction: 0.04855644702911377, true ratio: 0.03420000150799751\n",
            "Epoch: 72, Train Loss: 0.16580475019291044, Validation Loss: [not yet available], prediction: 0.04874756559729576, true ratio: 0.0820000022649765\n",
            "Epoch: 73, Train Loss: 0.1654717047000304, Validation Loss: [not yet available], prediction: 0.04886492341756821, true ratio: 0.16210000216960907\n",
            "Epoch: 74, Train Loss: 0.1656077653169632, Validation Loss: [not yet available], prediction: 0.04868427291512489, true ratio: 0.2084999978542328\n",
            "Epoch: 75, Train Loss: 0.16545222187414765, Validation Loss: [not yet available], prediction: 0.04954403638839722, true ratio: 0.24279999732971191\n",
            "Epoch: 76, Train Loss: 0.16553268916904926, Validation Loss: [not yet available], prediction: 0.05289366468787193, true ratio: 0.5540000200271606\n",
            "Epoch: 77, Train Loss: 0.16542586740106344, Validation Loss: [not yet available], prediction: 0.04860725626349449, true ratio: 0.06430000066757202\n",
            "Epoch: 78, Train Loss: 0.1654528102837503, Validation Loss: [not yet available], prediction: 0.04885869100689888, true ratio: 0.0625\n",
            "Epoch: 79, Train Loss: 0.1650670725852251, Validation Loss: [not yet available], prediction: 0.04882640019059181, true ratio: 0.1898999959230423\n",
            "Epoch: 80, Train Loss: 0.16576216593384743, Validation Loss: [not yet available], prediction: 0.048848457634449005, true ratio: 0.14900000393390656\n",
            "Epoch: 81, Train Loss: 0.16509648524224757, Validation Loss: [not yet available], prediction: 0.04395667091012001, true ratio: 0.374099999666214\n",
            "Epoch: 82, Train Loss: 0.16527201626449822, Validation Loss: [not yet available], prediction: 0.04886304959654808, true ratio: 0.051500000059604645\n",
            "Epoch: 83, Train Loss: 0.16608388125896453, Validation Loss: [not yet available], prediction: 0.048861660063266754, true ratio: 0.23800000548362732\n",
            "Epoch: 84, Train Loss: 0.16570709608495235, Validation Loss: [not yet available], prediction: 0.04886339232325554, true ratio: 0.06159999966621399\n",
            "Epoch: 85, Train Loss: 0.1656069508753717, Validation Loss: [not yet available], prediction: 0.026947708800435066, true ratio: 0.07270000129938126\n",
            "Epoch: 86, Train Loss: 0.16569738630205394, Validation Loss: [not yet available], prediction: 0.04886450991034508, true ratio: 0.032999999821186066\n",
            "Epoch: 87, Train Loss: 0.1656327335163951, Validation Loss: [not yet available], prediction: 0.047270022332668304, true ratio: 0.029200000688433647\n",
            "Epoch: 88, Train Loss: 0.1654646795243025, Validation Loss: [not yet available], prediction: 0.048864807933568954, true ratio: 0.01600000075995922\n",
            "Epoch: 89, Train Loss: 0.16526549011468888, Validation Loss: [not yet available], prediction: 0.04873717576265335, true ratio: 0.0812000036239624\n",
            "Epoch: 90, Train Loss: 0.16526578599587083, Validation Loss: [not yet available], prediction: 0.04866921901702881, true ratio: 0.4821000099182129\n",
            "Epoch: 91, Train Loss: 0.16556881032884121, Validation Loss: [not yet available], prediction: 0.0488998144865036, true ratio: 0.006300000008195639\n",
            "Epoch: 92, Train Loss: 0.16534315370954572, Validation Loss: [not yet available], prediction: 0.04886125028133392, true ratio: 0.210999995470047\n",
            "Epoch: 93, Train Loss: 0.16525763398967683, Validation Loss: [not yet available], prediction: 0.052894871681928635, true ratio: 0.03420000150799751\n",
            "Epoch: 94, Train Loss: 0.165578880533576, Validation Loss: [not yet available], prediction: 0.048567693680524826, true ratio: 0.24279999732971191\n",
            "Epoch: 95, Train Loss: 0.16518128328025342, Validation Loss: [not yet available], prediction: 0.04886285588145256, true ratio: 0.052299998700618744\n",
            "Epoch: 96, Train Loss: 0.16547118686139584, Validation Loss: [not yet available], prediction: 0.046571969985961914, true ratio: 0.13359999656677246\n",
            "Epoch: 97, Train Loss: 0.1653007220476866, Validation Loss: [not yet available], prediction: 0.04658227413892746, true ratio: 0.5956000089645386\n",
            "Epoch: 98, Train Loss: 0.16526029333472253, Validation Loss: [not yet available], prediction: 0.0488734170794487, true ratio: 0.19949999451637268\n",
            "Epoch: 99, Train Loss: 0.16573028024286032, Validation Loss: [not yet available], prediction: 0.0488797202706337, true ratio: 0.6937999725341797\n",
            "Epoch: 100, Train Loss: 0.16534148380160332, Validation Loss: [not yet available], prediction: 0.048873841762542725, true ratio: 0.2962000072002411\n",
            "Epoch: 101, Train Loss: 0.16610912419855595, Validation Loss: [not yet available], prediction: 0.04886726289987564, true ratio: 0.08070000261068344\n",
            "Epoch: 102, Train Loss: 0.16552713885903358, Validation Loss: [not yet available], prediction: 0.048885200172662735, true ratio: 0.17569999396800995\n",
            "Epoch: 103, Train Loss: 0.16576483622193336, Validation Loss: [not yet available], prediction: 0.04869770631194115, true ratio: 0.01730000041425228\n",
            "Epoch: 104, Train Loss: 0.16619022525846958, Validation Loss: [not yet available], prediction: 0.04883946478366852, true ratio: 0.08780000358819962\n",
            "Epoch: 105, Train Loss: 0.1656806780025363, Validation Loss: [not yet available], prediction: 0.04887588694691658, true ratio: 0.8458999991416931\n",
            "Epoch: 106, Train Loss: 0.1651762818917632, Validation Loss: [not yet available], prediction: 0.04887682944536209, true ratio: 0.17839999496936798\n",
            "Epoch: 107, Train Loss: 0.1653570557013154, Validation Loss: [not yet available], prediction: 0.048696260899305344, true ratio: 0.08489999920129776\n",
            "Epoch: 108, Train Loss: 0.1653329249471426, Validation Loss: [not yet available], prediction: 0.04887557029724121, true ratio: 0.045099999755620956\n",
            "Epoch: 109, Train Loss: 0.16541166100651025, Validation Loss: [not yet available], prediction: 0.04887926205992699, true ratio: 0.08799999952316284\n",
            "Epoch: 110, Train Loss: 0.1656957833096385, Validation Loss: [not yet available], prediction: 0.03401078283786774, true ratio: 0.08139999955892563\n",
            "Epoch: 111, Train Loss: 0.16530779823660852, Validation Loss: [not yet available], prediction: 0.0487484447658062, true ratio: 0.24279999732971191\n",
            "Epoch: 112, Train Loss: 0.1656706883572042, Validation Loss: [not yet available], prediction: 0.046730924397706985, true ratio: 0.21119999885559082\n",
            "Epoch: 113, Train Loss: 0.1653654816094786, Validation Loss: [not yet available], prediction: 0.0488823801279068, true ratio: 0.08070000261068344\n",
            "Epoch: 114, Train Loss: 0.1654502154327929, Validation Loss: [not yet available], prediction: 0.04877458140254021, true ratio: 0.0820000022649765\n",
            "Epoch: 115, Train Loss: 0.1652303977869451, Validation Loss: [not yet available], prediction: 0.04886912181973457, true ratio: 0.0625\n",
            "Epoch: 116, Train Loss: 0.1658442563377321, Validation Loss: [not yet available], prediction: 0.026963436976075172, true ratio: 0.07270000129938126\n",
            "Epoch: 117, Train Loss: 0.16580296251922846, Validation Loss: [not yet available], prediction: 0.04885994270443916, true ratio: 0.08799999952316284\n",
            "Epoch: 118, Train Loss: 0.16492784754373133, Validation Loss: [not yet available], prediction: 0.04888539761304855, true ratio: 0.04399999976158142\n",
            "Epoch: 119, Train Loss: 0.16542271906509995, Validation Loss: [not yet available], prediction: 0.014269903302192688, true ratio: 0.026200000196695328\n",
            "Epoch: 120, Train Loss: 0.16501173619180917, Validation Loss: [not yet available], prediction: 0.0481538362801075, true ratio: 0.35920000076293945\n",
            "Epoch: 121, Train Loss: 0.1657024070620537, Validation Loss: [not yet available], prediction: 0.048281848430633545, true ratio: 0.09749999642372131\n",
            "Epoch: 122, Train Loss: 0.16627198504284024, Validation Loss: [not yet available], prediction: 0.04865908995270729, true ratio: 0.05739999935030937\n",
            "Epoch: 123, Train Loss: 0.16558718625456095, Validation Loss: [not yet available], prediction: 0.04889153689146042, true ratio: 0.06210000067949295\n",
            "Epoch: 124, Train Loss: 0.1650609362870455, Validation Loss: [not yet available], prediction: 0.04887006804347038, true ratio: 0.061000000685453415\n",
            "Epoch: 125, Train Loss: 0.1651755348779261, Validation Loss: [not yet available], prediction: 0.04883154109120369, true ratio: 0.45320001244544983\n",
            "Epoch: 126, Train Loss: 0.1659871818497777, Validation Loss: [not yet available], prediction: 0.04871691018342972, true ratio: 0.06210000067949295\n",
            "Epoch: 127, Train Loss: 0.16587367802858352, Validation Loss: [not yet available], prediction: 0.0486137717962265, true ratio: 0.0812000036239624\n",
            "Epoch: 128, Train Loss: 0.16596103026531636, Validation Loss: [not yet available], prediction: 0.04889099672436714, true ratio: 0.15479999780654907\n",
            "Epoch: 129, Train Loss: 0.1655022794380784, Validation Loss: [not yet available], prediction: 0.04860746115446091, true ratio: 0.07169999927282333\n",
            "Epoch: 130, Train Loss: 0.16552208345383407, Validation Loss: [not yet available], prediction: 0.04882826283574104, true ratio: 0.598800003528595\n",
            "Epoch: 131, Train Loss: 0.16518866531550885, Validation Loss: [not yet available], prediction: 0.04889285936951637, true ratio: 1.194200038909912\n",
            "Epoch: 132, Train Loss: 0.165222411416471, Validation Loss: [not yet available], prediction: 0.0486266128718853, true ratio: 0.13099999725818634\n",
            "Epoch: 133, Train Loss: 0.1650904123671353, Validation Loss: [not yet available], prediction: 0.04889693483710289, true ratio: 0.0820000022649765\n",
            "Epoch: 134, Train Loss: 0.165194446220994, Validation Loss: [not yet available], prediction: 0.04889374226331711, true ratio: 0.01730000041425228\n",
            "Epoch: 135, Train Loss: 0.16545572513714432, Validation Loss: [not yet available], prediction: 0.048597075045108795, true ratio: 0.03420000150799751\n",
            "Epoch: 136, Train Loss: 0.16528811249881983, Validation Loss: [not yet available], prediction: 0.04761742055416107, true ratio: 0.0737999975681305\n",
            "Epoch: 137, Train Loss: 0.16575600989162922, Validation Loss: [not yet available], prediction: 0.048394687473773956, true ratio: 0.44269999861717224\n",
            "Epoch: 138, Train Loss: 0.1648568369448185, Validation Loss: [not yet available], prediction: 0.04897570610046387, true ratio: 1.538100004196167\n",
            "Epoch: 139, Train Loss: 0.1657570583745837, Validation Loss: [not yet available], prediction: 0.048898227512836456, true ratio: 0.11389999836683273\n",
            "Epoch: 140, Train Loss: 0.16539523005485535, Validation Loss: [not yet available], prediction: 0.016795797273516655, true ratio: 1.2827999591827393\n",
            "Epoch: 141, Train Loss: 0.1650383316911757, Validation Loss: [not yet available], prediction: 0.048956453800201416, true ratio: 0.006300000008195639\n",
            "Epoch: 142, Train Loss: 0.16571806464344263, Validation Loss: [not yet available], prediction: 0.048874858766794205, true ratio: 0.7247999906539917\n",
            "Epoch: 143, Train Loss: 0.16513208709657193, Validation Loss: [not yet available], prediction: 0.048900991678237915, true ratio: 0.210999995470047\n",
            "Epoch: 144, Train Loss: 0.1648400935344398, Validation Loss: [not yet available], prediction: 0.04888254031538963, true ratio: 0.061000000685453415\n",
            "Epoch: 145, Train Loss: 0.16552681606262923, Validation Loss: [not yet available], prediction: 0.04894474893808365, true ratio: 0.5494999885559082\n",
            "Epoch: 146, Train Loss: 0.16538098976016044, Validation Loss: [not yet available], prediction: 0.04873017966747284, true ratio: 0.2084999978542328\n",
            "Epoch: 147, Train Loss: 0.16531340032815933, Validation Loss: [not yet available], prediction: 0.048903223127126694, true ratio: 0.13099999725818634\n",
            "Epoch: 148, Train Loss: 0.16565896719694137, Validation Loss: [not yet available], prediction: 0.015736809000372887, true ratio: 0.0812000036239624\n",
            "Epoch: 149, Train Loss: 0.1657628996297717, Validation Loss: [not yet available], prediction: 0.04890519008040428, true ratio: 0.31619998812675476\n",
            "Epoch: 150, Train Loss: 0.16577033270150424, Validation Loss: [not yet available], prediction: 0.046506933867931366, true ratio: 0.5494999885559082\n",
            "Epoch: 151, Train Loss: 0.16528909066691994, Validation Loss: [not yet available], prediction: 0.04884745180606842, true ratio: 0.45320001244544983\n",
            "Epoch: 152, Train Loss: 0.16569079281762242, Validation Loss: [not yet available], prediction: 0.04887719079852104, true ratio: 0.0210999995470047\n",
            "Epoch: 153, Train Loss: 0.16570345740765333, Validation Loss: [not yet available], prediction: 0.048915617167949677, true ratio: 0.0640999972820282\n",
            "Epoch: 154, Train Loss: 0.1655781554058194, Validation Loss: [not yet available], prediction: 0.04872644692659378, true ratio: 0.08489999920129776\n",
            "Epoch: 155, Train Loss: 0.16511562950909137, Validation Loss: [not yet available], prediction: 0.048908546566963196, true ratio: 0.17839999496936798\n",
            "Epoch: 156, Train Loss: 0.1652787876315415, Validation Loss: [not yet available], prediction: 0.04890942573547363, true ratio: 0.7692999839782715\n",
            "Epoch: 157, Train Loss: 0.1657351464033127, Validation Loss: [not yet available], prediction: 0.04111243784427643, true ratio: 0.017100000753998756\n",
            "Epoch: 158, Train Loss: 0.16502189673483372, Validation Loss: [not yet available], prediction: 0.028386274352669716, true ratio: 0.121799997985363\n",
            "Epoch: 159, Train Loss: 0.16571623347699643, Validation Loss: [not yet available], prediction: 0.04872602969408035, true ratio: 0.03420000150799751\n",
            "Epoch: 160, Train Loss: 0.16565785836428404, Validation Loss: [not yet available], prediction: 0.04878373444080353, true ratio: 0.4821000099182129\n",
            "Epoch: 161, Train Loss: 0.16567563451826572, Validation Loss: [not yet available], prediction: 0.048912473022937775, true ratio: 0.7692999839782715\n",
            "Epoch: 162, Train Loss: 0.16540214316919447, Validation Loss: [not yet available], prediction: 0.04889354109764099, true ratio: 0.09130000323057175\n",
            "Epoch: 163, Train Loss: 0.1655371689237654, Validation Loss: [not yet available], prediction: 0.04881110042333603, true ratio: 0.045099999755620956\n",
            "Epoch: 164, Train Loss: 0.16527081513777375, Validation Loss: [not yet available], prediction: 0.048914313316345215, true ratio: 0.12479999661445618\n",
            "Epoch: 165, Train Loss: 0.16532408399507403, Validation Loss: [not yet available], prediction: 0.04892696440219879, true ratio: 0.0640999972820282\n",
            "Epoch: 166, Train Loss: 0.1656734338030219, Validation Loss: [not yet available], prediction: 0.048051245510578156, true ratio: 0.0625\n",
            "Epoch: 167, Train Loss: 0.1649313577450812, Validation Loss: [not yet available], prediction: 0.048915136605501175, true ratio: 0.2962000072002411\n",
            "Epoch: 168, Train Loss: 0.16579224746674298, Validation Loss: [not yet available], prediction: 0.04883657768368721, true ratio: 2.1512999534606934\n",
            "Epoch: 169, Train Loss: 0.1651078806258738, Validation Loss: [not yet available], prediction: 0.048917144536972046, true ratio: 0.8458999991416931\n",
            "Epoch: 170, Train Loss: 0.16555472249165176, Validation Loss: [not yet available], prediction: 0.04859204962849617, true ratio: 0.44269999861717224\n",
            "Epoch: 171, Train Loss: 0.16499135727062822, Validation Loss: [not yet available], prediction: 0.04892629384994507, true ratio: 0.018699999898672104\n",
            "Epoch: 172, Train Loss: 0.16545307114720345, Validation Loss: [not yet available], prediction: 0.048839129507541656, true ratio: 0.4821000099182129\n",
            "Epoch: 173, Train Loss: 0.16546879820525645, Validation Loss: [not yet available], prediction: 0.04874764755368233, true ratio: 0.07169999927282333\n",
            "Epoch: 174, Train Loss: 0.1660121913999319, Validation Loss: [not yet available], prediction: 0.04802033677697182, true ratio: 0.28519999980926514\n",
            "Epoch: 175, Train Loss: 0.16558133382350207, Validation Loss: [not yet available], prediction: 0.02839433029294014, true ratio: 0.121799997985363\n",
            "Epoch: 176, Train Loss: 0.1657908434048295, Validation Loss: [not yet available], prediction: 0.033241380006074905, true ratio: 0.03420000150799751\n",
            "Epoch: 177, Train Loss: 0.1650048766285181, Validation Loss: [not yet available], prediction: 0.04886519908905029, true ratio: 0.0812000036239624\n",
            "Epoch: 178, Train Loss: 0.1652781660668552, Validation Loss: [not yet available], prediction: 0.04815753921866417, true ratio: 0.23800000548362732\n",
            "Epoch: 179, Train Loss: 0.16551996767520905, Validation Loss: [not yet available], prediction: 0.04861011728644371, true ratio: 0.053199999034404755\n",
            "Epoch: 180, Train Loss: 0.16553918411955237, Validation Loss: [not yet available], prediction: 0.048743750900030136, true ratio: 0.08489999920129776\n",
            "Epoch: 181, Train Loss: 0.16519679594784975, Validation Loss: [not yet available], prediction: 0.0555228590965271, true ratio: 0.25200000405311584\n",
            "Epoch: 182, Train Loss: 0.16519281091168522, Validation Loss: [not yet available], prediction: 0.04890093207359314, true ratio: 0.08799999952316284\n",
            "Epoch: 183, Train Loss: 0.165675637871027, Validation Loss: [not yet available], prediction: 0.048919834196567535, true ratio: 0.08780000358819962\n",
            "Epoch: 184, Train Loss: 0.16550007220357657, Validation Loss: [not yet available], prediction: 0.04677773639559746, true ratio: 0.21119999885559082\n",
            "Epoch: 185, Train Loss: 0.16530846506357194, Validation Loss: [not yet available], prediction: 0.04892337694764137, true ratio: 0.2198999971151352\n",
            "Epoch: 186, Train Loss: 0.16514297779649495, Validation Loss: [not yet available], prediction: 0.014172011986374855, true ratio: 0.28049999475479126\n",
            "Epoch: 187, Train Loss: 0.1651319745928049, Validation Loss: [not yet available], prediction: 0.04892696067690849, true ratio: 0.0625\n",
            "Epoch: 188, Train Loss: 0.16526907496154308, Validation Loss: [not yet available], prediction: 0.0489359050989151, true ratio: 0.018699999898672104\n",
            "Epoch: 189, Train Loss: 0.16533424519002438, Validation Loss: [not yet available], prediction: 0.037281230092048645, true ratio: 0.08489999920129776\n",
            "Epoch: 190, Train Loss: 0.16538280099630356, Validation Loss: [not yet available], prediction: 0.048931919038295746, true ratio: 1.194200038909912\n",
            "Epoch: 191, Train Loss: 0.16572166569530963, Validation Loss: [not yet available], prediction: 0.0489388033747673, true ratio: 0.17569999396800995\n",
            "Epoch: 192, Train Loss: 0.16571494818199425, Validation Loss: [not yet available], prediction: 0.017452673986554146, true ratio: 0.16830000281333923\n",
            "Epoch: 193, Train Loss: 0.16541030751541258, Validation Loss: [not yet available], prediction: 0.06968293339014053, true ratio: 0.029200000688433647\n",
            "Epoch: 194, Train Loss: 0.16492958627641202, Validation Loss: [not yet available], prediction: 0.04684007540345192, true ratio: 0.21539999544620514\n",
            "Epoch: 195, Train Loss: 0.1654458936303854, Validation Loss: [not yet available], prediction: 0.04893464967608452, true ratio: 0.03720000013709068\n",
            "Epoch: 196, Train Loss: 0.1653899156488478, Validation Loss: [not yet available], prediction: 0.04894275218248367, true ratio: 0.08940000087022781\n",
            "Epoch: 197, Train Loss: 0.16561289597302675, Validation Loss: [not yet available], prediction: 0.04836446791887283, true ratio: 0.953499972820282\n",
            "Epoch: 198, Train Loss: 0.16534519344568252, Validation Loss: [not yet available], prediction: 0.04390108585357666, true ratio: 0.21119999885559082\n",
            "Epoch: 199, Train Loss: 0.16529568750411272, Validation Loss: [not yet available], prediction: 0.04893774539232254, true ratio: 0.14409999549388885\n",
            "//content//model_RatioNet2_bs10_lr1e-09_epoch199\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9edwkV1kv/n2qqrvfZdbMTEgyQ0gCISQgBJyAcAEDuLAJygUFvGJQfgjC5V4VWfSCgMSrNyqCqD9AIcZLRJAEggazIJFgWLINSWYmy2QySd7Z93m37q7l3D/OeaqeOnWql/ft7nlnpr6fz3ym3+ruqlPVVec53+f7LKSUQoUKFSpUqDAIeMd7ABUqVKhQ4eRBZVQqVKhQocLAUBmVChUqVKgwMFRGpUKFChUqDAyVUalQoUKFCgNDZVQqVKhQocLAUBmVCicNiOibRPSrx3scCwERXUlEHzevX0RED/Ty2QUea4aIzlvo9ytU6ITKqFQ4rjATHP9LiGhe/P3L/exLKfUKpdTfD2usnUBEbySiHURE1vaAiPYR0at73ZdS6lal1AUDGtctRPQ2a//LlFLbB7F/61g7iOinBr3fCicWKqNS4bjCTHDLlFLLADwG4OfEti/y54goOH6j7AlfA7AKwE9a218OQAH4t5GPqEKF44DKqFRYkiCiS4loiojeT0R7AHyBiFYT0b8Q0X4iOmxebxDfSVflRHQZEX2XiP7UfPYRInpFybHeT0T/bG37JBF9SuxrOxFNm/0UGJRSqgngywDeYr31FgBXK6UiIvoKEe0hoqNE9B0ienqncxd/P5uI7jLH/ycAY+K90mtCRJcDeBGATxvm92mzXRHRU8zrlUR0lfn+o0T0v4jI6/cadgIRNYjoL4hol/n3F0TUMO+tNWM+QkSHiOhWcfz3E9FOc94PENHL+j12hdGjMioVljLOAHAagCcBeDv0/foF8/fZAOYBfLrD958H4AEAawH8HwB/Z7unDL4E4JVEtBwAiMgH8IsAriaiSQCfAvAKpdRyAC8AsKnkeH8P4PVENG72sxLAz5ntAPBNAOcDOB3AXQC+6NqJBBHVoVnQP0Bfi68A+K/iI6XXRCn1+wBuBfBuw/ze7TjEXwJYCeA8aJb1FgBvFe/3eg074fcB/ASAiwE8C8BzAfwv897vAJgCsA7AEwD8HgBFRBcAeDeAS8x1/1kAO/o8boXjgMqoVFjKSAD8gVKqpZSaV0odVEp9VSk1p5SaBnA5iu4miUeVUp9TSsXQE/uZ0BNXDkqpR6En+V8wm14KYE4p9X0xjmcQ0bhSardSarPrYEqp/wSwV+znFwE8qJTaZN7/vFJqWinVAvARAM8yhqcTfgJADcBfKKVCpdQ/A7hdHLPfa5LCGM83AvigGdcOAH8G4FfEx3q6hl3wywA+ppTap5TaD+Cj4hih2eeTzPndqnRBwhhAA8BFRFRTSu1QSj3c53ErHAdURqXCUsZ+41YCABDRBBF9xrhpjgH4DoBVZnJ0YQ+/UErNmZfLSj57NYA3mddvNn9DKTUL4JcAvAPAbiL6VyJ6WocxX4XMBfYr5m8QkU9Ef0xED5ux7zCfWdthXwBwFoCdKl/59VF+sYBrIrEW2mA9KrY9CmC9+Lufa9jpHOxjnGVeXwFgG4AbjYvxA+ZY2wD8T2jju4+IvkREZ6HCkkdlVCosZdgltH8HwAUAnqeUWgHgxWZ7v+4YF74C4FKjR/wCjFEBAKXUDUqpn4ZeUd8P4HMd9vMPAF5GRM+HZhns4nozgNcC+Clod9M5PY59N4D1lsvpbPG62zXpVIb8ADRTeJK1751dxtQvdjmOsQsADEP6HaXUeQBeA+C3WTtRSl2tlHqh+a4C8CcDHleFIaAyKhVOJCyH1gyOENFpAP5gUDs2bplboPWJR5RSWwGAiJ5ARK812koLwAy0O6xsPzsAfBfAPwK4SSnFK/3l5vsHAUwA+KMeh/Y9ABGA9xBRjYheB61JMLpdk73QeolrrDF0cMHlRLSciJ4E4LcB/N8ex+ZCjYjGxL8A+lr8LyJaR0RrAXyYj0FEryaipxijeRTa7ZUQ0QVE9FIj6DfNOZZe9wpLB5VRqXAi4S8AjEOvsL+PwYfpXg3NJK4W2zzoiXYXgEPQesU7u+zn76FX11eJbVdBu312AtgCPf6uUEq1AbwOwGXm+L8E4BrxkW7X5JPQwQOHOZrNwn8HMAtgO7QxvBrA53sZWwmuhzYA/O8jAD4O4A4A9wC4F1q/4uTN8wHcDG2svwfgr5VS34bWU/7YnNce6OCGDy5iXBVGBKqadFWoUKFChUGhYioVKlSoUGFgqIxKhQoVKlQYGCqjUqFChQoVBobKqFSoUKFChYFhqRfpGyrWrl2rzjnnnOM9jAoVKlQ4oXDnnXceUEqtc713ShuVc845B3fcccfxHkaFChUqnFAgokfL3qvcXxUqVKhQYWCojEqFChUqVBgYKqNSoUKFChUGhlNaU6lQoUKFfhGGIaamptBsNrt/+ATH2NgYNmzYgFqt1vN3KqNSoUKFCn1gamoKy5cvxznnnIP++5WdOFBK4eDBg5iamsK5557b8/cq91eFChUq9IFms4k1a9ac1AYFAIgIa9as6ZuRVUalQoUKFfrEyW5QGAs5z8qoVKhQoRT3Th3Fjx4/cryHUeEEQmVUKlSoUIo/+bf7cfn1W4/3MCoIHDx4EBdffDEuvvhinHHGGVi/fn36d7vd7vjdO+64A+95z3uGOr5KqK9QoUIpmmGMKKl6Li0lrFmzBps2bQIAfOQjH8GyZcvw3ve+N30/iiIEgXtq37hxIzZu3DjU8VVMpUKFCqUI4wRxZVSWPC677DK84x3vwPOe9zy8733vww9/+EM8//nPx7Of/Wy84AUvwAMPPAAAuOWWW/DqV78agDZIv/Zrv4ZLL70U5513Hj71KVdj0P5RMZUKFSqUIowVkqo7bCk++o3N2LLr2ED3edFZK/AHP/f0vr83NTWF2267Db7v49ixY7j11lsRBAFuvvlm/N7v/R6++tWvFr5z//3349vf/jamp6dxwQUX4J3vfGdfOSkuVEalQoUKpYiSBJVNOTHwhje8Ab7vAwCOHj2KX/3VX8VDDz0EIkIYhs7vvOpVr0Kj0UCj0cDpp5+OvXv3YsOGDYsaR2VUKlSoUIoorixKJyyEUQwLk5OT6esPfehDeMlLXoJrr70WO3bswKWXXur8TqPRSF/7vo8oihY9jsqoVKhQoRRhkoBwauRknEw4evQo1q9fDwC48sorR3rsoQr1RPRyInqAiLYR0Qcc77+YiO4iooiIXm+9dzYR3UhEW4loCxGdY7YTEV1ORA+a995jtv8yEd1DRPcS0W1E9KxhnluFCqcColghipPjPYwKfeJ973sfPvjBD+LZz372QNhHPyA1JIcpEfkAHgTw0wCmANwO4E1KqS3iM+cAWAHgvQCuU0r9s3jvFgCXK6VuIqJlABKl1BwRvRXASwBcppRKiOh0pdQ+InoBgK1KqcNE9AoAH1FKPa/TGDdu3KiqJl0VKpRj48dvhkfAD3//p473UJYMtm7digsvvPB4D2NkcJ0vEd2plHLGJg/T/fVcANuUUtvNIL4E4LUAUqOilNph3ssthYjoIgCBUuom87kZ8fY7AbxZKZWY9/aZ/28Tn/k+gMWpTRUqVECUJPBPkZIkFQaDYbq/1gN4XPw9Zbb1gqcCOEJE1xDR3UR0hWE+APBkAL9ERHcQ0TeJ6HzH938dwDddOyait5vv3rF///4eh1OhwtJAGCf44g8eHVnuSBSrKvmxQl9YqsmPAYAXQbvFLgFwHoDLzHsNAE1DvT4H4PPyi0T0Emij8n7XjpVSn1VKbVRKbVy3bt1wRl+hwpDwg+2H8PvX3odNjx8eyfHacVJpKg4MSzZYaljIeQ7TqOwE8ETx9wazrRdMAdiklNqulIoAfA3Ac8R715jX1wJ4Jn+JiJ4J4G8BvFYpdXARY69QYUliPowBAK1wNBN9FCcVU7EwNjaGgwcPnvSGhfupjI2N9fW9YWoqtwM4n4jOhTYmbwTw5j6+u4qI1iml9gN4KQBW1L8GLdQ/AuAnoYMBQERnQxubX1FKPTiws6hQYQmhHWlj0h4Be0gShUShKtNiYcOGDZiamsKp4D7nzo/9YGhGRSkVEdG7AdwAwAfweaXUZiL6GIA7lFLXEdEl0GxjNYCfI6KPKqWerpSKiei9AL5FuqD/ndCuLgD4YwBfJKLfAjAD4G1m+4cBrAHw16YHQFQWnVChwomKdqyZSjiCpMQw0YYrShSUUqdMD5FuqNVqfXVCPNUw1ORHpdT1AK63tn1YvL4dJVFaJvLrmY7tRwC8yrH9bcgMTIUKJyXCSBuTUegcMps+ThQCvzIqFbpjqQr1FSpUcKAVj879JY1KpatU6BWVUalQ4QQCayqjdH8Bla5SoXdURqVChRMImVEZMVOpCktW6BGVUalQ4QQCG5VRaCrScEVJlatSoTdURqVChRMIHP3VHoX7K67cXxX6R2VUKpRivh3jY9/YgqPz7gY/FYaP6360C3fsOJT+PVL3V1IJ9RX6R2VUKpTi1of24/P/+QjufPRQ9w9XGAr+9IYHcNX3Hk3/ZoE+jEbs/qo0lQo9ojIqFUqxdfc0AKAq/XT8ECcqN7m3mKmMgDnkQ4qrm6BCb6iMSoVSbNl9FED//vRWFOPh/TPdP3gCIk4Udh6ZH9nxoiTJhQ+P1v1VaSoV+kdlVCqUgplKv4XzrrlrJ175yVsx346HMazjipu27MGlV3wbh2fbIzlenKjc5M5Jj6Nxf1WaSoX+URmVCk5MN0M8dmgOABD3aVSOzIVoRQla0clnVA7OthHGCkdGFLwQJyrnhmpHXPurylOpsDRRGZUKTty/Zzp93a/rIxaFCE828LVoj4ApAPoaSgPSHqGmIjPqK02lQq+ojMoJgjhR+Jd7do2sh8OWXcfS10mfx2S3SXISG5VRsTDt/hJMZYTuL7ugZIUKvaAyKicIfrD9IN599d24Z+roSI63dXdmVPr1tPAEdDIzldYImUrkYiojz6g/+X7LCsNBZVROEMwZ0XtUk9mjB+fwhBUNAP0zFZ6ATsbVbTRi95cOKZZMxeSpjDijvtJUKvSKyqicIGCfdr8T/GKON17z9TErTSXFKN1fSqli9NfxKihZaSoVekRlVE4Q8AQ9Kp0iThRqvr49+o3+ypjKyTcRpUZlBD3i+ac+btFfVZ5KhQWgMionCHhiGdWzHSukRqVfQ8ZjPbmZyugm9dCVpzIS91eVp1KhfwzVqBDRy4noASLaRkQfcLz/YiK6i4giInq99d7ZRHQjEW0loi1EdI7ZTkR0ORE9aN57j9j+KXOse4joOcM8t1GDV6ajcn/FSYJaYJhKv0blJNZURhlSnAY8ODLqR9P5sdJUFoMwTvDJmx/CXDs63kMZKYZmVIjIB/BXAF4B4CIAbyKii6yPPQbgMgBXO3ZxFYArlFIXAngugH1m+2UAngjgaea9L5ntrwBwvvn3dgB/M6hzWQpIJ+qRGRWgwUylz0Oy2+tkNCrRCDUVPtbxK9NSaSqLweZdx/CJmx/EbdsOHu+hjBTBEPf9XADblFLbAYCIvgTgtQC28AeUUjvMe7k71hifQCl1k/mcLCT1TgBvVkol5j02Nq8FcJXSiRzfJ6JVRHSmUmr3ME5u1OBV46jyVJJEoRaQfr1ATeVkdJnwtRiF+ytJr2NRqB8FcwirPJVFgQ3/KFjlUsIw3V/rATwu/p4y23rBUwEcIaJriOhuIrrCMB8AeDKAXyKiO4jom0R0fj/HI6K3m+/esX///r5O6HiCH/BR3Z9RkmRC/QI1lbLv/dmND+Dz331kcQM8TuBzG42mUnR/paXvR+3+sn7L2VaEfdPNoY/hRAYnqI4q/HypYKkK9QGAFwF4L4BLAJwH7fYCgAaAplJqI4DPAfh8PztWSn1WKbVRKbVx3bp1gxvxkMET9Kg0lUQI9f263FxagMRNW/bi1odOHIMuMUqmEid5A6KUSle9o1j9dspT+fObHsSbPvv9oY/hREY4Qv1tKWGYRmUntPbB2GC29YIpAJuUUtuVUhGArwF4jnjvGvP6WgDPHMDxljw4AmiUIcV1Y1T6tWNRF00lskqPnEjgcxulppImXIpJfiQZ9Yl0f+WPd3i2jUcPzp2UpXgGBWZ6rcr9NTDcDuB8IjqXiOoA3gjguj6+u4qImEq8FJkW8zUALzGvfxLAg+b1dQDeYqLAfgLA0ZNFTwGOQ0hxolDzKX3d73eBcnE3ipORMa5BY5R5KrFwIyqlciveUWgqndxfvDA4NDeaFgAnIsIR1mlbShiaUTEM490AbgCwFcCXlVKbiehjRPQaACCiS4hoCsAbAHyGiDab78bQrq9vEdG9AAja1QUAfwzgv5rt/xvA28z26wFsB7DNfPY3h3VuxwP8gI8u+kstXFPp4qoLY3XChqjGDtYwtGOJ6xfGeaMymtpfCp5eVxR+L74Oe481cdvDB/CJmx60v37Kg/WvU02oH2b0F5RS10NP9nLbh8Xr26HdVK7v3oTMtSW3HwHwKsd2BeBdixzywBAnCp+46UH8+gvPxerJ+qL3x66IfqK/bnv4AJ5+1kqsHK/1fbxYKQQ+gWgB0V+c/FhiOMITmKlEo2QqVul5npzGat5I/PRRkmCs5mOuHReYChuVfdMtXLdpF27YvAe/9dNPHfqYTiQwU680lQoDwfb9M/j0t7fhOwMSpCNH8uPH/2UL3n7VHc7PH50P8d/+9gf4yh2PO9/vhiRR8IjgEZUylb+9dTte+clbi2M9iTUV1hBGqakAeaYyWQ9GklEfxQpjpv6branw2PYda+LxQ3MjYU4nGsKoEuorDBCDzirP9pdt23FwFjsOzjo/v+vIPBKFBbf0jRKFwCP4RKU6zvYDs2l3SIlupe/DODlhBd4s+XHUnReTdOKebAQjSUYMY4UxU1XB/i15cbP3WAuPH55DGKsT9jcdFji45lQzuJVRGRLiQRuVVKjPJ6SVuZh2HZkHsPAOgUmi4HkEzyt3f7WjxDm5SYN679RRvOH/vw3NMDNuUdydqRycaeGRA26DeTzB12KUZVoAfU3ZkE02NFMZdiJsZEr1+B4V7jP+/R47NIe9x1oA8jXKKmQC/ajaVSwVVEZlSBg8UymGFMeqnA2wUYkWuEqKlYJPmqmUnUM7SpzvyYKS9+w8gtt3HMbeY1miXJS4vyfxiZsfxP9X4to7nsiSH4fv/soL9Ylwf/lm25CNSmzYqkcOTUWP5a7HDosxVkxFwhUKfiqgMipDQlr/yrGavHHzHhya7S8UM3SEFHO/DRd2HtGTeJnRefzQHD5y3eaOuofvEzyv3Ki0oqKAK48ZJ0k6Cc8bpqKUbjrVzagcngtxZC7s+JnjgVFWKc4xFampNHR8zbDdKu1YV1UIPCpoKjy27fszNnmqaQfdkEZ/nWLXpTIqQ0LqrrImz/l2jN/4v3fiq3dO9bm/opHSXQHdN2zGVNyT9y0P7MOVt+3A7qPzzveTRDMVj6ij+0up4jlmBSWziY+1nV7dgq0wSXuHLCXw9R9F9JfdJItXvMuMURl2WHYUJwj8MqZSPPapph10Q5qncopdl8qoDBBRnOCvb9mGZhiXitXzYQylspV7z/t2hBTHSfmKPzUqJX5ubk9cNjHFSsE3ro9SoxK7OzzmmEqSZyq9VltuRfGSdKeMNE+lJPprwri/hj2GyOQq1XyvNE9F4lRbkXcDLwRPtesy1DyVUw0/mjqK//NvD+DCM1fAJ3c2Ovvi+9U6XEUaE1UueKdCfcnEzEbFtYpKEgWlAN/jkGL3mNKKuUmCulifSINqZ6Dz8bqtslthsiR90aNsJywXBMfD/RXGCWqe1zNTWYq/1/FEu3J/VVgsWmY1HgoBu2BUeHLtO0ud81SybYlyG6coTrDHCONlLX2ZObgmAmYRPhF8r7zeWGZULKYiDGDq/grzzKhb8mMzijsyseOFUSY/ymsUCvfXZIOF+iEzlVgnwLo0Fddi5lRz83RDylROsetSGZVF4IPX3Is/F+UpuHBcLJL7bDcPC7z91gNixmG7v1wP997plrO/uQR3o3O9zxO5x0ylxADwucSFcNPM2KRCPTOjxG2ICvsOl6brYKTthHN5Ki6mMlyDGyYKgV/OVNYtbwAA1q8a15+PltYC4HhjlDlNSwmVUVkE7nr0MO6ZOpL+LVfuqVhtPfhlq/tucGWpJyXRX+z6AsoZUUf3FzMVr7tQr8fmdo3EiUqNiM1Uugr1xr20VI3KyPNU4qJQP3ymkqDmUammsmG1NiZPOX0ZAKAdL73AiuOJ9ikq1FeayiIQxknuYZMaA5G7FwlPlv3eaK6QYmYqSimQ0XCAzKi43BYMZg4uas5GgnMUytxfKVMpFeozptLs06g0Qy4bHgPov3bZsCA1Ffu6Dxq5Mi2JFOpHZVRUGv1l/16xUtiwegJve+F5CHzCfzy4H+2KqeRwqgr1FVNZBNpxPqOcJ9lI5GEUhfrehGobriZdvGv7GDuNUVm/erwHob74PhsRj8yEUjLULPrL8reL5EceW9PScE50ppKo/tnmQo8FGKYSMVMZjaYSJgkCk6di/8ZxouAT8Kpnnok1pmDqqbYi74aoEuor9It25GYqcoVeFv3V7wOYFpSU7q+SsOV9x1pY1giwYqxWGmU23y6PQuMxa/dXd6G+yFR4e1IU6rsUm2Sw8V1qD6Qc97DHVlpQckCailKdS72Exv3lYipRrOB7evqom/pgS+23Ot4YZZfOpYTKqCwCYZzkHnxO1guTDkxlgdFfaY96KdQrt1FpxwnGah4CvyiwMubCKD0HG3mj0rlMi+v4MqQ4E+rzDK2TUVFKFZjNUoGLmQ4L0pjL5Mfx2mCYyuv+5jb85b9vK30/inWeSuB7BQOWKAXTbiftu1MxlTz4Xq+adFXoGXZBxdS1I4xNufvLfaMdnGnh7777SGEF6Q4pNsewo6/iBIFn3BZd3F9tx/uxEOo7JT8y6yrVVOJMqG9aDK1T8mOUqPQ8l9rqV57qsHNVpLHm6K964A1sEn943wy+vqm843YYq9T95fqNmanweJbaAuB4I6yYSoV+YXcwdEZ/9SnU37hlL/7wX7Zg19Fmbrur7Au/tqvDZvkFXmlGfRri65i0U6bSoUyLbgmcHxuPiT8umUqzbWXUJ+WuF1nReKmtfnUQhn497FwVGWQRmuivhp8ZlcUK41Gi8PD+2VSDK76foJaWacmfa5JkTKVRub+cYG9EFVI8QBDRy4noASLaRkQfcLz/YiK6i4giInq99d7ZRHQjEW0loi1EdI7ZfiURPUJEm8y/i832lUT0DSL6ERFtJqK3DvPclFJGqM8e7JYwKt2YSpk/nBlMyyrj4mrRywbLPkaYsNuCugr1LqMj3V8ufzqQX33lyokk+e2RFVIclnxPQj6ES+2BTJLM/TSKMinyNTOVekBm2+KOz/u/9UF3IzldpdjNeHW/nTxTWYpldY4nquivAYOIfAB/BeAVAC4C8CYiusj62GMALgNwtWMXVwG4Qil1IYDnAtgn3vtdpdTF5t8ms+1dALYopZ4F4FIAf0ZEi+/jWwJ+gKQbSwrX3TSVsgmhLGHKVVCSd2FrGmGUIPDI6bZgzHdyf0lNpST6Sz4oUeI2FLESmooVUmyfi4Q896X2QEZJkob0Dp+pFKO/6oGXTuaLZXF8T5V1Jw1jzVQC33PqZr5pYJ8J9VWeikRVUHLweC6AbUqp7UqpNoAvAXit/IBSaodS6h4AuatujE9g+tRDKTWjlCq2GMxDAVhOOnFgGYBDAKLBnEoRaQ0rB1MJO2oq7HZyT6hlyXVZj/psG7MWW5/RbgsWWB15KCKRzuX+4v16HaK/5PjsZlLpdtGMqxkWmVGZwZPur0EalelmiN/6p004MNNa8D7iJCvoOGxNpVBQMjaaipnEF5PBngjd6rsPHXD3xUlkmZZyo1LzKR1jhQwyt2yhfY1ORAzTqKwHIBukT5ltveCpAI4Q0TVEdDcRXWGYD+NyIrqHiD5BRA2z7dMALgSwC8C9AP6HUmpov2Sqn0hNJe6BqbDhWSBTyTXpKgkpDmOFmk+olUR/zXXRLHhT1k64uA85PjmZyKCBKJG1v4puv1L3l2AAg3QxfX/7IVx7907c9ejh7h8uQZwkwqiM0v1lmIrvpZP4Yq4N33/rV43jWDNKa8UxuFdP0KGgZGZUKqHeBVcQz6mApSrUBwBeBOC9AC4BcB60mwwAPgjgaWb7aQDeb7b/LIBNAM4CcDGATxPRCnvHRPR2IrqDiO7Yv99N+3uBi6lIob48T6Vz8mNZFdx0f7nkR/cxojRpzXNO3LJvvbvJlh6jR+VNulq9MBXRpKvpKLXfqfkXY5BMZdu+GbP/he8zSlRWen7IRsVV+r7me6gPIPqL980lX+wIQjb+NcNU7JU2dwYFkI5nqbkqh4nbth3Asz56I6ab5Y3kJJM8Mhfilz7zPTy0d3oUwzuuGKZR2QngieLvDWZbL5gCsMm4ziIAXwPwHABQSu1WGi0AX4B2swHAWwFcY97bBuARaOOTg1Lqs0qpjUqpjevWrVvQiQFSlC+KylGclEd/OQRribIsXF5Zyt3xxGDvKzRtYAOPnMeZa3eetPmU/A5MpUxTyb8uCvW9ub+Go6mwUWn22ctGIklUpqmMOqTYuL8CM4kvpkkXGw3WQ+z7lO+bwLhR5W/FLIaZitfhXjtZsePgHI7Od+5OKr0RD+6dxg8eOYR7dx4dxfCOK4ZpVG4HcD4RnWsE8zcCuK6P764iIp71XwpgCwAQ0ZnmfwLw8wDuM595DMDLzHtPAHABgO0DOA8n0nyLXEhxFjYblbimpO7iAhsjezXtcqfxy0L0Vzr5uPNUuEJx2TiyPBX0Hf1lMxFbU+nJ/SWZygAnqm37jVFZJFMZH5H7K04S+B6BSLi/gsG4v5h5cDhwobR9ylS4TEvxdwu8rO5ZrUS/GyXCOCm9pwaNssRjezyMw3Ptrp8/WTA0o2IYxrsB3ABgK4AvK6U2E9HHiOg1AEBElxDRFIA3APgMEW02342hXV/fIqJ7ARCAz5ldf9FsuxfAWoDamSMAACAASURBVAAfN9v/EMALzHvfAvB+pdSBYZ1fKnQnxVW11FRskVuGHbvA2+UKnfu6A1btrxLDFTFT8d15Kt3cXzzB+J4HIvQQ/eU2FLmM+jT6y/09iWEwFaUUHmb312KYisrcX8OP/tJGveZ5qVDfCDzUBhD9xdd+zIRH27vi+7rmKCgZi0AORj3wjpv7K0kU/uF7O3DxR2/E39xSXiFg0McEOgvwUaxSo31wpp373smMoVYpVkpdD+B6a9uHxevbod1iru/eBOCZju0vLfn8LgA/s5jx9gP2l7qEei2UlzGV8qRDwK2p2OXu0+2qOAZ9fFkI0MVUOru/+DnxTUFJV5JiL9FfiRDqU6ZSYoAkhsFU9h5rYaYVmf33vs9X/+WteMvzz8EvbtSe3Ei6v4a8Mo+TJGUDUZygFSZYM+kNxN3Ev1PGVIoLEwBpnoorv8hmKq7w9E6YbobwPcJEPcCuI/NITOXjfnHt3Tvxoa9vBgBMHXYncg4aZZ4IiTDWC5BWlODQbMVUKnSB7M/Ok27GVJICU3lw7zRCMzEA5bW/mJG0SpiAJB5Z9JcdUqyMwFrsgwHkjUqn2l+epw2LW6h3s52ipsK6j0IUJzlj2kvy46BWv6ynAP1pKlt2HcPDxm3G1QIypjJ8TcU3BiRKFObaUWrQylybPe+b3V81t1HJNBVC4FtMReQxMep+/0buXVffjQ99TRuDD33tPnzwmnv7PAuN+/ccQyPwsGH1+MiirPi57uRuC+Msp4mNylLrZDoMVEZlgXCt1PMZ9ZlQf2SujVd+8lZ840e7utb+YteT3H9ulShYA790TQiBp33vrgd9PuyiqSTZKrUs+it//sKQ2JqK2H/TqpVWmvw4hDyVbfuyqJtejQrnctiRd6MKKdYhvZTqFbPtOG0lrJnBIkKKY2Yqxv1VqDUno788p4vTX6T7a9+xZpozNN2KOorenbD7aBNnrhxDPSgWvhwWssVSJ/dXkkbXVUylQlfIm8nWQex+KjOtCFGisPdYS9T+6qyp5JiKrK3Vg/srqy7rNgjMVBqBm8lIod4jdz95OaGVhQlHSb6MzXw77lGoHwJT2T+D5WMBVk3UcppNJ9hlcPj/8RGFFKdMxbCS2VaESbPyrS9SGOdzKXd/GaZi8lRcTGWxQn07TtJ7K0nUgqPp9hxt4oyVY/qa9PCbKKXw7Qf2LYo1lIXzS4SJwoRZBLBRORU0lcqoLBAuoVoK9WmpeiHaz7TCHqK/Oru/XCHFxeRHLgTo5dxzDBbqV4zXnKtdu0mX6znoLU9Fmb4bevJphnGPeSp635N1f2DujEcPzuG8tZMYC/yeJ6+s2Vj2uwJ6Mq0H3tCYyqe+9RBufWi/KdqomUIYJ5hrx5gwK9+a7y0qo57vvzKjYuepuDQVb5FGRXZOjZVa8PXUTGW85zFs3T2Nt37hdnzv4YMLOh5Q/uxJhHGSLgIqplKhK3IPmaWDREJTiYSBmW5GovZXN6YiNYviAy0NRSEcNNHlNWos8lrHYqaycrzW3f1F7nbC3aK/PMrcgOwCmA/jnsu0EOlmVINiAzOtCCvGaxireT0zFR6r7GQJ6Ki4RuANLU/l7777CK6/d3datLHmE46ZJLtJw5ICn0qrMvSCVKhPo79s91fGVGzGK9tNMxZiZNtRkrLBJFGFaLpeDESSKOw9pt1fNZ96WoRwwIYMre8XZWWYGLGlvx1MNZXBL0SiOMGn//0hbN19bOD7Xggqo7JAyJuXH+5UvBc1rxLJVJpRrgeJa7JOmYp4wFzuLzvbWoI1FU6Ss2/82XaEuu9hvOY73XA8do/zVPqJ/kpXwH7K2JaPaaPSDPPurzLD2ooSjAX+QMNUm2GCRuBjrOb3rKnYq1H+vXzS5zcsphLGCVpRkiYYBr6Ho/PGqDSk+2sAQr1hKraLkyf0WuAVGG+2cJBC/UKYSvYMaKaS/S6HZtv4sY/cgB9s78wmDsy2ECXKGJXexiA9CnPtCG/67PfxwJ7+Mt3TthMlx+Pt/Hvx7zeMOIJ7dx7Fn974IF71qVvxt7duH/wB+kRlVBaItkPzaIVZ8iOvSGT9q2PNKF8zy7FqSV1p0miJ16lRyTEVt/uLV5L2jT/fjjFe901pfIf7SzbpKo3+co8vy3/wUqF++VgtPW7kcKPYaIYxGjVPr34H9BS2whhjNQ+Nmt9z8mMaNmqV5PF9zVSGpamwUYmSrDJCZlQyoX4xHQXtkGJX/TgAqHnZfcQfSZmsn2cq/Rq5UDCVKM67vw7OtNAMk64hwntM36EzVo47x8DXLXfcNMdMYdeReXxv+0HcsHlPX2MvazthH4N/r/R7Q2AqnAM2UQ9w9Q8fG/j++0VlVBaI/IrbYiqi5lWi3JoK4C6zkWbUh8WJGshCiuW9aRsGKdS7jjPXjjFR9wsru4MzLRydC7PoHiJQD+4vl4jLTCVKFJbn3F89aCqhTvKr+4NkKjHGar52Wy2QqcjrsnwsSKOVHj80h71WQcaFIjHsrh3pUj+eif7iY3GIai3oHMLbjpKO/ef5nuDkR/s3TvNUxH1kd+2UTKXmU9+/VVtkwCeWphJaelYZdhujwkxFjuETNz2Iiz92I6YO5wucZ4Vfs0CS+/osn9JNU+Hrx79Xun0Imgpft9NXNIaekNsLKqOyQMjeEXa9Lhn9pV1hervWVOJUuA7jBF+9cwo7DswW9tVy7B/IHuikhKkopYymkrm/bEbETMV2obzr6rvwB9fdlwsZ9T04hfp2XJanol/XAy/NU8m7v7ozlVaUGYCBGZUowVjN0+6vHveZTqLCRQNoLeHMlWPYfVSvot919V14/1fvGcg4+bdi91dgor9SpsJ5Kp5Xmus004rw4394E27eus/5vjxOvSz6izUVwXjtKDju6wL0L9Rzkzu5z1iw+qyuXedJOGMqY7kQ+i/98DF88lsPQSkdpCEhn1N+tjbv6k+PcJVNkuBxsKZif2+Q4EXS6on6ouraDQqVUVkgbG1AtteVNa8S0ahqpqXdXyy2hrHC+756D758R9YhgG+6soKNyuH+ck3q0m1h38g6ic4v5LHsPdbCIclUuPNjiabimpCkrz4yk8ay1KgkPUV/NZmpBIOrJ9UMY4wFPsYWwFTSSL44uy5nrhrH7qNNKKWwff8s7ts5GJE0TBcocaqp1LxMBGd3Sqfw2cOzbUy3Ijx+qLwFEf8OZe4vGenGvegzd2B2HRj1oL+8GRayM6ait9uFWrv1Idl9tIm67+G0iXoud+ez39mOs1aOAUChf440XHxOO4/MpxFavY5f7ssGG3wOUrG/N0jwNVs1XktdYccTlVFZIHJ5GkliCfcqR4/5xj0yFyJKVHqjzbUjxIlyRlKV9iuxRGPAqqcl3RZeB/dXLTBNvJTYHuX83B73qC9xf3FbXZdR04K4HhczlfkwzrcbLu38GKMxQKFeKZW6v/oR6jO3V97t43uEs1aO4dBsG3uONTHTinBgpoWDi2j+xWBD0ZaaitAuWPjt5P7iiaVTIAGfU+r+Kkl+9MXiJO3poxxGpU9XpV3LLkrdvpkuKf8vw56j83jCygY8j3LBAnPtGD+2YSUAYP90/neRLSqkxnHfzqP49v37SsvZ6xp8FnstdX8xU+nf/TXdDHHbtt7LFvL9vHJCG5VObs9RoDIqC4Qt1EtfpvbVmgdQUHp2YfDKfbqpQxpdOkN+/1Koz3/O/j4bN92ky114cD6U7q/svbl2nAuHDvwOTMUUN9RdAYsuLRlyu6whhXp5ru5JqGVcVYvNGmeEsc6MH6txKHBv+yxqKvp7vkc4c+U4AOD2HVnDr34jiNxjzbu/OPqLwe6UTu4mzkNqRXqC+eg3Nhc0g9BiKh1rf/l5xusKKe7X/SXLHAGZRmj3G+rm/tp9tIkzV4xnYzC5O+04wdplDdR9Dwdm8gwkx1TE/j9x84N465W34xs/2u081r/euxuXXH4zWlHctUrxYtxf1969E//t737gDDJwgRdvq8brUOr4NwSrjMoCYUc82Rnm0g9v30i82uR4+dBiPUB5ba00A1ns0uV+kkJ90f0VO91f8+0YbTF23zAVZ/RXqN1fdlfALP/BczKVXG2wkgmjGRqmMiChvmmuZd9MhcXiQp4K4cxV2rVy+yOH0s/fPwCj0pJMJWb3l2AqUlMpuX6ch9SOdGmXL/znDvz7/Xl9hX+HtExLiaaSYyrW6tyuUtxP9Bffd3YNLT7/TJPs/PvvOaaz6YE8e2tHOoR87bJ6kamI31Oe992PHQGANCfIxqMH53BkLsR8O05doeWaSma0JaMrY+cSM60IiQKOzPXmjuO5YtWEXrw125VROSFh176yXVjS/2w/bOz+SpmKw73lCtmVJVOSLppKkAspzh8/Cyn2Cu6WSJTO0D3qCa7noBWbni0e5XrKxGKyaqZMJQCRdm20IwUOGnKVf+FzZ01lIEbFGJFGzV9Q8qPt6vA9wvpVzFS0UZmo+4NnKior08JI81Q6uL+awv3F7iTbkHZjKvw3V2aQ28rKtPTzW6W5IpZG2Ep7EmVhv52w91gTT1jRyMYQZ4uyRs3D2uWNgqYiC7/y/p+wopGej2wNIZFLGVCdjV4oFnd1wTTtDpvO7xq21TdTYaMy5OZx3VAZlQVCPtChiLEnypdmSRxMZVnKVMLCvlz9VPj79cBL3QR5dpK9Tm9mz0ujc+ywzPkwxnjNhBSb/fCDJEtnBCb6y7Ua437pNlORkxXbjMAnjAV+ylTKxGEG6x+DKoXCrsmxgKO/evM7p0UDHWVaeHX8wN5pLB8L8KwNq3D/AFrFZkI9R39lUXw1n9LgiI7urzBjKnz9bAE3dVNyleIeNJWshXbGYhi1oLds9uw8mankx5NWnIg7T9r6uwrNMLESQpM0LLvue1i7rGhUZCtwXgT93isvxJVvfS7Gal6p2C1ZVFkvo+wY2eKOf7NOn3eN79h8bxn/zTBG4FFWuaLEKI4KlVFZIGx3FxuB8Zqfy1ORFYsZqfvLMBVXjxGXUF/3PWfSldQm5M2c5qlYN3JoIrdkufLZdsaackylQ/RXIyi2mpWaCiPwCON1Y1RiJRpDdWYqjT4jisrAq3QOU1aqu69ejs/+3/Moda0oBWxYPYELzliOh/ZOL7pgYChW2lxQkt1fUvStdciol5oK30c2O5OVD+S5MWTYsO1Gle2mGQ3DVHoViW0DZd/3vQj1/Fk+B74mfM/UAw/rOhiVWIQUP3ndMrzw/LWYqAel5VvyVcg7u7+kG1oalTJ27hpfr0yFnxcOnDneEWCVUVkg2qKYX5QkKW2fqAc5TSWxxEAgq990LHV/FXUGV0hxPfDTh1bem9Io5TQVZip2GZckQc33clnZqR9e5A74ZHrUlzGVTppKkAmUgadv+Pl2gjBOSl0ujFaU6Iz6QWkqzFSMpgL05iKwNRXb7cNi/YbV43jaGcsx147x+OHyMN5ewBMiJz9yB08gH55a1tYAAOZyTGVh7i++j1yaipOp+J3ZZ+E8I76m+u8kNSpZGSM9zvLfv5VqZRl7ixOVuTsDD2uX13Fgpp27h/PRX9kiDEB6n7rA+41j1YNQz+7DvPurl+vT7tOoyMhGOc7jhcqoLBBlTGWy4efYiYupcPRXJtS7mEox+bERZKygrExLrrqsnw8Flfvjycp2f0XigfGNpuJaXXHYbzH6y6wea4Kp+JqpzLV1C4BuTCUV6gelqYjJp9HHgxdbk2gkjC2gs7gBbVSetGYSwOI7D/L5toxQrzPqmalkhrqT+6vZlpqKcX9ZLhFbqC8LKXZpKmlIscyoD/prcczPj112qJkWXM27wVywmQozAn6uGoF2f8WJwhExQWcZ9RnjYMOpGXU3ppKk2kippiKSRyVT6aVMS1bWqT+mMnYqMBUiejkRPUBE24joA473X0xEdxFRRESvt947m4huJKKtRLSFiM4x268kokeIaJP5d7H4zqVm22Yi+o9hnluYYxIZ5R6v+XlNRdy4DNv9JY2OzKhOt4mEQt5VmaYiq8vyZCSZDGfc6xWUXu0qpVLKH8ZJLsnPM6XvbbdGOxZMJS4aNen+qvkeljUC3Vcm1sUi7XOQSJlKB/fXu66+Czdv2et8z4Z0f42ZcfVSziK0jElaUNJMQGetYqYykYqkva4uS48p/P26MCiljHOi0aP7y6Gp2MxMRukB5e4vp6biSn40q/FeFwF2voddndtuO+CCZCQA0vt9tqW3141RAfIJkDzGUIT+s+GcqPu5zqgSkkV1Yyo8/pqXMZVljaCnbp39C/XMVHq/t4eJoRkVIvIB/BWAVwC4CMCbiOgi62OPAbgMwNWOXVwF4Aql1IUAngtAxkT+rlLqYvNvkzneKgB/DeA1SqmnA3jDIM/HRhhnGeWRiP6abARa7BYPi30jZdFf+qZxRX+5EiLrgVcS/VWiqaQrzOL7nMfCWc3sMgnjLPmR3V/6ePnzZ6E+sNxfsvYXg0XEmVaEMFbpze96IJVSaTho3c8b6PR84wT/es9u/OCR3vphpO6vIHMRtKIYW3cf66iBxJb7K7KMCjOVJ64ex8rxwRoVQLMLGf01aTGVMoM7J5mKmQgLTIVrf5WGFJdrKrbLCMiYSq8aGC/K7GRenhB7KdPCBpN/U5up1AMP65YboyLCil2aSspUan6p0N0UbSu6tRPOqjxnTGXleK0vTeVYH9FfjZqfNo9zMZU4UXj1X96KG/ssnLkQDJOpPBfANqXUdqVUG8CXALxWfkAptUMpdQ+A3J1ojE+glLrJfG5GKdXNWf1mANcopR4z3ykvfDQAtOMkdUfIrPiJen4ijFWRqSzrlKciBX4rQazcqEimYPQXP4uPdxW/zNUGi5WI/soeGM9Ef/E55s5faCquRMy6g6nMtiId/VWSxQ3IicJDLXBXWZ5ts07Q2wSWhRR76ap2274ZvOKTt+KmreVsxxZjpVsQAM5btyz9n5nKQlviMqRWN2eielivmLQ0lTLXS1MylZCZSlGoJ0JpLpNdqgcQ18Ph/mqIe6mn8yypq9YSLILHWYYiU7GMiu+nTGX/jDQq2TNW0FRMQIkLkqlEXYwe37PSY7ByvDYUTUW7ojOh3uXanW6GuG/nMdzXZ42zhWCYRmU9gMfF31NmWy94KoAjRHQNEd1NRFcY5sO4nIjuIaJPEFFDfGc1Ed1CRHcS0VtcOyaitxPRHUR0x/79+/s9pxTtKMFELavhxQ/DeM3P9VOJhXFgTFp5KmVFFrP+LJmhcIUUy9h3WaYlFU8d7qlA+OrDJMFsK3PFcXkQAKCUqbiNiu5fntdUAjERAXpiWjYWYKYZIYpVFlLseCB5EuTkR6BYboTH2qsgmbq/BFN5eL8u4rnrSLkGYmsqcXrt9Lhe9rTT8c3/8SI85fRlJkSbBspU5loxfC8rt2MzlUS5V8rO6C9r9R2a35h/JzvCL5sUJePNG1k7pBjo3f0l81SSRKWBJwWhvofoL/5N+X6fFZrKOjYq00X3lzQOfC4d3V+SqaRRmCWaiojYrAeeKWbqlTKb/Hf7NCphViwVcDMVNrTzi2hM1it6MipENElEnnn9VCJ6DRHVhjiuAMCLALwXwCUAzoN2kwHABwE8zWw/DcD7xXd+HMCrAPwsgA8R0VPtHSulPquU2qiU2rhu3boFD7AdJyndjC33lyx1Im9cziwvJD86VvpAvpoqoFfaWU/vbCyhQ5PJhxRLJlQMdQyjJL0RQyPUc7Y0P2wFo8LJj46ugNIPz2NZ1ggw3YoQJklpvSkg8/1zSLG8DgyeNHrNYWkK9sPH3mmMyeEOzMIWi9OaaOap8TzChWeuAKCN78rx+qKNijzXdpzA95AySltTAdzCuKz9lUZ/WZoK58CkRsUy8HGi4BGzVav0vcuodBiPC2ntL6FP8JiB3vJUJAPVY9Djke6vFeNBoVSLLBEjQ6cBYLwWlLu/pKZiaUE2eNxaqPexrFErsPoy8LXh6NBu4KreqVFxjJ91pjKDOUj0ylS+A2CMiNYDuBHArwC4sst3dgJ4ovh7g9nWC6YAbDKuswjA1wA8BwCUUruVRgvAF6DdbPydG5RSs0qpA2bMz+rxeH0jjJM0byCMVdpMaqLuI1F5IZ8flNUTdQAu95fUJJL0YbVdAXXf7f5yifY1z11QUmbc14TLQt5srTBJXRv8v/0wcM8TW1PhyDJ7wmH3VxgJpuJ4wFoi/Lde4qef6ZOptNLJJxMzmaEc7VAKw9ZS7AnIxsrxAEfnu5fW2DfdxF2PHXa+Z0/KvueleSp2SDHg1jBkmZY0+dFmKnGia7uRm6lExujIY5VpS8DChXq7VIqtqXSM/hJamRwDB8DUAw9EVCjVkhnHJH3N5zJe75D8KMbWLU8lFM9Z3fewfCwohN+XYUGaimFDgHuxlTGVpWNUyGgarwPw10qpNwB4epfv3A7gfCI6l4jqAN4I4Loej3c7gFVExFTipQC2AAARnWn+JwA/D+A+85mvA3ghEQVENAHgeQC29ni8vtGOMqYSxXmmAuR/WH6PxVy79lcuTyVRqVZTYCqm8RWA8tL3YoXEK1zJVHifNVHFOIyTvFGJ4vQ9Ziw2y+cyLfbqK05UrkIyoF0ok40AidK+3U4hxZyEOVEXRqXAVHj13Z+mogtK6mPv6oGp2PWnssnU/fmV47WemMrnvrMdb/v7O5zv2UZF5qnYIcV6bA6210OZFm7k5nkEomKTLi5mqceQv4/c7q/+hHr5uZxRMWyAF1Kd3F8pqxV5KkCeqQDAacvqOCwWD848FZFg2j35MRHthEuMSpQtBF//4xvw1v9yDgKvN/cXj6/n6C/DVOq+B4/choOvyVJiKkREzwfwywD+1WzzO3wehmG8G8AN0JP7l5VSm4noY0T0GrPTS4hoCjpS6zNEtNl8N4Z2fX2LiO4FQAA+Z3b9RbPtXgBrAXzcfGcrgH8DcA+AHwL4W6UUG5yBI4yzyT9K8hn1QP6haUUxaj453F9h+n1GLIxKVgcpW/WwLSkrfd8W7i1e4UaJwo2b92Dnkflc/kHq/oqTnK+1GSaZ+8vMG9KIpRFaJsHSDmkOPIIvZt6a76W5ObPtOJdwd+/UUfzo8SPpZ7mnxeqJOup+3rgy+mYqkRal6362mtt5mI1KB6ZiuTiykGL3Y7Nqouj+Omx6rd/5aFZ48th8lK6mbbStScoX2tdkfSHuL6Op2Bn1ghH7VFxBczgzoOuMybG5jAqzhF5bHLcdiy4eM4CueSCA1N+MUQmKmor+388dw9VPRSY/NsPEGRWYJj/mmIp7fDIg5uXPOANvef45ujpFn0yllwoF7DUgotKCqXxN5kaQwxJ0/wgA4H9CaxnXGsNwHoBvd/uSUup6ANdb2z4sXt8O7RZzffcmAM90bH9ph+NdAeCKbuMaBNpRkjMqWUZ9Zms5ea8V6Qd4+ZgurDgumnTp//NMRU8erSyEMc7Eb3eZFof7SzCVMErwm1+8C7/xk+fh5y/WsRLSn15wf0VZd0o2Lq4EyzKmYmsqvkdYJnp1S6byv7+5FbOtCF9/9wsBZEZlzbJ6ajxKNZU+hPqxwE8fOiCLIOsUrRVbK3M7+dHGyvEaHrTqf+0+2sR0M8JDe2fw4086TY8nitE2+UFk7cvJVHgV3ZBMpVwYn0/dX5lQz5US0kiuWKWLDlcpHs04LaZiaSrSDZgtUHqL/godCyEgm7jthmAuFIX6vKYi81fkMdxMxWgq9azigt0LRZZpyfrA6P+PzLWxyri39fnlGRC/7kdTiRL9XMqoPxeYqQAmJPpEEOqVUv+hlHqNUupPjGB/QCn1niGPbUmjHScYr+kfm0OKAy+fPcthlq0wQc3zsKxRS3uvS9jRXzx5SEEx8PPZ7fLeLAsZ5glkuqkz2Vthks+4L3F/NcNsFct9yOWKSdZWCnwq5MkUNRVKe6rosRE8U3hTlzbJIrByTCV1qeQfEnaR9R5SnKQMZSzIE+wjHTQQyVSUykKtfb/cqNhMhR9wfqj1eLKSODZsIyH7qUhNRbJMGy6hXh6Xz4n3G3jFUjw64CLPAApCfa5Hvfu3KkPomOR5zPr4eT3RBTukuG67v3x375mUcYkoTb5deVHochOlkWmiDFOcKPzwkUP48Y/fnLaXludXE896v5oK0FtWvY7+0uOWzfEkZpea+4uIriaiFUQ0Ca1hbCGi3x3u0JY2QpGnEhpNpR5kuR9A5uttRTECn/DE08ZxxoqxXNIYkK3GlKloPGGMFdP7MNZGSXdh1N8pE+pToyHcJjzRteMsgzjwvJwffK6dn/RSod4RcsoMgXNhCkxFCMB8rJzI7HEosg7FPjTbTo+fGZVaaUjxTBr91QdTMQ+dLB8DAEdmO0R/WUmpruZUEivHa5huRpbwrMfIOpAeT2LGX3z4i0K9LNOSXUNeWbuYQcpU4iQ3wcgVLAv1gNv9FcdZWHlNMFpAGBU/v3AA8nk2nSDHLc+5UFCyL6Ziub9q7P7ynO4vzgULPEoZY1kEle4eWmQ4Uayw++g84kThoIgwi2IdPZcLrSdyusvshQQvUIHuuopSCs0ocynr1g7l7q+lJNRfpJQ6Bi2MfxPAudARYKcsuEovl7pPQ2wdfmbt/vLwm5c+BV9/1wsLk5K9AiwwlVilq/vEcn/p6KviAxOIgpJ8Y4ZxFu1SE4wpstxfzTDO+dvl8fh8AB1NVYj+SpQ2GmLC4ZBi+bfn6XNpG8PAGseh2TZWjtcQ+F7qy7cnzixPpTNTmW/HaIYxmlG2kpPlYwDoMOc4wZ6jTedqXb7mCcHr4P4C8lE7PJHPth1MxWFUbPYi80TyeSruxFB5TKWyayWPC2RReoB2f7mZijEqln7jcgM2OjAnF3KBLNKoiKKN3fbXFIsbOc6MqWTbXcwoTvIuQSDPVH7rnzalWpgdWCBzmNKyL3H+OaxZHgnfL7q/dh+dx/P+6Gb8851TueuxZpl2pR3tkkyr3aiZMSxL3pxZgiHFNZOX4X9FDwAAIABJREFU8vMArlNKhQB6W5KcpOAyLTXTga8VJrksdgBp5ngrSlJhfOVEDUTZ6hMoCsKpUJ/6l5PUnWUX4KsHllDucG+lTCVSFpORQn1+JS1zMYB89BfftBN138FUklwWth5LJtTz3yzw88PKhRgPzbWxZlI/UGVCfRb91fkBefs/3IHfv/Y+U6BSnxARpa/ZCDxyYBYvvuLbuO5Hu3Lfl6tKV6SQDVepFjZ8OfcX6xwupmKt9H1hoHN5Kl3cX9L1mY3Fcn95mfurGFKcpPcoH4vvLVeP+tT91WdIsf2dTqXv9x1r5gIeWsY7wPcoL0Ls6C+7TlqOqSQqN/nzs7fzyByuvXsnbn3oQG5c+ntJTtu0WyPrY6iCUXFpKn90/f04PBfi8UNz4rtJWgmgW65KVlQzc+92ZCojEOp7NSqfAbADwCSA7xDRkwAMP99/iYKbANVS90+SMhVpLFKmIh5yhhQ57dLq7OaQiWBM0flZZI2j5nv5mznJfLm6cyPSCq25Blyi3XDb1lSiOB0fD1u629gAjdV809ZWPHCxI/nRI0wKkTnwMtbFLr4pUzL+0Ewbp7FRKQkp7jX6a/v+WWzbN51zf/G4AeApp+syK7fvOIR2lGDbvpnc93NRedL/XmJUXEUlU6YijEpLaB42bE0i8AlPOX0Zzlw5hrNPm0i3p9FWjqTFdpRgFbOmZtHA6XPLjIYrKkkyFf4tJXOW24HMqNy8dS9+9ys/KpyXjdDhjgKKnR9l9NffffcRXPaF28X5xLALlwL5gpL8vyvajFtUyGeT7w1e5PC9Lu81O/nRxVT0QjB/n9iayve3H8Q3zEJGLpDCWGGNMSrd3F+yqylQzlSWnPtLKfUppdR6pdQrTeLhowBeMuSxLVmkyYhGqA5jlbrDZLhpQyQj2asWaXzCJEmrBwOZICujVAJf1+FS6QrJHCPIx77bD3zge6k7JoyTfElu4f6aD+M05LkVJqkxcWkqfCOP14pMJRtrdr6B52F5TqjX2lNkjDGQPcSH59pYbRsVW6gX7q9OIZdH5trYP91Ky1gweCJ68jpdrn6T6U2+51gz9/3YMtZsWLsxlSNdjEon91cYqdy94RHh6WetxPc++LLU2Mox2EyFj7dygo1KVHgPYJeqvg7a1+/SVPJupWJlAbGAMtf0m/ftwVfunOrqBsszlaJr1VVQcq4dY7oZ5fJwZOFS6f7yKLtGdvHNfOn7JPd78oKO78e0OGfOIOc1FT4Xmw0V3F9Wb6J/u28PJuo+Jup+bv9hlGAtu7+6GBXZ1RQoF+rTSMo46RimPQj0KtSvJKI/55pZRPRn0KzllISs61Mzk6Om4r5TU2k6mIq84bhSMN+o4/XMbaaPlwn1dkhxPbCZgn6d+sM9ymkqMuNeur9mW1E6KeZCirn2l3gYpPvLpakEnpX86BPGal5qqOomki1OsodiyiQjHpwV7q8uTEVeIxutKMZsO8aBmTbmS5jKk01ByE0mT2avZVTsPjeuTHIJl/uLWUk++quD+ytO8vpTybHK3F+8EmWmMp1zxeWF+lQ38zIGzJBMxTfMUmai2+OyJ9BuLNI1yQOOdsKWCxLIcotaYZxbLEihnrPpAeQ6nPK5876ZWTPY/cXuKFnxmREnsp1wIphPPgqyZl0jHSmZ3VMzrQirxmuYbAS5oJN2nKTPQLes+laUZyqleSpC0xt2rkqv7q/PA5gG8Ivm3zHoEimnJLKsdEpX6k6hXkyK9kNYiABLsmZek3byYyxCipO8X7tuMZV2rHtzk4je4lyMtlhV6TIt2Wp3vh1jxZieiHRIsZd+H8gzlZz7yy9qKoFHuVVs4GvXHU+WnHEfW0xFKYXDsxlTyUqRuIV6fY3cRiU75wT7p1u5UGKeiNiobNuv3V67j9pMxVqdOvqISKx0ub/aRYGUXR2u6LV2nOT0p7Jjlbm/eEJJgwaaUTpRStdHnGSMiF24Erb7puZ7uf4ytgtwrKZ/0wlrQVQGyU74efKomPCbj8DTn+MIQW5OxeBrMteOc2H7Ze4vXijIZ3Pcdn+FRVdrlOTLtPD9mQvtj5NcJCiAAqufa0eYaARoBF6eqcQ6sGR5I+jB/ZVnKuM1rySjPts2bBdYr0blyUqpPzC1uLYrpT4KXeTxlERaXj7wUfPICPWx6dlejIhpRUX/ql0/KowTwVRs91eCgEOKzT2ZGhXfdn8VJ4O0WGQkor9EFeN2lGAujNOJSDMr/X3PEf3F+xuv+/C9vKbj0lRq5lyXG6PFeSy6Za7+7s7Dczhm8ml4lcbfi61ltAzPLUuAlJnye6ebuRVtylSMpsL2cq9lVOzyN66S7xIpUxHH7pinUsJUOKQcKC4+GGUZ9Wy8OBHvWDNMx5ULKRZCvU6qze8/tiZb3XraLGis9wCdtf7ldzwf7/vZC3LnWAYXc5ioB7mwXftzKVMxYeCtKM9AuVIyoJ/N3NiFy4t/VnZjycmfvQSs8TmZSpzkkh/lc5qdU96NCRRDt2dbMSbruhCkLFaZKD3m5WNB1zwVJ1NxLFbkQmzYEWC9GpV5Inoh/0FE/wXA4vqmnsDIMRWzUp8PYxMNVcwybkVJwYjwezw/RaJh0GTB/aVvUFdIcSPwcgli9spLTko595coKDnXjhEnKp18okSlEyfvSkoXvNIZr3E7YZemkn2fV7Us1nOAA9/cyxsBDsy00+QxLrzJeRB2DgX7zIHysOLDIv9EhlzyNQN0G2C+Vr5HmG5FucnfPq84USAqF+obgY/xmu+M/uKHWuY7tGyfE7KWAjzGMqYSCJYpMW8xlXaUZG5NqQvESY6pFEKKhaYC6PtFRk25xvWcs1enLLNbuLfLqIzXfaGpFCOq+N49ZIw2F1JkSHZiC/hhrBNY7URj+3lhpsU14TKj4mYqUlOJumkqnpe7znPtCBP1PFORi74VJu+pE4pMxd1kbLYVYYVhwGW1zQaFXo3KOwD8FRHtIKIdAD4N4DeGNqolDplRXjPRT7OtCJP1IOdHZRGRa39J8I3MLqEwybpFNmpeupIHstInkj6zcWkEfq5suX0zy4khlMmPfhapxpMgTz4Acv50HgNj3hLq7YKYsgeIPH7m/tJMhvdznmEM90wdBaALAMprZIvIM60oFa3Lwortml62prJ6ooaa76URWz+2fiUAYI9gK/aEZq/eXbCz6jOhvrjidbV95QUELzrKWFGZ+yvVVCay39LFVKSWoFfQ+bHYUVEcWAFoplJm7Pie76qpOMKIJ+t+VlAyziZtOSZA11PT38szFd+jdJGWMzYiyTcfGqwgq4IDxYoLaVBAmDdG3aK/wtjhnfAdTKWRZypZ7T5d1mm6R6aSJfdqw2wvEmZaUdoFc0m4v5RSP1JKPQu6FtczlVLPhq4cfEqCbyKZUT7fjo07qKipcO0vCabcy81EqyctFtl1YmL2gGn/LFFWUJLv33pghRRbN7N83YpEmRYvK//Bk+CK8aIvP639pRxGpV5kKnGSoCbyVOTxuYZR4OlwZ14FchTW7Y/oHITTmKl4RaailMJsK8KaSf2AlE1etlGRmfTLGgFOX65bAbOb6Hnn6bpcUqzPG0vtqitLfGSsHK/l6omlGfXtyLCUvCD7zXt34xc/8700io21OZ6cyybvMveXranoc6zl3uPz4d/fcwr1+fuo7nupDmKv7iU6lV+XcNXiGq8HxeivJP8bAJmmYjMVnf+VhRHLsQPaUNlMJbSEes+jnKu0jKlk7YQzl7I08G0HU/HIoanYTCXKFqzLx3pnKnx/j9eKmhY/M2xUlor7CwCglDpmMusB4LeHMJ4TApKiBoZaz7a1f9SlqbQdIcV18zkWZWW3yMAjNGpeLhGsZkKK7X4qNb/YzyTPVGz3V8ZU6pZRkRMRT56+I/qr2Y5BpM/PdxxfaypZYh2DQ5Y5MZNXTJeccxqWjwW4/t7dAJCyELvjIKAflihRacax7Wa5YfMe/OjxI+nEzhOGXIH+zs9cgCveoGuVrjYT7k+ctwZAOVPh1WlXpjLhZipK6YdZjrcdJdg0dQQ/fORQ+qAz0+R7p1xTcbu/5nplKkkWnRR4VGiY5nKjytL3ZcaVV8zdin26NRVdTViG13NkJB8XENFfUZwLKQYyA5JvZ22uVZTkGBIvFOxrLMvhZHkq2fcSy/1layp3PXYY399+CE87Y0Vuv3aS6Wy7yFSy2nyeYSrdjIphKgEXlPRy2wH9uycKWGcWUsNOgOzLqFjo/HSdxAiF+4ujmDRTCZyVW4HiitMuEtgWeofvkVkZZqGKgRVSzJN8I/DzoYxJ3qjI13qlJjWVcvcXP2guoX6uHWO8pqv+FplKXlORx5fRXx5l7q/JRoCfuvAJaeVgNip8yeT5sTbByWF2BNXHvrEFf/nvD+HQbBuTdT9tJyvdJE85fRmeuWGVOec6fI+w8UmrAehclX+9Zzf2TTcLNdXKtASJFdZEIB/g2VaUe9hbUZy2+ZVh33VfairuR5TvH1vs5+OtGs9yWibquvthM9T3aZyoXJ6K5xVrf+nFga1LZEalnKlkVX5d+Og3NuPqHzzmzHCXkWNxUjQ6/B3JVMasWm58T+c0FRF+HQr3Eie02nrnuLhXWH8oMBUp1IvxzbYivOcf78aZK8fwgVc8Lbdf9mowK51rddFUxmo9CPVGUxFCPZC/71gn5GdhSbi/SqC6f+TkBP+QNZOVPh/qUuaTlvtLrqLsGzfVVMZYHM8ioZipZNVsTUMl4/5SSuXKtBSiv8QYfIupyIx79kFn7q8iU+FhyzlnPozTB4+jv5R4yHxRVt/l/tKuN0oNRN338IpnnGGumZdOLmy0bD80gDRCzGYqR+baeOTALA6bUuRM+e3Jh3HRmcvx7CeuwvKxGlaO13Djlr1419V34Zq7djqZSjejsnys5oz0AvTDLSfbdpS1cc5K6SS5Vs9lk3eauGoZAzv5EUDaFXC+HeElf3oLrrxtB2S/FJ/cTbrsqgiyoGR5sAKvlN3urxvu24N/v38f2sIlzIZRGpV85e3emUon91dLRD+O1fxUU7Gv8XhdGhV3nopkT3Lxd/djRzB1eB4ffvVFuUUakNcIkyTzbnTWVKKOCb52pWYeO98HV31vBx47qCPZRuX+6lion4im4TYeBGB8KCM6ASD7idQ8D8fm9SQybrm/5I1t37j8ntRUZHLdZD1If/y5doTJhp8lIqpskq8HXq47XhjnQySDHFMRZVo8nTuix180KnZBycTSVHhFxOeVKD05RbamIozpcsFUfOH+atQ8PP+8NZio+1g5Xsv1GLHzYHjCzoyKFJ8TzLZjPH5oHhtWT2D1ZC3NTJZMReK3f+YC/JY5tzNWjKUNw+bbcS4AgkOKy5gDY1kjL67Oh5JlxTn3hzYq+v2MqSjURPRXmZtJunQkmPnICU0bFR+PHZrDnmNNPHZwNuf2cXUkjJIkV4VYJtnGqjtTKasgPduOcWw+1LkYgYfZdpx1TU3LE8WFhRKPCQAOmcg+N1MxRsXKUwH0/c/GYdwYFTv5EciM29plDRyYaSFO8lqYrq6QXScZFcfnfcbKscK5S32Sx2HnqaReEF9rKllkqXuqtpmKDJTYP93Ch7++GT990RMASKNyHKO/lFLLlVIrHP+WK6V6bfB10sEW6pmiTljuL0nBi3kq+m/WGWSeSuDp1TobldmWvqn8lDVkQqHNVEIRKgogF43WjjJNJXNPUZr0t0aUASmUvpeaigmflu+nE46Vp+IU6k1pfM7sbfh60nvdc9anUVjZdcoHInBmMLu/5MPOJUnacYLNu45idQ9MBUBqxOREoLUbK1IoVvC7cHt7ddlsx2ko50zB/ZUV8pRMpd4DU2GWWaqpSKNS8zFe9/HAHt1AjCszp/XdvGKPehdTkT3qyxjbWK0zU5lvxzgy39b9iLhtttlvWkkiTBBZCyUeE5CP/mpYiwW+bq7yLdL9O1HXbuMoKWoqPEE/8TS9bp4P43TSl25pALmiqG0rD8yGZCp8H082glwWPOcCsaYCoKOuUsZUmsZ7AgA/2H4QANIF1lJ2f52yyMrHazcOr/QnG/kyLY0OTMXWVGRGve/pnu584822Iixr+OnkJ+k3Jz9m7qd8UEDe/aWM5pL1j6gFHo7Ohzhv3SSe+oTlhe85o79MpJs8LxliKTUVed4clFDz8n1YOHLlD1/7DHz2LRtz18nOQk6ZCgv14gGXJS0OzLTzRiVwMxWJM1Zoo+KRMcC2+0sV/e82lo0F4D4xenwx1pox2JpKO0rSv7nEua5+TVn0V4lQzyzTrjYwH8aoB17OhdMIPIzXfOwyi4fZVpQGfwDZNW5HOhpN53PYeSpZ/azYsbpnjHUIKY5iXUHhqGEqfI5FTSUuRN7pz2V5Ksqs9scCm6nocbncX20h1I/V/DRPxWafPI4Nq3UBz7l2hFak6+E1Ag92WHIYZc+edI3bkNGMc8aNO1n3c/tM3V+Bl3oOOoUVN011dH5Ox4T7kVksL7ZWTdRR82nJlGlZEIjo5UT0ABFtI6IPON5/MRHdRUQREb3eeu9sIrqRiLYS0RYiOsdsv5KIHiGiTebfxdb3LnHtb5B40poJ/PoLz8WayQYCkzQHZHkbjJz7qxD9ZYyKi6n42v0124py9DfrwijzVPK+de68yKhZ7q/QSsTk129+7tlOY+Tq/DjXztxfdtgvr2J59Sf3yRn13IY4uxaZhmLD7hfDOgyXBpdRRnZJi9Mm6+nnytxfEr/y/CfhD3/+GVg9UU9dMLJPiNYSOu+Dz5HZ63w7xloT/jzbjnIr+FYUFzQVjv7qxlQAE/nnCCker/mFREC5op9txTkh3jfur1se2Id3fvEuPLx/psBUdIuFzP1V5gZsdAgpnhPn2o4y15XMqNfnkFjurzxTaUcJjsyFUAoFpuLSVGQzOployZqKXaOLjcoTVxumYvrycFmitqWvcBJrJJiQ3bcHEKw/zpjKRD1Ic0tkciZrKvp6lTMVzdayY6WaSjsu5B4ta/ilyZGDxNBcWETkA/grAD8NYArA7UR0nVJqi/jYYwAuA/Bexy6uAnC5UuomIloGQF6h31VK/XPJMf8EwI2DOQs3nn7WSjz9LO2mCXwvzR2ZbAS5STSf1VtMhAJE8mOscn0qJho+ZluxiJDKHp5YqVztL4BrOenVYK5DoNW3xXaP1U214tc9Z0NaODBRGUPJmnRlY2+GcZrfweebVW3VRssl1L/saafjw6++COefvszqO1M+U9tMJTMqRU3FjpRZNVFLmUqnYzCesX4lnrF+Jf7m29u0+ytW6Soyy77uvJ8VwmVx+nI9vrXL62bs+Ye5XeL+kiHFZcmPgJ4sbffXbCvSlQ5EDpXO9M/GPdOKcqXvfdKLFHadzbRivTiQ+U6WUF/mBuzEVGR4rk9Z3oRLqLdzr4B8UAK7bO3J22lURPh1W7AijoKzWVfm/mKmEqd1xnwvc3/5ZsHDjEDmwXR0f4lrPdnwc+WcpKbC27sxFenqGxfRX3YXzmWNGibqwZLJqF8Ingtgm6kV1gbwJQCvlR9QSu1QSt2DvMEAEV0EIFBK3WQ+N6OUmkN3/HcAXwWwbxAn0AtyxehKkh8BR0ixZ7m/4iQVhgNPF1+cbUfpJDrZCIRQr7LkRysJzvYR210D59px7oY/Y+UYXnvxWWkYL7+XdQXUn7Mz6m1NReYwBF7WTlhOwpONAL/2wnPheWQxlfLbUE5mQFYYb9V43RQgzG4dnpj5YVw9Ucezz16NZ21YifNPX45ewQUIoyRJV8Kc8NYt+ot/z5lmVp6f2VLB/RUL95cQ6utBxlQ6HU+7pPITx66j86k2xNehUfNyTO1YM0Sist/GN3qJ1AbsqCg7pLiMqXgmHL5Tn3RAC/Y8ARaNSmxyYfRns+ivJL0uXNKnoKn4rKl0dn+lQr0zT0U/x3wdtVHRkWa+J+ptBflGc7Jisb2IBLLw8DhR6bWYqAcityevyTDr7aSp2JWa2cBIfYcx2dDa2pJKfuwT6wE8Lv6eMtt6wVMBHCGia4jobiK6wrAQxuVEdA8RfYKIGgBAROsB/AKAv+m0YyJ6O5fw379/f+9nUwLp1pqsB5amUh5SzF3qMvdXPvproh5grhVnRkW6vxI4mQpQTLTk8TCzmGvHuYfoH9/+E/ij1/1Y+jd/1xbq7eivcSv6S2oqvu9OfpQIemUqvpupTHDSmGQqxk3w9LN00tnqyTrWrxrH19/9wnRV3AsagS4XEiUqfWCjRBuZTswBQG4i4OrVXMtMC/XZBNoK8yHFSqk0E5snwrLkRwCmmGl+4thxYA7nrtUVCjLR2svlXrD2FAhNJRGul7Zhabam0kueCh/PxVTsyYwn03bqksqa04Vxko5Z1tbiXItSphI4NBVHnsqYMGi2gfypC5+AX33+OWk02rxJWtWVmDOm0jDVLGSZFpnDZoMfyygpYyoZu+A8FaCLUbEqNdfSFtxJwf01WQ9G4v5aqkJ9AOBF0G6xS6ArIl9m3vsggKeZ7acBeL/Z/hcA3q+UcoedGCilPquU2qiU2rhu3bpFD1T6YydKMuoBh1BvVe7N56l4WNbw0Y6TtOHTRN1PV26xyoR6W1ORAiyQTRycYT3XjqwINd9KlswL9K4e9fPtBGM2UxGRQYFHqeukbFLslanUrOivmVaEsZqXuojkiphX+5zYuHoinyfQKxo1L62gzO4cXUanM3MAICJ2wnRinWz4mKz7OaaycryGVpw3KnyedZ9SQ9sphLkmdA5Au5x2HZ3HOWu0UUmZiil0CegCplxtQBbTjBOVunHacVxYwdf8jDF2M66sEdiwM7ltoX5SRH/FiUonfnlv8eKAmYqtlfG93HAkAMtJX7Ii+9m89ILT8eGfu0j0q48EU6Fc7T8gc/WFgrW4hXq9LRFMZVIwlWaYiH1T7l4qg93VVJ4rG6gnrZnA8rEAnqdbEww7o36YYcE7ATxR/L3BbOsFUwA2KaW2AwD9v/a+PEqSo7zz92VmHX13T89oZjRXj6SZESMkpNHoQNYBwiAJYyQfLOK0duVl8VqsvVjmMDZrWPP22Xq79sOLMZINGC9e7PUCq/XKRkLr57WfDdYgBp0IBklIIyTNIc309FlX7B8RX2RkVGZWVlfW0T3xe69fV2VlZUZGRcQX3/f7DqKvArgcwB8LIZ5X5ywT0ecQ8jH7AXxJkb3rAbyRiGpCiK929hjpsAv82As2wybqbU6lVhcgCol65kWOzC7r80ytIdRUrIlXbzTtMIEw8680fyUvCIFl/qK4OJVKLdRU/KhHWr0hUPRDL7i4yQUg0UwYd54ZXX1qqaoFsa2pnFysouAT9rKmMlzESiDzrsndetkwf9nJB+PAv+ep5ZqevOWCj2FlzhyryraPlYMmTiU0nXjaeSGdqI/WXn/mpQUIAcysl1yAqamwmWhmegRPHZuX1+aIepWTqmZou7EJJdWC12ghXMsFLzZNi62paJdiI2YDgKElct+HGpIWKifa4VRCziI0tYVaUdKzmIGEy7UGSgWprbF7Mc9v/p1r9fD6cRsl/i1NTWW4GNVUqsYYYDNcWlT93HItooXqchYGv/OB687V3pJDRb9l6pdO0U2h8gCAXUS0E1KY3Azg7W18d5KINgghjkImrzwAAES0WQjxPMnV7iYAjwCAEGInf5mIPg/gr7otUICosJCxJAneX3bwIyeUNLy/TDdcXpyOzkmhMlwKwgW+Ecap8KJuul2aO0y+Zqip1BMXerNdnrGLBUKhIoRoiqiX9zdIyiDepdiEnyB8m88L4yMAaeLiPpORyIZLsaod8qYLNmOpWsfezeNN18sCmc1ALqy8W601BOqitaZimizMYmajpQBzy3UsqYzVQ8UAS2qxks9VjdjTWVNJS2Bpl8llYcHmL+7XUiE0f+3dPK7P480F56Tia3GMjs2pVAxNpRgkLx1JNT0WLYKYnQcqTdqDjKcaN0zDfN/pkSLGSgGeePGUvpeJkFNpLnUgNYnmmJikTdawwUMuVesoBz4WvTD+g6/Lv3NVzQG7SB0jjPlqNMWpAMoN2BgDXNguSQjUGwKP/mgWN110ZtPzm+avrVNDeNW2Sf1MvFHtFrpm/hJC1ADcBuBrAB4H8BdCiEeJ6ONE9GZAu/8eBvAWAJ8hokfVd+uQGsj9RPQwZAT/XerSX1THHobUSH6rW8+QBeGOPqx+x0gLfuSFfaSUwKkob6+jKmvuaMk3Fnh26wzzd+msrvWGLm4l7xNj/kojxrVHULz3V6Uuo4nj4lS06u57IVGfpKkY3ZGuOUU5ldmlql64bdv9ycUqxoekh8u7Xz2TmEqkFTjArdpo6MW9Xm8mr+PAHNncUi2SlnykFJq/yoGPku9F4mpOLlYjZpVMnIof5VRYWMwwp2IssMwNvWJz6LAQBj+y+Uv283KtESHy+V5aY2ghXG2zJCOJU+H76gy7VUtTMdLgBz5h18ZRHcjZrKk0cypRk1BI1AMyjijpWbhYmvb+UiUptKZSCDdUsn0NVbogXTuvN4CF5bqOe4loKpZpLS2p5HdfmMXccg2XzKxren6ZPDPkZ/QzFQMsVFevpgIhxD0A7rGOfdR4/QCkWSzuu/dBptq3j1+b4b63tNvWlYIn/bBKsEgE7ZabpqnceOGZmB4tGjmcGiBi05Onhc2RU0pTKQYWpyIXfN7xVw27s1kBjxcGNgUtVur62nHQRL3l/cWa0VIlOilN7y9Tdfc82RdJAoPbXQq82PgU8zyTUzm1FNVUTNv97GIocDoBE/URTkW5n7YSVL6yW59aqmLR6KuRYqCJ+nLRRzHwcOSU3DBw2VjTdBJyKumaSq0u8PffP4ojs8t4+tg8pkeKodAthH18vcqttmUqzK4U5v5SQkUtaLzzbionbBTQSjd/hXVRTMy3IOpHSqFJyuRUzDLGgedh98YxPPjMCf1sdp8A8S7FpkcUj19hCc9I+4qsidSwXG1gesRTOevkc9i7J6bzAAAgAElEQVQmLs5YnDzmw7kyX5H1l4jI4lSigmC8XEjkVA48/TIAYL8hVMxMC2bMC2OouIrjVE4X8MQcNhbqwPN0XQx9zBqAuzaOYdfGMZxQyfGqdQGCEVFfjAoVHoCAXOCFkIF4tveVmX6DrwUY3l/VZmIy7nmainQp85dZS8U8n2tTAIgE7iURzdwdaXwKX6NmcSpbJuXCWC54lvdXVT9nJ9AuxXUR2Y02RPIu1ATvLtkENKTMXy/MLiliVe5O2Qlj40QZh47MaUeDQkChptIi+LFSb+Cz//AU/uHQMWxfN6y1FCCqqezaOIYLtk7i3kdf0J+b3l+mUGHTTJRTIZ1jLi62w0S5EK+psPmLSX/TAwuwzV+G510kPoawy8j80ETUq/GUlPuLBaOZcSDpWTgN00JFmi1LBV/FqVQj12VUVBxMMcGca86VheW6tkbEaiqGeXw2QVN54OmXcOZEWc8HIKwpU6mHGToimkphdbsUnxZgYTFsDFKerGneXwz+wTkPEZ/LwY68mx0u+ZHkjlzTIjB2P/I6Ue8vfj1lcCpp5i87PsKziHqz6qN5Xlzgl+9RU7QyI9RU0iPdbU7F1FRKgY/lah0PHT6BI6eWMLtUiyTFXCnMgMfQ+6uRKfU9EGYqDjkVqXma5q9iEJq/OD3MUbWBkJxKtH/jwG6+s0s1VOsCPzg6rz2/gDCGwy5QxgiM36lhcCqcQiQSUW+4FDdSEkoCMgAyzaWYC6SVLU6lHPggYvNX6FJsJpQMfMLujaPhM1oLuxakxjOH5LVc9InSrQgMItKL8LIqCBZ4ZMSpRMdurS496IoJmoqn56rQmorsB4NTseJcxsqFiJmUIYTAA0+/FNFSzD6o1BphHjHjWYdLARar9YjZNG84odIheNE0o9h5IcgycEOiPXQT9v1QUzl6allmQ/a9SBr6upB15PX3DTIzLksxm7/S1HOznTr1vVWki6NxwyzFinMw8l2FBLCXyAnwfeLSWZgoWJxK1PzlYW65hrff9U3c8TdP4ORiFRNDnSvf7FJca1iaSkahMloKMLtUjXh/rR8t4cipZe0CWgw8nel2Y4xQOX/LBM7fMqErXMZBCxVj0ZmZHtav4wIBTdMnj10ZGR5uCnjxj2gqnqcLZtUa6WZAs8CcicVKHaXA04G2dhyK58m2clEpM/AUUJyKR9iToqkUYzQVzanUwjigOLf7OLC5SMaDKJdiI07FRK0hhVYhYUzrjN7K+ytJUyEK+96uz8M4/PIiXpxdxiUzU02fMdemy1wYv9WuM0YhBPDYj2abvpcXnFDpELzjjmgqesFMdilmFDQnYmsqcvIfm6to/31TaxAqlYrpfcWJAE11d8vkEDaNlzWBDCS7+Zqf2WYwXth5BxoXUW+r7jIHWDpp2cr8ZXIqVRXXYboUP3lsHnPLNXz72RO5cSpF39ceULKOTbiYtgp+BAzzl2Eq3L5uCAuVOp47sajNX4xNE1JwHJuTptBi4OHCbZP43++7MmKmsVHwCdWawOxSFRfvmEK54OGi7eEiE3Iq4TXMcWBqoyZRv6iIXPO3M4PqGi2CH9M0FS5vAJhEfWj2LQWheSbks8JqkL7nYcNYSV8jC1Hve6TiS+qo1gRKvhcxy6bFAg0XfSxU6zpyPYiJU2Gwtp4Ud2XmyZtXBbrMfmBOhT2/AFmOIo5TeejwSQCI/N5hH3gRU5853/crIXTghy8nPnOncEKlQ2iivtgsQLKYvzxP5tuSwXWh9xebv+oNoQWMqTWwfblgLPq8+Jo7k7dfuh1/94HXRAZ6Wv4qHoBNwY9qV63JZ/W8pveZ7bnyhr0bcdnOZvWcnxForamYlSV5x6Y1lcDXedcOHZlDrSGaCiOtBHKnXQ/T+Kt4kCxFugBJrppp7ssFH9uVBvHUsXmtqTDY/HVMuY+nBYOaKPgeqo0GZhdruHjHFB76D9fhyl3rw+eIIa1N85e5gWjEaCoRl2Jr89Mq+DHJ+2u4GOgCYkMWUS+FiqcDA8OEk0JrdVwHiE1gicGPMWWGpUtxHYUg6qWZJiCHirI+zoJKTeR7pLWwJvNXQ3pcJW3aTK1+QRXokteRx5eUWcr8/cfK0sHDLtT1xAuz8EhWMbXBrua6yqvx+2+eGMKWySE86ITK4EIT9TGaSmQhT3Xj9awCWp4q1ys/Z1OY7VLsGWpyzfy+cS9P7f7ioubjECYZlP+VQ5oe1EmcSiRORd3rjre8CjdeGJ+ZJ7umElZ+5B1bqKk0fzcPTiVcgKQLMZeMjqtnHgcu1MWcylDBx3aVnLBalwkeObgRCM1fLFSyOANwOxdU0tHxctDUl6WCNPOYgnAkwqlEzV+8uLN3k/m9MClja+GaHPxYszQVNvsooaI8oeYr0TFWM0hnbjOT9Vm8v7j9zDPYfZL2LMNFHwefPYF6Q+DcTeMIPNIbGTu9kDavJYxp/lm5ngo794SFzRpNCV/HygEaotlz7rsvnMLM+pHY7NuyoJrQ5i9baF68YwoHfvhSakXJTuCESofgBXzE4lR8j7RbLdDCi8cjNVmVbZkkScjXZNurdilW9n3P5FQawigV3Hyvoulm3ELAyf+WptKICpUmTqVuciqth1V7moq8Lmsq4+XoZDS1obw0FUBOcs72y4tpWjAiI/T+Ct1XuTaHbLcXWZA4cWEoVFrfA5C/EddrjxOmw8WgqWLgsLFZMeNUzNxf2vxlChWrBECacJUu2UmaSoz5y9JUbN6u1ohWLAWAmy/Zhttee05iSYkmAj/wDO+s7JrKcNHXaW0u3DZpBe2Gr4mk63O11oikiDFhJpRcWE7WVMz5w+Zcm6x/4sVTOHdTfJJUaRZtRAp+mdg/M4UXZ5dx+OXFxOfuBE6odAgekKbtm+u/m5+nBrEFnkpYKLR6D4Taz6ht/lJpWnyPIiq1PfEi9zA1lZRJpCPq7cqPrKlUwtLJ5ue1iEtx60WRhVU7msqspanwZHzDeZu0W2U+nErUbFnwPaNIVzbvr4VKmAyUy/luHA9ru5j32DAm6/L8SKUeyaqpmBH1cc9965U78el37osc87xwsxKWE2aXYnbG4DgVc8xEhUqacC0XZLvsEsVc3G2ySagIkNpIlQqezkRtmr/CwGB57IKtk7j9uj0xfRI/rooqzoaJejsFTRJYW5oeKWLr1FBTjRnGcMFX9VQakTgxE2aalvlKyKlwZuflWrP5jAWwWStooVLDMy8tYM/G+IwRmlNRWTpsTeziHZJX+VaXTGBOqHQInpimWcH3QldfnnzpsSHxNnsWJsMxRH29Id+HLr0N7eoY5ycfNX+laSosTBC9J2sqnLMoJvcXE4OmaScJoaaSfq5ZxtbmVNg7aN/2SV2GOB9NJRrDwIItS/AjEJLhx+aWUQrCqnxsAhuyOJXhQoALtk7oiPhW2hsjsqON8Xo7c3IIV5y9vuk483XmGG2IUGNYjONUNFHfWrgm1alfqMqFlH8jXrDNrMelwDc4ldClOEy2mt7/rFE1Zy8Oyeui78Vm8o4Dz70Lt02CiCI1ZiJ1TIqBrmyZNL88rfU3sFCpR/gtzg5RteLb4oTK91+cgxDAnkRNRXEqCRkgzt00jtFSgIPPnkh87k7ghEqH0JpKIcqpNGkqqeQ4hfWyjUHAZq8RvaORxxtCChbPixbJ4hoepodPeA+T30njVNhzK/TgkteXny9Wo0R9nPdX0k7NRBzvFHue0hKA0ATAu/L9O6Zwze4NOO/MCVywLUehYi04UrA1WqZ8Z7DQO3pqOWLz5qJPtvdXuejhmt1n6PeZORXjGu1oaLwBsrMnsGNBbPAjexnWZd2fVJdibc6JmsAWlKaya+MoioGHzcrsB4QLbinwsGALlUYzp5KEczeNY9cZo02lDnihZfNXVk6F0/FfqHJnmQ4KUVdtH9WGaCo9YYLbvlCR2RqGjcJ7nNnZ5lTYqYFNcAB0ipokoVLUAlTEzi/fI9zz767Cb7xpb+JzdwIXUd8hdBEsY4AEvqGpZDB/BT7pOJUIqaoGtO39xdmAzTiVSr2hyxqPxQiVuFxIcTCrAcp7yv928CNPqEjuL2TnVLhfWlVkNCPqbU3lsrOmcdlZ0wCAt12yHaOlANvWDcVfqA2YbfJVrA2bv7J5f4VCxdxsbNdCJcxM6yvTxzV7NuB3v/49AEgkem2YAq4dB4WxUtT8FQqVFE3FCCC0Nz82EjUVxSNcvGMdHv3YddpduyGiHNt8JcrbRZ1Y0vv/4h1TuO/91zQdL/geKrVw0W+HUwGgEzIm5fYbKvg6KWiSSZefkTdHw8bYKAXSucHmVDhDxMnFij723RdOoVzw9HiyUfQ9HeCYtO5sn47/bh5wmkqH4B9tKELUe007/VRNRaV1sQMXWZiERL28llCciueRXiDmlmvhohuT28vmCRLbYu1eOZ8ZCxWugU4W52IWK8riEptVU+EUIkAoVOI0samRIt796pnUPGJZYZeBDlSsjF1WIAmjJbnAP/vyYoRrM4UKLzzcl+dvmdBZD7IS9XG29yzgcWXHIvGGIc37q1YXqTXqgZALadZUQh7BHme+1lRC81fBj3O3X9mSxbt3zmSdVVMZLxfgEfAqVaMnKQv5SClQcSrx2oH5jDyOTSeKsgoY5TgVRpz563svnsLujWOJ7ebgR46z6jWcUOkQdrleeYwMs1dGTaXeXMOCJ/9ojEtxQ0iydEx7h9S0+WssxhQSjSBO4VQMjyAGJxwE5MIwFHnW0Pxm5/5KQ1aX4sByKR4q+F2fKHGBc7VGIxLNn4YzJ6VZ56X5in4NhEJFZqaVfci7cd8jXLVLFo0rZeCk7HauxPylywlT1PzFwiWOh6vqcZp8/aQ69YvVelMwpxYqnNqoEAa7yowMMhanlpFTSUJRuRSfWJBCJa7mUBzeefl2/LdbL9NmqKQifMNFX2dBbpVQcm456uwiryUDRpnzYYwUZW0i0/z1o5OL2DaVrGkUVJqWtJiZbsKZvzpEbPCjwalkIeo52yynoWCwoGJ/dt6Em+avsopFmF2q6jiOuJ08Zy8VokWcStDcXs8j7f21YNQW5+sC0tbOzj7tuRS3yv3loW4Q9XGEdN6IloGWG4TZxRpqDZHJzHTWhlF889deh+VqAxsnSpHjpcDDpomyHhdDxbCv3nn5DkngZhBcQPg7FnyKjdlJQivzF8NOKAlAe3WlaQwlramEQqWqgvFMkw/QXGLBLhdRUI4a9YycShJ4oT250J6mMj1awhXnlGLPNcfJSDHQsVpJGyVu+6zWVMLvs6ZSrTci5lciwuRwQScfBYDjcxVddCv2WZVWJjWVzjX3duGESocIgx+jQWWmu6Y8lh4bUo3jVNTkZ+HiG+avegMqDoZ0euy5FE5FZy9NIRKB0HXUdBn1ibT314mFKqZGwoXVdJNku3eWgZxZU/ENTWW5GquF5Y2opiI5leMqHiSrmYkDGk2sGyniHz54LdaNFPG33z0CIOrgcenOdbg0IQNBHHi3PV4utGX2SyLq7YBFO6EkEA1UTAJrKmasCrsp25oKa8T8v2w5vHDFyWpGTiUJxcDDycUqTi3XMDlciAindq4ZJMSpDJd8ySumzK8mTaUQo6nUG02bivGhgjZ/VesNnFys6vxpceDgXZuf6RWc+atDcPErc+cgORWLqE/TVLzQ+6tgqb6AQdQzKS5kni8+dXyogNnFmt4BjRbj9wrZCj9FFxogrA8DSJOOWaaXzRZmka48gx/Z9AQgs/mpU9jpdQLPw0vzMjCxU++yDWMlGeRXCDmVlYIJ/XazCPB4sseoXbs8LpaDBU9q8GOh2fylXdGtsdnMqURNjwWVcr9TTqXgezq4dLINTcVGEqfCWsd8pdaSU5lTFoWhGE1lOUYoTQ4VcFKZv15WpTKmU4QKZw/ol/nLCZUOsW/7FP7qfVfivDMn9LHCijiVFE3FIurDNC3y/Xg50Oav0VKQ6O6pzSUpE9M2iQBhZUBADmpzlxQGxRm5v9oh6ltlKTY4ldnF3msq7MmnI9dzuj/3UVyajezXCH//djCqxhP/dqa2aSIutQ+byFoFP5rnAjDK5yaYv2LMoYHnSSeJeh6ciqcTdk4OF6PeX22YiKLeX1HzF9BcnC/6XXk8jqg3NRV7/kwMFXBCeX/xOFyXIXt1v8xfTqh0CCLCK7dMRI6dtWEEZ62Xid4yaSq+4f1lnDesgx/ZpVge54SSWqgMyZoLcy128oUMmkoxRlPhehtAs6bCE4ir6iXV57bhxexM4+CrlOuNhuihptIc/MjmlzziYICw39KyELeCrg7YZpteffY0Xr93ozazJJmyot5fUZ4kbTxz/z370gL+7JvPAEAkD1rcPeI0VzYj58GpFIMw3mliOKqpZPHos9sLRF3PTQGRaP7yo95fZl+EnIpoEgSTw0Vt/jquBGMqp6ITSq5B8xcRXU9ETxDRISL6UMznVxPRg0RUI6KftT7bTkT3EtHjRPQYEc2o458noqeI6KD6u1AdfwcRPUREDxPRPxLRq7r5bGn4yE/sxR++62IA2YIfORbDrqjHO8q0NC1AWB3u1FItEqVrIxQqGTQVi1Nhe/GppVpE9eY4C+kXn13djitklnZerSFkEa6em7+i0dd5OQrwwtuJ+StYoVC5eMc63PXu/U3mr6brpwiVVgklAeCT938fv/aVh3FsbllzKonmLxYqBbPvSWViDj0L2zFVmTAX6knL+6s9TqWZZwKiGliiUGHzV5r3V4wgmBgqaO8v5vbSzF9m8OOa8v4iIh/ApwC8HsBhAA8Q0d1CiMeM054BcAuA22Mu8QUAnxBC3EdEowBM15RfFUL8pXX+UwCuEUK8TEQ3ALgTwGX5PM3Kob2/Us1fyvvLCirbv2MdfvwVG3HWBlnNz6xt0miEg3RcVYebW07fyfNimVSZDkjgVJSmwqWPp6wBbZb1zeJODJiaSuvKj4B85lNL/SDqo7mTctdUcjF/ddampEU1uptn3qXR9JkNNulxMO7xuYpOEtlE1OticPK9rSWa2QxkO1bOqTAmh4uR+dgep2IQ9YYANJ+rVfAje2nGeX8VfGoKfp0YKuDUUg31hsBLihdKI+q5XHO10UCp0HtfrG7e8VIAh4QQTwIAEX0JwI0AtFARQjytPov4MhLRXgCBEOI+dd5cq5sJIf7RePsNAFs7bH8uyBL8yKVabU5l27ph/NHP7dfvWXlgToXfjw8VNKcykVKjPayfkeL9FcepENBoAC8tsD3XFipyl+WpnWUWcBuyxKkAkkRerjViAzvzRjNRH/ZFXkKN71HOxfzVWZ8kc3BhP/DvlEVTsbXP43PLBlEfb/7i8RANPPV0LZswoeRKNZXwuhNDBZiXaYtTicn9FVjjPmnTxuNozkpDA3ANmjqE8Jo4FXYGml2s4vh8BR6FkfZx4ASoS9UG1g2vLfPXFgDPGu8Pq2NZsBvACSL6MhF9m4juUJoP4xPK1PW7RBTHWN0K4K/jLkxE7yGiA0R04OjRoxmbs3JkJupVfqO0BT8MfoyWth0vB1iqNnB8vpLOqQSt22IX6QKU+UsITRJOWQN6qGgGbmWboDxvsmQpBqCjrDvhILKiiajneuGlYMWLmo1SDpqKFiodCjrT1GkuaHGaylIti/kr+kzH5is69YotVOx0RtEUOdL7q9ZoaE5lpcSzKazGy9HfsR2PsjjvL1nuu9lUaMPT81e2x04eu1xrYHapFsOpqPxfSqhMDRdT+5/vv1iprT1OpQMEAK6CNItdAuAsSDMZAHwYwLnq+DoAHzS/SESvhRQqkeMMIcSdQoj9Qoj9GzZs6ErjTWQh6qWHS+viR0mcCtvUXzi5lLqT5wGWnqVYmb+MhcbzZJzKy/NSbW/SVAJf5xrKmrfKj9mZxraHhYoyn2Q1r3UCu0qmjgfJyfQF5GP+YmHXabsii5uxKYmrp7KcwfxV8D0UAw9X75bz6/jcsnbJtnfYusKo6vJyTOBprZ4HpyJvMFYOEPheLpwKj92ClfU4bYzGFfUDgHddvgO3vfYcvGLzOPZZJYLNVC0vzVVSTV9AOH4XKvUVOzZ0gm7O0OcAbDPeb1XHsuAwgINCiCeFEDUAXwWwDwCEEM8LiWUAn4M0swEAiOgCAH8E4EYhxPEcnqFjZAl+LPgUJupLGQRRl+LwPWsntYbI5v2VMolY0wgiOzmlqWhOJbqIlYu+qq+dXJ/bRlaXYu43zkeVNS18JyAi3S7fCzWVvPgUQO7mPYrPfpAVRa2pdGb+irqxN2dLMO+VxfsLAD7zrovx2z9zPnyPcHyugudPLmGk6De1tSmi3tJUuCpqXpwK7/rjsgVkgR8jVGTRr6jZrtX37c3E1EgRt1+3B3/9S1fhhvM3Rz6bGJJC5MRCBS/NtxYqrOksVOqZ52Oe6OYdHwCwi4h2ElERwM0A7m7ju5NExKrEtVBcDBFtVv8JwE0AHlHvtwP4MoB3CSG+l9tTdIhM9VQScn81X0v+b+jKj/K9af5Is/nzAEvTJuJyf3G9jZcTzF/lQHp/tRNsxee1itPQyQ4r7TkCdAqdhdkoPZtniphywcdnb7kEb92/rfXJCdg4XkbgkXZfXynMn8ysYBq3m5+PKeAVh9fuOQObJ4awbqSI4/PLeHF2CZsmyk2R/6Gm0uy4ITUANg13pqnwuJlUC3TWLMU2Ys1ffjbzl3mvdsy4LAhPLlZxbH4Z60eTY1SAcH4v9Mn81TXWUwhRI6LbAHwNgA/gs0KIR4no4wAOCCHuJqJLAHwFwBSAnySijwkhzhNC1InodgD3K+HxLQB3qUt/UQkbAnAQwHvV8Y8CmAbwB2rg1oQQIcvdJ9jpWmLP8eJzf9kwORVTAJnmj3SXYnl+WvAjD3ZTI5BEveRUxstB00AdKvp4eb6CasHPbP66ZOcUfuNNe3GRSimeBO4P9h7KUgAsD5QCH6dQ0x5IQL6aCgC8Zs8ZrU9KwbZ1w3jkY9d1FEAJRDkFc/zYHlJEwPMnZAna9SlxEiamR4o4NlfBsbllbJ5oLktgZ0q2I+rl3Kh1zKnw93iB5lLfQqycUyl4MnV/MfAiQjat8ikLUdu1Og0R81cmTUW2pSFWHtfTCbrqSiOEuAfAPdaxjxqvH0CCl5by/Log5vi1Cef/PICf76S93YBdljcOxUBmYs3KqdQbQmcpBmxNpbPgx8t2rsPvvfVCneqb215XQiVuQDOnMlLPTtSXAh+3Xrmz5Xl2WvZeayoFL1ww8oqmzxOdChQgyp/ZFUwZRISC5+E5JVTOGGvObRaH9aMlHJtbxgsnl/Bj5zRXodS5v2JczANF1Fdz4FR43Jgbg8CT114pp+IpoVdQiS/1vVI2PivRVLjNx+YqOLGQnvdL3j+bKa5bGFSifs2Ayca0hH9MRrby/uJLCJX6Ps4sk+79xcRiminOw00XbWkyf9WFwMsLlaYYFYC9v2Ta77wXfe4P5lR6LVS6qakMCswhF9FUrMW24BNenF0CAGwcTzfBMKZHizgyu4wjp5YjlR4ZPBTjgh+1ptLonFMpWpyKec+VxKmYGpatqaTNL/5+Ow4aBd/DSNHH06rcdFo0PRCdI72aLyacUOkyTKI3CYHvabtx2gDXgYAimqZlrE1Opd2J6Svvr5fmK1gX4x9fLoSV5vLeGfEzs/mrF0Q9EE5Gk1NZq0LFHA+mV5I9FgPfQ0PIBTHNzGpieqSE504sot4Q2BQnVFLMX4Hv6TQtecWpRDWV1pq7jbiKrgWbU0kZozw9bO+vVpgcLuI7h2VN+emUvF+AVTo8Jxf4duCESpfhEbVcxFl1Xq42UgdB1KU4HNgjRV+T9mmTPUuW4qT7NoTAy/PxmgoHP9pV6/KADn7sE1EvYyXydykeJESIeqMqpK1dcz9sHC9lTrVv7qpjNZUUoj5M09JAXSUrXekiWbCIevOeK8n95RsOOEXbpTiVqFeaSptC5e2XbccPjy8ASI+mB5DZaaBbcEKlyzCD55LPYRNPLVuciuJU+LJEpBe8LMGP7boZyiJdMqI+LucQC5W4DKudgpPwLTCn0qNJojUVb+1rKmbG4VErLb4JXqyy8ikAsMHwVNo03kzUZ0p9Xws1lRUnlGS3cMP8FZc9ohXs7OO+5ynzV7NXWBySXIpb4Rdfew5+5fW7UQo8zKxPry9vzpF+mL9cka4uQ2oq6YP24h1T8EjmSkqPU5H/G0KS9SbvMV6WSefS4h6yJJSMg09S4C1VG/GcSsFHtS6wWKnnPojZU21hubfmL52Cw/e0JtmLqpP9gLlT5/ETN2Z5/JyRkU8BWmsqOveXup/peFDwPQwVfCzV6pESwytB6FLczKmsxKXYDGou+u3HqbRr/gKA971uF977mrNbah+m+c2Zv9YguMhTGi7duQ7/8aZXAkjfidtpWswdJi94aR5KIafS3kDzPcLRUyqRXQKnAgCzS9Xc6zfoNC0VDn7slUuxqak02+PXEszhaRfwMhGsQFOZVppKKfAiJDnD1lTMTYlHMrB2sVI3inStbHxxbNWZk6G2xPOyLU3F0m7iOJVMmkobLsUmspizonVwnKay5rB+tIT1Y619+t9x2Q5Mj5SwZ9NY4jlkuBTXhYi4go6VCij4lLqTz5KmJQ4eEZ59Wdpzd0w3q96sys8t5x9sxZO415xKtyPqBwlx5RbitNmiwalkBZtLN8cEPgKGGclYrAs+QQg53ocKPpZr0rPQPL9dXLhtEl9//9U454xwfvFztzNmfUsQjZcLmBwuxBY0i4N2Kc7BFTwJUU6l95qKEypdxq+8YY+eEK1w/Ss3pX7OA1m6FEej3seHAoyWglQCNUucShw8kpO86Ht4VUywIpePFSL/nZHt/dXziHrDpXgQ41TygLlQc0R93OKtNZUVmL/iPL8AI47LGLelwNdVRHnxnV+uwaPkjMqtQEQRgQIYnlxtXDKwNKs7330xhouBLnkNZLM2rMT8lRX9jlNxQqXLGCr6uWXW5cFf11mKw0V8emcAABOuSURBVM8u2DqJ5RbCi4n6dgcaT4QLtk7EBtuZu668OY8wor5/msolO9fhhldu0qactYY4oj6NU9nYhvlruBhguOjHRtMD8bEi5hjiuXNqqbZiPiUJHIOU1ZONv2P+3zolNXdOYQRkNX91U1NxQsUhI0yXYrNGPSC9Q1rhwm2T+LFzpjPHGOj7qomwf2Zd7Oc2uZonfFuo9GiSMHdT8D3s2z6FT7/z4p7ctx8wF3QuYe3HaLPsNNGOpgIAH77hXOw9czz2MzuiHpBChYn5smFazavsAMMuwJYFdloZfTyjG+9Kvb/aQYSod+YvhzREXIotoj4Lrjh7Pa44uzlVRivwuLx051Ts50NdFCq8O12o1EDUOxuxqamsdcRyKjFaAWu6Z4xn11QA4F2vnkn8zHbRBaQ5lbVuHlunlmu5ezIFfmvPTBtJUfhZAw6TUt/niYjTgItTcUiD6VLcEL1b8DiZ4MXbkzQVQ91OSaa3EvBOi9N4t2Oq6AQ6+LFH9+snoqnv081f5YKXa/VN33IpBqIFrLQTyFI1912373ltu9cneYyZpRzSxihvBLtp/nKcikNm6DQtjdYFvfLEhrEy9m2figSOmTDNX3nvjExOpZeBXK/ZcwZOLdVWTAyvJpiCc7iQFvzo4YyxeC+ulcIu0gVITYUXQ5NTaSebcBbYpaKzgJtgbzZ449Vq/LNg7K73lzN/OWQET2ahOJVebaI/9ubzIt4tNiJCJeeF36yn0svgw0t3rsOlO+M1s7UGu0aIHSHOuPXKnRFCOpd7x3p/hZqKyankbf5aGacix7e92eAszq3MsywY20l93y6ypozpFpxQWWXwPVkwS1hxKt1EMfBQTLGUmqp8tziVSr3Rs8DH0w2RGiEq1ilOK7j8rOn8763jU8L7lQKvKZ5jbqmWe+61QgecSpLLdavxzzKnV5xK1vpGecJxKqsMHoVZigeFRC4H3bPhxlXac8gXvOsmkv1tLurdhtZUTPNX4IdmIrX4zlVqXcjW4MV6uaXBzlJsf9ZqjLLwzKMOThKISPdVP9K0OE1llcEjUgkl0TPSuhVMTSX/eir99WQ5HcB9XFCOEDKXVe+cQIDoIn3LFTM4Pi/TAg0ZgbV5b6ICj1KroMYhTVMp+F5rTqUH3l/clmq97oh6h9bwiMLaEgMiVMqBSdTnvJvMmFPJYeVgjyROnJnEqXTz3uZYvnJX6PZuEtrdCH5sm1Pxm9vLKFgp8GPv2QOiHpC/Ya+dWxhdvSMRXU9ETxDRISL6UMznVxPRg0RUI6KftT7bTkT3EtHjRPQYEc2o458noqeI6KD6u1AdJyL6pLrXQ0S0r5vP1i94BNRUCotB2bh7htqf987I3Ek6odId6BxYqn+LCZxKd+4t/ydpRuViNBV+nnjNng14w3kb2/pOqFk1fxb4GcxfJM2L3fYq1CmZ1pL5i4h8AJ8C8HoAhwE8QER3CyEeM057BsAtAG6PucQXAHxCCHEfEY0CMN2PflUI8ZfW+TcA2KX+LgPwafV/TcHzCFWlqQySu2s58FCpda/yI9C7tPenG3jXrVPbj5WxvkUhqNzuneBNxSj6HjyScVl5cyrvuGxH29/R1SLjgkP91t5fgUddN30Boal4rZm/LgVwSAjxJAAQ0ZcA3AhACxUhxNPqs4i/KhHtBRAIIe5T581luN+NAL4ghBAAvkFEk0S0WQjxfB4PMyjwiFBV0cbtRtR3E0NFH7NLte5yKk6odAWeFWPxqbfva5vAXil4zUsy5XKm4vlKfSAcU+I4IAZXqkzDpokytq9LL7KVB1i4rTXz1xYAzxrvD6tjWbAbwAki+jIRfZuI7lCaD+MTysT1u0TEiYgy3Y+I3kNEB4jowNGjR7M/zYDA9waPUwFCb5a8d0a84AGOqO8mfMNjaGK40HZ+uE7uC6Rr3Ty28uZUVoIwS3HMZ77XchF//+t348//zau70bQI+mn+6v+vFI8AwFWQZrFLAJwFaSYDgA8DOFcdXwfgg+1cWAhxpxBivxBi/4YNG3JrcK/gEXRa8EEyfzHx2I2F30yB4dAd+Bl22d1AGFHfWqgMkqYSxzldOjOFfdvj8+MxAt/rqjsxQ9dOWmPlhJ8DsM14v1Udy4LDAA4aprOvArgcwB8b5qxlIvocQj6mk/utGnhEqNVZU+lzYwxwTZW8c38BciJX68IFP3YR/RIqcQklbbDLej9SjtgIsxQ3f/axG1/Z49YkQztdrLGEkg8A2EVEO4moCOBmAHe38d1JImJV4looLoaINqv/BOAmAI+oc+4G8G7lBXY5gJNrjU8B2KV4EDWV7g1iNns4TaV78In6E30dk/rexpA2f/V/vIdxKoM9Fot9DH7sWs8IIWoAbgPwNQCPA/gLIcSjRPRxInozABDRJUR0GMBbAHyGiB5V361DaiD3E9HDAAjAXerSX1THHgawHsBvqeP3AHgSwCF17r/t1rP1E75HqChNZZCI+m5xKkC4Q3XeX92D75OOU+npfam1+WtIm7/6//sTydiWQdrQxUEGsvbHZNhVNk4IcQ/kYm8e+6jx+gFIM1Xcd+8DcEHM8WsTzhcAfrGT9q4GUCROZXAGtuZUurDwO06l+5BEfe/7N82bilEuDo6mAqigycFoSiI4ELMfWTfcLF1liHIqgzOyy10k6nnhcd5f3YPn9cf8lZb2hMGm1UHgVAA57wZBa0pDwff6onkCTqisOvgeoao4lQGSKaH5qyuairymM391D4FHuafYyQLtTbVKOBVAtmPQ9zfFoD+bBMAJlVUHaf5SmsqATDIgrP7YjXK/vjN/dR1en8xfXoY4Ffb+GhTtwPdXh6bSL2eCwe4Zhyb4RDpOZZCECu8m2836mgWOU+k+SoXexE/YCOM+ks8pD5imcv6WCezeONrvZqRieqSE6R6l2rHhshSvMniGUBkk768bXrkZgd+dRHlOU+k+/tNPnY/p0VLrE3NGWjAhQ5u/BoRT+dNbBz+l4PvfsBvvfc1Zfbm3EyqrDJ5HqNUGz6X4/K0TOH/rRFeuHfjMqbjgx27hsi5UdcyCuHLCNgaNU1kNGC0FPUu1Y8Nt/VYZvAin0ufG9AjO/LV2kcX8NWicikM63K+0yiBTlgye+aubcC7FaxdZIurLA2b+ckiHm6WrDGRWfjxNzAGsqTiX4rUHnfsrRWAMDVBCSYfWcLN0lcEjYKFSk69Pk0nmiPq1C+1SnMapFNmz8PQY76sdbpauMvhEWKo2MFYOsG9beprttQKX+2vtIuRUVkfuL4fWcL/SKgPv6N57zdmYGC70uTW9ge+yFK9ZtKr8CDhOZbXBzdJVhlLBw/rRIm65YqbfTekZCs78tWbBG4ZUTaXoOJXVBBenssrw6z+xF/WGwEiffND7Aef9tXaxY90w1o8WsTWlbruLU1ldOH1WpjWCPZvG+t2EnkNzKn1II+LQXcysH8GBX3996jlOqKwuuK2fw8BDcypOUzktMTFUQMEnTA73J5eVQ3twmorDwMNF1J/emBgu4N5/fw22Tg31uykOGeCEisPAw3fBj6c9dq4f6XcTHDKiq7OUiK4noieI6BARfSjm86uJ6EEiqhHRz1qfbSeie4nocSJ6jIhmrM8/SURz1vl/S0TfJqKHiOiN3Xouh94icES9g8OqQddmKRH5AD4F4AYAewG8jYj2Wqc9A+AWAH8Wc4kvALhDCPEKAJcCOGJcez8AO/Lv1wH8hRDiIgA3A/iDHB7DYQAQ+ISCT6dNBgEHh9WMbm79LgVwSAjxpBCiAuBLAG40TxBCPC2EeAhAwzyuhE8ghLhPnTcnhFhQn/kA7gDwAet+AsC4ej0B4Ec5P49DnxB4ntNSHBxWCbrJqWwB8Kzx/jCArNVtdgM4QURfBrATwNcBfEgIUQdwG4C7hRDPUzQK9zcB3EtE7wMwAuDH4y5MRO8B8B4A2L59e+aHcegffnrfFpy9wdnUHRxWAwZ1+xcAuArA7QAuAXAWgFuI6EwAbwHw+zHfeRuAzwshtgJ4I4A/JaKm5xNC3CmE2C+E2L9hw4auPYBDfrhg6yTe9eqZfjfDwcEhA7qpqTwHYJvxfqs6lgWHARwUQjwJAET0VQCXA3gBwDkADiktZZiIDgkhzgFwK4DrAUAI8U9EVAawHgYX4+Dg4ODQXXRTU3kAwC4i2klERUjy/O42vjtJRKxKXAvgMSHE/xFCbBJCzAghZgAsKIECSNL/dQBARK8AUAZwNKdncXBwcHDIgK4JFSFEDZL/+BqAxyE9sx4loo8T0ZsBgIguIaLDkCatzxDRo+q7dUjT1/1E9DAAAnBXi1v+CoB/TUTfAfDfAdwihBDdeDYHBwcHh3jQ6bzu7t+/Xxw4cKDfzXBwcHBYVSCibwkh9sd9NqhEvYODg4PDKoQTKg4ODg4OucEJFQcHBweH3OCEioODg4NDbjitiXoiOgrghyv46noAx3JuTh5w7Wofg9o21672MKjtAga3bZ20a4cQIjZ6/LQWKisFER1I8nzoJ1y72segts21qz0MaruAwW1bt9rlzF8ODg4ODrnBCRUHBwcHh9zghMrKcGe/G5AA1672Mahtc+1qD4PaLmBw29aVdjlOxcHBwcEhNzhNxcHBwcEhNzih4uDg4OCQG5xQaQNEdD0RPUFEh4joQ31uyzYi+lsieoyIHiWiX1LHf5OIniOig+rvjX1o29NE9LC6/wF1bB0R3UdE31f/p3rcpj1Gnxwkolki+uV+9RcRfZaIjhDRI8ax2D4iiU+qcfcQEe3rcbvuIKLvqnt/hYgm1fEZIlo0+u4Pe9yuxN+OiD6s+usJIrqux+36c6NNTxPRQXW8l/2VtD50f4wJIdxfhj8APoAfQFahLAL4DoC9fWzPZgD71OsxAN8DsBeyrPLtfe6rpwGst479DmRJaAD4EIDf7vNv+QKAHf3qLwBXA9gH4JFWfQRZyfSvIUtAXA7gmz1u1xsABOr1bxvtmjHP60N/xf52ah58B0AJshz5DwD4vWqX9fl/BvDRPvRX0vrQ9THmNJXsuBTAISHEk0KICoAvAbixX40RQjwvhHhQvT4FWbNmS7/akwE3AvgT9fpPANzUx7a8DsAPhBAryaaQC4QQ/w/AS9bhpD66EcAXhMQ3IAvYbe5Vu4QQ9wpZHwkAvgFZxbWnSOivJNwI4EtCiGUhxFMADkHO3562i4gIwL+ArO/UU6SsD10fY06oZMcWAM8a7w9jQBZxIpoBcBGAb6pDtykV9rO9NjMpCAD3EtG3iOg96thGIcTz6vULADb2oV2MmxGd6P3uL0ZSHw3S2PtXkDtaxk4i+jYR/R0RXdWH9sT9doPSX1cBeFEI8X3jWM/7y1ofuj7GnFBZ5SCiUQD/E8AvCyFmAXwawNkALgTwPKT63WtcKYTYB+AGAL9IRFebHwqpb/fFl51kaes3A/gf6tAg9FcT+tlHSSCijwCoAfiiOvQ8gO1CiIsAvB/AnxHReA+bNJC/nYG3Ibp56Xl/xawPGt0aY06oZMdzALYZ77eqY30DERUgB8wXhRBfBgAhxItCiLoQogFZgrkran8ahBDPqf9HAHxFteFFVqfV/yO9bpfCDQAeFEK8qNrY9/4ykNRHfR97RHQLgDcBeIdajKDMS8fV629Bche7e9WmlN9uEPorAPDTAP6cj/W6v+LWB/RgjDmhkh0PANhFRDvVbvdmAHf3qzHKXvvHAB4XQvwX47hpB/0pAI/Y3+1yu0aIaIxfQ5K8j0D21c+p034OwP/qZbsMRHaP/e4vC0l9dDeAdysPncsBnDRMGF0HEV0P4AMA3iyEWDCObyAiX70+C8AuAE/2sF1Jv93dAG4mohIR7VTt+udetUvhxwF8VwhxmA/0sr+S1gf0Yoz1whNhrfxBekh8D3KH8ZE+t+VKSNX1IQAH1d8bAfwpgIfV8bsBbO5xu86C9Lz5DoBHuZ8ATAO4H8D3AXwdwLo+9NkIgOMAJoxjfekvSMH2PIAqpP361qQ+gvTI+ZQadw8D2N/jdh2CtLfzOPtDde7PqN/4IIAHAfxkj9uV+NsB+IjqrycA3NDLdqnjnwfwXuvcXvZX0vrQ9THm0rQ4ODg4OOQGZ/5ycHBwcMgNTqg4ODg4OOQGJ1QcHBwcHHKDEyoODg4ODrnBCRUHBwcHh9zghIqDQxdBRHWKZkfOLbu1ynrbz7gaB4cmBP1ugIPDGseiEOLCfjfCwaFXcJqKg0MfoOps/A7JujP/TETnqOMzRPR/VZLE+4louzq+kWQtk++ovyvUpXwiukvVzLiXiIb69lAODnBCxcGh2xiyzF9vNT47KYQ4H8B/BfB76tjvA/gTIcQFkIkbP6mOfxLA3wkhXgVZv+NRdXwXgE8JIc4DcAIyatvBoW9wEfUODl0EEc0JIUZjjj8N4FohxJMq8d8LQohpIjoGmW6kqo4/L4RYT0RHAWwVQiwb15gBcJ8QYpd6/0EABSHEb3X/yRwc4uE0FQeH/kEkvG4Hy8brOhxP6tBnOKHi4NA/vNX4/0/q9T9CZsAGgHcA+Hv1+n4AvwAAROQT0USvGung0A7crsbBobsYIqKDxvu/EUKwW/EUET0EqW28TR17H4DPEdGvAjgK4F+q478E4E4iuhVSI/kFyOy4Dg4DBcepODj0AYpT2S+EONbvtjg45Aln/nJwcHBwyA1OU3FwcHBwyA1OU3FwcHBwyA1OqDg4ODg45AYnVBwcHBwccoMTKg4ODg4OucEJFQcHBweH3PD/AW9volXVmBwtAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sD5_iGFxK2t",
        "outputId": "731d1c92-ebae-4cf5-dffe-1c6cba52f835"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    }
  ]
}